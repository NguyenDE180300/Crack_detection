{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ce2adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import SwinConfig, SwinModel\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# --- Cài đặt tham số cố định ---\n",
    "IMG_SIZE = 320\n",
    "BATCH_SIZE = 8\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Thay đổi đường dẫn thư mục tùy theo máy của bạn\n",
    "train_img_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\train\\image'\n",
    "train_mask_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\train\\label'\n",
    "val_img_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\val\\image'\n",
    "val_mask_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\val\\label'\n",
    "\n",
    "# --- Thu thập đường dẫn tệp ảnh và mask ---\n",
    "train_img_paths = sorted([os.path.join(train_img_dir, f) for f in os.listdir(train_img_dir)])\n",
    "train_mask_paths = sorted([os.path.join(train_mask_dir, f) for f in os.listdir(train_mask_dir)])\n",
    "val_img_paths = sorted([os.path.join(val_img_dir, f) for f in os.listdir(val_img_dir)])\n",
    "val_mask_paths = sorted([os.path.join(val_mask_dir, f) for f in os.listdir(val_mask_dir)])\n",
    "\n",
    "class CrackDetectionDataset(Dataset):\n",
    "    def __init__(self, image_filenames, mask_filenames, augment=False):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.mask_filenames = mask_filenames\n",
    "        self.augment = augment\n",
    "\n",
    "        if len(self.image_filenames) != len(self.mask_filenames):\n",
    "            raise ValueError(\"Number of image files and mask files do not match.\") # Số lượng tệp ảnh và tệp mask không khớp.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def resize_image_and_mask(self, img, mask, target_size=IMG_SIZE):\n",
    "        return img, mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.image_filenames[idx])\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not read image file: {self.image_filenames[idx]}\") # Không thể đọc tệp ảnh\n",
    "\n",
    "        mask = cv2.imread(self.mask_filenames[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise ValueError(f\"Could not read mask file: {self.mask_filenames[idx]}\") # Không thể đọc tệp mask\n",
    "\n",
    "        # Chuyển đổi ảnh sang RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Áp dụng Augmentation\n",
    "        if self.augment:\n",
    "            # Random Crop đầu tiên để có các phần khác nhau của ảnh\n",
    "            # Đảm bảo ảnh đủ lớn để crop, nếu không, resize trước\n",
    "            if img.shape[0] < IMG_SIZE or img.shape[1] < IMG_SIZE:\n",
    "                img, mask = self.resize_image_and_mask(img, mask, target_size=IMG_SIZE)\n",
    "            # Lật Ngang\n",
    "            if random.random() < 0.5:\n",
    "                img = cv2.flip(img, 1)\n",
    "                mask = cv2.flip(mask, 1)\n",
    "            # Lật Dọc\n",
    "            if random.random() < 0.5:\n",
    "                img = cv2.flip(img, 0)\n",
    "                mask = cv2.flip(mask, 0)\n",
    "        else:\n",
    "            # Nếu không augment, chỉ resize về đúng kích thước IMG_SIZE\n",
    "            img, mask = self.resize_image_and_mask(img, mask, target_size=IMG_SIZE)\n",
    "\n",
    "        # Chuẩn hóa ảnh và mask về [0, 1]\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        mask = mask.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Đảm bảo mask có chiều kênh (1, H, W)\n",
    "        if mask.ndim == 2: # Nếu mask vẫn là (H, W)\n",
    "            mask = np.expand_dims(mask, axis=0) # Thêm chiều kênh ở vị trí 0 -> (1, H, W)\n",
    "        elif mask.ndim == 3 and mask.shape[0] != 1: # Nếu là (C, H, W) nhưng C không phải 1\n",
    "            if mask.shape[0] == 3: # Nếu là 3 kênh (ví dụ, mask gốc là RGB)\n",
    "                mask = mask[0:1, :, :] # Lấy kênh đầu tiên\n",
    "            else:\n",
    "                raise ValueError(f\"Mask has unexpected shape {mask.shape} at index {idx} after transform. Expected channel dim 1.\") # Mask có hình dạng không mong muốn sau khi biến đổi. Expected channel dim 1.\n",
    "        \n",
    "        # Chuyển đổi từ NumPy array sang PyTorch tensor\n",
    "        img_tensor = torch.from_numpy(img).permute(2, 0, 1) # Ảnh: (H, W, C) -> (C, H, W)\n",
    "        mask_tensor = torch.from_numpy(mask) # Mask đã ở (1, H, W)\n",
    "\n",
    "        return img_tensor, mask_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3a9345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # Ưu tiên sử dụng nn.Upsample để tránh checkerboard artifacts\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), # Lên gấp đôi kích thước không gian\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1) # Giảm số kênh về out_channels\n",
    "        )\n",
    "        \n",
    "        # Kích thước đầu vào cho ConvBlock sẽ là out_channels (từ upsample) + skip_channels\n",
    "        self.conv_block = ConvBlock(out_channels + skip_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_features=None):\n",
    "        # Bước 1: Upsample đầu vào x từ tầng trước đó\n",
    "        x = self.upsample(x)\n",
    "        # Tại đây, x đã có kích thước không gian lớn hơn (gấp đôi so với trước upsample)\n",
    "        # và số kênh đã được điều chỉnh về out_channels.\n",
    "        \n",
    "        if skip_features is not None:\n",
    "            # Bước 2: Đảm bảo kích thước không gian của skip_features khớp với x.\n",
    "            # Đây là điểm sửa lỗi: nội suy skip_features để nó khớp với kích thước đã được upsample của x.\n",
    "            if x.shape[2:] != skip_features.shape[2:]:\n",
    "                skip_features = F.interpolate(skip_features, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "            \n",
    "            # Bước 3: Nối (concatenate) x và skip_features theo chiều kênh\n",
    "            x = torch.cat([x, skip_features], dim=1)\n",
    "        \n",
    "        # Bước 4: Chạy qua khối convolution để xử lý các đặc trưng đã nối\n",
    "        x = self.conv_block(x)\n",
    "        return x\n",
    "    \n",
    "class SwinUNet(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.IMG_SIZE = IMG_SIZE\n",
    "\n",
    "        # Cấu hình Swin-Base\n",
    "        config = SwinConfig(image_size=self.IMG_SIZE, num_channels=input_channels,\n",
    "                            patch_size=4, embed_dim=128, depths=[2, 2, 18, 2], num_heads=[4, 8, 16, 32], # Swin-Base configs\n",
    "                            window_size=7, mlp_ratio=4., qkv_bias=True, hidden_dropout_prob=0.0,\n",
    "                            attention_probs_dropout_prob=0.0, drop_path_rate=0.1,\n",
    "                            hidden_act=\"gelu\", use_absolute_embeddings=False,\n",
    "                            patch_norm=True, initializer_range=0.02, layer_norm_eps=1e-05,\n",
    "                            out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"])\n",
    "        self.swin = SwinModel(config)\n",
    "\n",
    "        # Bottleneck: input từ hidden_state[3] (config.embed_dim * 8 kênh)\n",
    "        # For Swin-Base, embed_dim=128, so config.embed_dim * 8 = 1024\n",
    "        self.bottleneck = ConvBlock(config.embed_dim * 8, config.embed_dim * 8)\n",
    "\n",
    "        # Decoder 4: in_channels từ bottleneck (1024), skip từ hidden_state[3] (1024 kênh), ra 512 kênh\n",
    "        self.decoder4 = DecoderBlock(in_channels=config.embed_dim * 8, skip_channels=config.embed_dim * 8, out_channels=config.embed_dim * 4) # 1024, 1024, 512\n",
    "        \n",
    "        # Decoder 3: in_channels từ decoder4 (512), skip từ hidden_state[2] (512 kênh), ra 256 kênh\n",
    "        self.decoder3 = DecoderBlock(in_channels=config.embed_dim * 4, skip_channels=config.embed_dim * 4, out_channels=config.embed_dim * 2) # 512, 512, 256\n",
    "        \n",
    "        # Decoder 2: in_channels từ decoder3 (256), skip từ hidden_state[1] (256 kênh), ra 128 kênh\n",
    "        self.decoder2 = DecoderBlock(in_channels=config.embed_dim * 2, skip_channels=config.embed_dim * 2, out_channels=config.embed_dim * 1) # 256, 256, 128\n",
    "        \n",
    "        # Decoder 1: in_channels từ decoder2 (128), skip từ hidden_state[0] (128 kênh), ra 64 kênh\n",
    "        self.decoder1 = DecoderBlock(in_channels=config.embed_dim * 1, skip_channels=config.embed_dim * 1, out_channels=config.embed_dim // 2) # 128, 128, 64\n",
    "        \n",
    "        # Final Upsample: upsample cuối cùng không có skip\n",
    "        self.final_upsample = DecoderBlock(in_channels=config.embed_dim // 2, skip_channels=0, out_channels=config.embed_dim // 4) # 64, 0, 32\n",
    "        self.final_conv = nn.Conv2d(config.embed_dim // 4, num_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.swin(pixel_values=x, output_hidden_states=True)\n",
    "        # encoder_features sẽ lưu các skip connections cần thiết\n",
    "        encoder_features = []\n",
    "\n",
    "        # Các hidden_states của Swin-Base:\n",
    "        # hidden_state[0]: (B, 4096, 128) -> reshape (B, 128, 64, 64)   (H/4)\n",
    "        # hidden_state[1]: (B, 1024, 256) -> reshape (B, 256, 32, 32) (H/8)\n",
    "        # hidden_state[2]: (B, 256, 512) -> reshape (B, 512, 16, 16) (H/16)\n",
    "        # hidden_state[3]: (B, 64, 1024) -> reshape (B, 1024, 8, 8)    (H/32)\n",
    "        # hidden_state[4]: (B, 64, 1024) -> after final LayerNorm\n",
    "\n",
    "        # Trích xuất và định hình lại các đặc trưng từ Swin Transformer cho các skip connection:\n",
    "        # Skip connection cho decoder1 (H/4): hidden_state[0]\n",
    "        hs_skip_res_H4 = outputs.hidden_states[0]\n",
    "        b, n, c = hs_skip_res_H4.shape\n",
    "        s = int(np.sqrt(n))\n",
    "        encoder_features.append(hs_skip_res_H4.permute(0, 2, 1).reshape(b, c, s, s)) # index 0\n",
    "\n",
    "        # Skip connection cho decoder2 (H/8): hidden_state[1]\n",
    "        hs_skip_res_H8 = outputs.hidden_states[1]\n",
    "        b, n, c = hs_skip_res_H8.shape\n",
    "        s = int(np.sqrt(n))\n",
    "        encoder_features.append(hs_skip_res_H8.permute(0, 2, 1).reshape(b, c, s, s)) # index 1\n",
    "\n",
    "        # Skip connection cho decoder3 (H/16): hidden_state[2]\n",
    "        hs_skip_res_H16 = outputs.hidden_states[2]\n",
    "        b, n, c = hs_skip_res_H16.shape\n",
    "        s = int(np.sqrt(n))\n",
    "        encoder_features.append(hs_skip_res_H16.permute(0, 2, 1).reshape(b, c, s, s)) # index 2\n",
    "\n",
    "        # Skip connection cho decoder4 (H/32): hidden_state[3]\n",
    "        hs_skip_res_H32 = outputs.hidden_states[3]\n",
    "        b, n, c = hs_skip_res_H32.shape\n",
    "        s = int(np.sqrt(n))\n",
    "        encoder_features.append(hs_skip_res_H32.permute(0, 2, 1).reshape(b, c, s, s)) # index 3\n",
    "\n",
    "        # Bottleneck feature: từ hidden_state[3] (config.embed_dim * 8 kênh, 8x8)\n",
    "        x_bottleneck = outputs.hidden_states[3]\n",
    "        b, n, c = x_bottleneck.shape\n",
    "        s = int(np.sqrt(n))\n",
    "        x_bottleneck = x_bottleneck.permute(0, 2, 1).reshape(b, c, s, s)\n",
    "        \n",
    "        x = self.bottleneck(x_bottleneck)\n",
    "\n",
    "        # Giải mã:\n",
    "        x = self.decoder4(x, encoder_features[3])\n",
    "        x = self.decoder3(x, encoder_features[2])\n",
    "        x = self.decoder2(x, encoder_features[1])\n",
    "        x = self.decoder1(x, encoder_features[0])\n",
    "        \n",
    "        x = self.final_upsample(x)\n",
    "        outputs = self.final_conv(x)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0cf243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(IoULoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs: prediction (after sigmoid)\n",
    "        # targets: ground truth\n",
    "        \n",
    "        # Áp dụng sigmoid cho outputs để có giá trị trong [0, 1]\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "        \n",
    "        # Flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = (inputs * targets).sum()\n",
    "        union = (inputs + targets).sum() - intersection\n",
    "        \n",
    "        iou = (intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        return 1 - iou # Trả về 1 - IoU để tối thiểu hóa loss\n",
    "\n",
    "def calculate_metrics(predicted_masks, true_masks, smooth=1e-6):\n",
    "    intersection = (predicted_masks * true_masks).sum()\n",
    "    union = (predicted_masks + true_masks).sum() - intersection\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    # Dice/F1-score cũng là một metric tốt để theo dõi\n",
    "    dice = (2. * intersection + smooth) / ((predicted_masks.sum() + true_masks.sum()) + smooth)\n",
    "    f1_score = dice \n",
    "\n",
    "    return iou.item(), f1_score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ab6232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, callbacks_config, start_epoch=0, best_val_loss_so_far=float('inf'), history=None):\n",
    "    best_val_loss = best_val_loss_so_far \n",
    "    patience_counter = 0\n",
    "    model_checkpoint_path = callbacks_config.get('checkpoint_path', 'swin_unet_best_pytorch.pth')\n",
    "    \n",
    "    # Khởi tạo hoặc tiếp tục từ history đã có\n",
    "    if history is None:\n",
    "        history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_iou': [], 'val_iou': [],\n",
    "            'train_f1': [], 'val_f1': [], # Vẫn giữ F1-score để theo dõi, dù loss chỉ là IoU\n",
    "            'lr': []\n",
    "        }\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_iou = 0.0\n",
    "        running_f1 = 0.0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} starting...\")\n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, masks) # criterion đã xử lý sigmoid bên trong cho IoU loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Để tính metrics, cần áp dụng sigmoid cho outputs và sau đó ngưỡng\n",
    "            predicted_masks = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            batch_iou, batch_f1 = calculate_metrics(predicted_masks, masks)\n",
    "            running_iou += batch_iou * images.size(0)\n",
    "            running_f1 += batch_f1 * images.size(0)\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_iou = running_iou / len(train_loader.dataset)\n",
    "        epoch_f1 = running_f1 / len(train_loader.dataset)\n",
    "        \n",
    "        # LƯU TRỮ TRAINING METRICS\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_iou'].append(epoch_iou)\n",
    "        history['train_f1'].append(epoch_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Finished - Train Loss: {epoch_loss:.4f}, Train IoU: {epoch_iou:.4f}, Train F1-Score: {epoch_f1:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_iou = 0.0\n",
    "        val_f1 = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(DEVICE)\n",
    "                masks = masks.to(DEVICE)\n",
    "\n",
    "                outputs = model(images) # outputs là logits\n",
    "                loss = criterion(outputs, masks) # criterion đã xử lý sigmoid bên trong\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                # Để tính metrics, cần áp dụng sigmoid cho outputs và sau đó ngưỡng\n",
    "                predicted_masks = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                \n",
    "                batch_iou, batch_f1 = calculate_metrics(predicted_masks, masks)\n",
    "                val_iou += batch_iou * images.size(0)\n",
    "                val_f1 += batch_f1 * images.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_iou /= len(val_loader.dataset)\n",
    "        val_f1 /= len(val_loader.dataset)\n",
    "        \n",
    "        # LƯU TRỮ VALIDATION METRICS\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_iou'].append(val_iou)\n",
    "        history['val_f1'].append(val_f1)\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation IoU: {val_iou:.4f}, Validation F1-Score: {val_f1:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['lr'].append(current_lr) # Lưu trữ Learning Rate\n",
    "        print(f\"Current Learning Rate: {current_lr:.8f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            print(f\"Best validation loss updated: {best_val_loss:.4f}. Saving model and state...\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'history': history # LƯU LỊCH SỬ HUẤN LUYỆN\n",
    "            }, model_checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation loss did not improve. Patience: {patience_counter}/{callbacks_config['patience']}\")\n",
    "            if patience_counter >= callbacks_config['patience']:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "    \n",
    "    return history # TRẢ VỀ LỊCH SỬ SAU KHI KẾT THÚC HUẤN LUYỆN\n",
    "\n",
    "# --- Hàm vẽ biểu đồ ---\n",
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Biểu đồ Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, history['train_loss'], 'o-', label='Train Loss')\n",
    "    plt.plot(epochs, history['val_loss'], 'o-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Biểu đồ IoU\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, history['train_iou'], 'o-', label='Train IoU')\n",
    "    plt.plot(epochs, history['val_iou'], 'o-', label='Validation IoU')\n",
    "    plt.title('Training and Validation IoU per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Biểu đồ F1-Score\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, history['train_f1'], 'o-', label='Train F1-Score')\n",
    "    plt.plot(epochs, history['val_f1'], 'o-', label='Validation F1-Score')\n",
    "    plt.title('Training and Validation F1-Score per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Biểu đồ Learning Rate (tùy chọn)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(epochs, history['lr'], 'o-', label='Learning Rate')\n",
    "    plt.title('Learning Rate per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1d4a870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinUNet(\n",
      "  (swin): SwinModel(\n",
      "    (embeddings): SwinEmbeddings(\n",
      "      (patch_embeddings): SwinPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): SwinEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.004347826354205608)\n",
      "              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): SwinPatchMerging(\n",
      "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.008695652708411217)\n",
      "              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.013043479062616825)\n",
      "              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): SwinPatchMerging(\n",
      "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
      "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.017391305416822433)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.021739132702350616)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.02608695812523365)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.030434783548116684)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.03478261083364487)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.03913043811917305)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.04347826540470123)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.04782608896493912)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.052173912525177)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.056521736085414886)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.06086956337094307)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.06521739065647125)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (12): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.06956522166728973)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (13): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.07391304522752762)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (14): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.0782608687877655)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (15): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.08260869979858398)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (16): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.08695652335882187)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (17): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.09130434691905975)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): SwinPatchMerging(\n",
      "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.09565217792987823)\n",
      "              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.10000000149011612)\n",
      "              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
      "  )\n",
      "  (bottleneck): ConvBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder4): DecoderBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(1536, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder3): DecoderBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder2): DecoderBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder1): DecoderBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_upsample): DecoderBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Checkpoint found at swin_unet_base_IoULoss.pth. Loading to resume training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_29256\\3277623425.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduler state loaded.\n",
      "Training history loaded from checkpoint.\n",
      "Loaded checkpoint from Epoch 62. Resuming training from Epoch 63.\n",
      "Previous best validation loss: 0.6030\n",
      "\n",
      "Starting Swin-Unet Base model training (IoU Loss only)...\n",
      "Epoch 64/10000 starting...\n",
      "Epoch 64/10000, Batch 10/188, Loss: 0.5361\n",
      "Epoch 64/10000, Batch 20/188, Loss: 0.6137\n",
      "Epoch 64/10000, Batch 30/188, Loss: 0.2431\n",
      "Epoch 64/10000, Batch 40/188, Loss: 0.3737\n",
      "Epoch 64/10000, Batch 50/188, Loss: 0.1885\n",
      "Epoch 64/10000, Batch 60/188, Loss: 0.5136\n",
      "Epoch 64/10000, Batch 70/188, Loss: 0.5317\n",
      "Epoch 64/10000, Batch 80/188, Loss: 0.4668\n",
      "Epoch 64/10000, Batch 90/188, Loss: 0.3307\n",
      "Epoch 64/10000, Batch 100/188, Loss: 0.5246\n",
      "Epoch 64/10000, Batch 110/188, Loss: 0.4413\n",
      "Epoch 64/10000, Batch 120/188, Loss: 0.3327\n",
      "Epoch 64/10000, Batch 130/188, Loss: 0.5530\n",
      "Epoch 64/10000, Batch 140/188, Loss: 0.4211\n",
      "Epoch 64/10000, Batch 150/188, Loss: 0.3815\n",
      "Epoch 64/10000, Batch 160/188, Loss: 0.2621\n",
      "Epoch 64/10000, Batch 170/188, Loss: 0.5803\n",
      "Epoch 64/10000, Batch 180/188, Loss: 0.5833\n",
      "Epoch 64 Finished - Train Loss: 0.4216, Train IoU: 0.5803, Train F1-Score: 0.7274\n",
      "Validation Loss: 0.5683, Validation IoU: 0.4330, Validation F1-Score: 0.5736\n",
      "Current Learning Rate: 0.00000313\n",
      "Best validation loss updated: 0.5683. Saving model and state...\n",
      "Epoch 65/10000 starting...\n",
      "Epoch 65/10000, Batch 10/188, Loss: 0.3194\n",
      "Epoch 65/10000, Batch 20/188, Loss: 0.2722\n",
      "Epoch 65/10000, Batch 30/188, Loss: 0.4937\n",
      "Epoch 65/10000, Batch 40/188, Loss: 0.4633\n",
      "Epoch 65/10000, Batch 50/188, Loss: 0.3036\n",
      "Epoch 65/10000, Batch 60/188, Loss: 0.2957\n",
      "Epoch 65/10000, Batch 70/188, Loss: 0.2981\n",
      "Epoch 65/10000, Batch 80/188, Loss: 0.4752\n",
      "Epoch 65/10000, Batch 90/188, Loss: 0.7638\n",
      "Epoch 65/10000, Batch 100/188, Loss: 0.2268\n",
      "Epoch 65/10000, Batch 110/188, Loss: 0.3864\n",
      "Epoch 65/10000, Batch 120/188, Loss: 0.5333\n",
      "Epoch 65/10000, Batch 130/188, Loss: 0.3243\n",
      "Epoch 65/10000, Batch 140/188, Loss: 0.3670\n",
      "Epoch 65/10000, Batch 150/188, Loss: 0.1937\n",
      "Epoch 65/10000, Batch 160/188, Loss: 0.5993\n",
      "Epoch 65/10000, Batch 170/188, Loss: 0.3867\n",
      "Epoch 65/10000, Batch 180/188, Loss: 0.6177\n",
      "Epoch 65 Finished - Train Loss: 0.4213, Train IoU: 0.5807, Train F1-Score: 0.7278\n",
      "Validation Loss: 0.5665, Validation IoU: 0.4350, Validation F1-Score: 0.5759\n",
      "Current Learning Rate: 0.00000313\n",
      "Best validation loss updated: 0.5665. Saving model and state...\n",
      "Epoch 66/10000 starting...\n",
      "Epoch 66/10000, Batch 10/188, Loss: 0.2899\n",
      "Epoch 66/10000, Batch 20/188, Loss: 0.3543\n",
      "Epoch 66/10000, Batch 30/188, Loss: 0.4400\n",
      "Epoch 66/10000, Batch 40/188, Loss: 0.3553\n",
      "Epoch 66/10000, Batch 50/188, Loss: 0.3104\n",
      "Epoch 66/10000, Batch 60/188, Loss: 0.3540\n",
      "Epoch 66/10000, Batch 70/188, Loss: 0.4589\n",
      "Epoch 66/10000, Batch 80/188, Loss: 0.3012\n",
      "Epoch 66/10000, Batch 90/188, Loss: 0.4459\n",
      "Epoch 66/10000, Batch 100/188, Loss: 0.2582\n",
      "Epoch 66/10000, Batch 110/188, Loss: 0.5695\n",
      "Epoch 66/10000, Batch 120/188, Loss: 0.3266\n",
      "Epoch 66/10000, Batch 130/188, Loss: 0.4454\n",
      "Epoch 66/10000, Batch 140/188, Loss: 0.4879\n",
      "Epoch 66/10000, Batch 150/188, Loss: 0.3709\n",
      "Epoch 66/10000, Batch 160/188, Loss: 0.3924\n",
      "Epoch 66/10000, Batch 170/188, Loss: 0.4044\n",
      "Epoch 66/10000, Batch 180/188, Loss: 0.2831\n",
      "Epoch 66 Finished - Train Loss: 0.4209, Train IoU: 0.5809, Train F1-Score: 0.7278\n",
      "Validation Loss: 0.5663, Validation IoU: 0.4351, Validation F1-Score: 0.5763\n",
      "Current Learning Rate: 0.00000313\n",
      "Best validation loss updated: 0.5663. Saving model and state...\n",
      "Epoch 67/10000 starting...\n",
      "Epoch 67/10000, Batch 10/188, Loss: 0.4420\n",
      "Epoch 67/10000, Batch 20/188, Loss: 0.3441\n",
      "Epoch 67/10000, Batch 30/188, Loss: 0.2599\n",
      "Epoch 67/10000, Batch 40/188, Loss: 0.2955\n",
      "Epoch 67/10000, Batch 50/188, Loss: 0.3699\n",
      "Epoch 67/10000, Batch 60/188, Loss: 0.3522\n",
      "Epoch 67/10000, Batch 70/188, Loss: 0.1754\n",
      "Epoch 67/10000, Batch 80/188, Loss: 0.3916\n",
      "Epoch 67/10000, Batch 90/188, Loss: 0.7274\n",
      "Epoch 67/10000, Batch 100/188, Loss: 0.2745\n",
      "Epoch 67/10000, Batch 110/188, Loss: 0.4101\n",
      "Epoch 67/10000, Batch 120/188, Loss: 0.4029\n",
      "Epoch 67/10000, Batch 130/188, Loss: 0.2791\n",
      "Epoch 67/10000, Batch 140/188, Loss: 0.4374\n",
      "Epoch 67/10000, Batch 150/188, Loss: 0.4092\n",
      "Epoch 67/10000, Batch 160/188, Loss: 0.5219\n",
      "Epoch 67/10000, Batch 170/188, Loss: 0.3627\n",
      "Epoch 67/10000, Batch 180/188, Loss: 0.5017\n",
      "Epoch 67 Finished - Train Loss: 0.4169, Train IoU: 0.5849, Train F1-Score: 0.7319\n",
      "Validation Loss: 0.5665, Validation IoU: 0.4348, Validation F1-Score: 0.5762\n",
      "Current Learning Rate: 0.00000313\n",
      "Validation loss did not improve. Patience: 1/30\n",
      "Epoch 68/10000 starting...\n",
      "Epoch 68/10000, Batch 10/188, Loss: 0.2457\n",
      "Epoch 68/10000, Batch 20/188, Loss: 0.3709\n",
      "Epoch 68/10000, Batch 30/188, Loss: 0.3758\n",
      "Epoch 68/10000, Batch 40/188, Loss: 0.4070\n",
      "Epoch 68/10000, Batch 50/188, Loss: 0.3677\n",
      "Epoch 68/10000, Batch 60/188, Loss: 0.3992\n",
      "Epoch 68/10000, Batch 70/188, Loss: 0.4141\n",
      "Epoch 68/10000, Batch 80/188, Loss: 0.3616\n",
      "Epoch 68/10000, Batch 90/188, Loss: 0.2582\n",
      "Epoch 68/10000, Batch 100/188, Loss: 0.5014\n",
      "Epoch 68/10000, Batch 110/188, Loss: 0.4028\n",
      "Epoch 68/10000, Batch 120/188, Loss: 0.3414\n",
      "Epoch 68/10000, Batch 130/188, Loss: 0.4694\n",
      "Epoch 68/10000, Batch 140/188, Loss: 0.3425\n",
      "Epoch 68/10000, Batch 150/188, Loss: 0.4086\n",
      "Epoch 68/10000, Batch 160/188, Loss: 0.3892\n",
      "Epoch 68/10000, Batch 170/188, Loss: 0.3887\n",
      "Epoch 68/10000, Batch 180/188, Loss: 0.2953\n",
      "Epoch 68 Finished - Train Loss: 0.4175, Train IoU: 0.5843, Train F1-Score: 0.7319\n",
      "Validation Loss: 0.5681, Validation IoU: 0.4333, Validation F1-Score: 0.5741\n",
      "Current Learning Rate: 0.00000313\n",
      "Validation loss did not improve. Patience: 2/30\n",
      "Epoch 69/10000 starting...\n",
      "Epoch 69/10000, Batch 10/188, Loss: 0.4979\n",
      "Epoch 69/10000, Batch 20/188, Loss: 0.4028\n",
      "Epoch 69/10000, Batch 30/188, Loss: 0.4273\n",
      "Epoch 69/10000, Batch 40/188, Loss: 0.5040\n",
      "Epoch 69/10000, Batch 50/188, Loss: 0.4436\n",
      "Epoch 69/10000, Batch 60/188, Loss: 0.3645\n",
      "Epoch 69/10000, Batch 70/188, Loss: 0.4991\n",
      "Epoch 69/10000, Batch 80/188, Loss: 0.2306\n",
      "Epoch 69/10000, Batch 90/188, Loss: 0.3335\n",
      "Epoch 69/10000, Batch 100/188, Loss: 0.3258\n",
      "Epoch 69/10000, Batch 110/188, Loss: 0.4371\n",
      "Epoch 69/10000, Batch 120/188, Loss: 0.2681\n",
      "Epoch 69/10000, Batch 130/188, Loss: 0.6954\n",
      "Epoch 69/10000, Batch 140/188, Loss: 0.5169\n",
      "Epoch 69/10000, Batch 150/188, Loss: 0.3840\n",
      "Epoch 69/10000, Batch 160/188, Loss: 0.5260\n",
      "Epoch 69/10000, Batch 170/188, Loss: 0.3981\n",
      "Epoch 69/10000, Batch 180/188, Loss: 0.3123\n",
      "Epoch 69 Finished - Train Loss: 0.4128, Train IoU: 0.5890, Train F1-Score: 0.7348\n",
      "Validation Loss: 0.5675, Validation IoU: 0.4339, Validation F1-Score: 0.5757\n",
      "Current Learning Rate: 0.00000313\n",
      "Validation loss did not improve. Patience: 3/30\n",
      "Epoch 70/10000 starting...\n",
      "Epoch 70/10000, Batch 10/188, Loss: 0.3155\n",
      "Epoch 70/10000, Batch 20/188, Loss: 0.3453\n",
      "Epoch 70/10000, Batch 30/188, Loss: 0.3223\n",
      "Epoch 70/10000, Batch 40/188, Loss: 0.4387\n",
      "Epoch 70/10000, Batch 50/188, Loss: 0.4853\n",
      "Epoch 70/10000, Batch 60/188, Loss: 0.1629\n",
      "Epoch 70/10000, Batch 70/188, Loss: 0.1905\n",
      "Epoch 70/10000, Batch 80/188, Loss: 0.5099\n",
      "Epoch 70/10000, Batch 90/188, Loss: 0.3142\n",
      "Epoch 70/10000, Batch 100/188, Loss: 0.5885\n",
      "Epoch 70/10000, Batch 110/188, Loss: 0.4241\n",
      "Epoch 70/10000, Batch 120/188, Loss: 0.5927\n",
      "Epoch 70/10000, Batch 130/188, Loss: 0.4572\n",
      "Epoch 70/10000, Batch 140/188, Loss: 0.4526\n",
      "Epoch 70/10000, Batch 150/188, Loss: 0.4988\n",
      "Epoch 70/10000, Batch 160/188, Loss: 0.5350\n",
      "Epoch 70/10000, Batch 170/188, Loss: 0.2581\n",
      "Epoch 70/10000, Batch 180/188, Loss: 0.4423\n",
      "Epoch 70 Finished - Train Loss: 0.4175, Train IoU: 0.5843, Train F1-Score: 0.7309\n",
      "Validation Loss: 0.5661, Validation IoU: 0.4353, Validation F1-Score: 0.5766\n",
      "Current Learning Rate: 0.00000313\n",
      "Best validation loss updated: 0.5661. Saving model and state...\n",
      "Epoch 71/10000 starting...\n",
      "Epoch 71/10000, Batch 10/188, Loss: 0.6661\n",
      "Epoch 71/10000, Batch 20/188, Loss: 0.5541\n",
      "Epoch 71/10000, Batch 30/188, Loss: 0.5530\n",
      "Epoch 71/10000, Batch 40/188, Loss: 0.4225\n",
      "Epoch 71/10000, Batch 50/188, Loss: 0.3302\n",
      "Epoch 71/10000, Batch 60/188, Loss: 0.3566\n",
      "Epoch 71/10000, Batch 70/188, Loss: 0.3458\n",
      "Epoch 71/10000, Batch 80/188, Loss: 0.5811\n",
      "Epoch 71/10000, Batch 90/188, Loss: 0.5689\n",
      "Epoch 71/10000, Batch 100/188, Loss: 0.4991\n",
      "Epoch 71/10000, Batch 110/188, Loss: 0.4095\n",
      "Epoch 71/10000, Batch 120/188, Loss: 0.5290\n",
      "Epoch 71/10000, Batch 130/188, Loss: 0.4548\n",
      "Epoch 71/10000, Batch 140/188, Loss: 0.3292\n",
      "Epoch 71/10000, Batch 150/188, Loss: 0.5731\n",
      "Epoch 71/10000, Batch 160/188, Loss: 0.2438\n",
      "Epoch 71/10000, Batch 170/188, Loss: 0.3262\n",
      "Epoch 71/10000, Batch 180/188, Loss: 0.1453\n",
      "Epoch 71 Finished - Train Loss: 0.4168, Train IoU: 0.5849, Train F1-Score: 0.7314\n",
      "Validation Loss: 0.5669, Validation IoU: 0.4344, Validation F1-Score: 0.5753\n",
      "Current Learning Rate: 0.00000313\n",
      "Validation loss did not improve. Patience: 1/30\n",
      "Epoch 72/10000 starting...\n",
      "Epoch 72/10000, Batch 10/188, Loss: 0.3502\n",
      "Epoch 72/10000, Batch 20/188, Loss: 0.2556\n",
      "Epoch 72/10000, Batch 30/188, Loss: 0.4110\n",
      "Epoch 72/10000, Batch 40/188, Loss: 0.4685\n",
      "Epoch 72/10000, Batch 50/188, Loss: 0.4222\n",
      "Epoch 72/10000, Batch 60/188, Loss: 0.1966\n",
      "Epoch 72/10000, Batch 70/188, Loss: 0.3228\n",
      "Epoch 72/10000, Batch 80/188, Loss: 0.5641\n",
      "Epoch 72/10000, Batch 90/188, Loss: 0.4898\n",
      "Epoch 72/10000, Batch 100/188, Loss: 0.4262\n",
      "Epoch 72/10000, Batch 110/188, Loss: 0.4186\n",
      "Epoch 72/10000, Batch 120/188, Loss: 0.4511\n",
      "Epoch 72/10000, Batch 130/188, Loss: 0.5932\n",
      "Epoch 72/10000, Batch 140/188, Loss: 0.5555\n",
      "Epoch 72/10000, Batch 150/188, Loss: 0.4418\n",
      "Epoch 72/10000, Batch 160/188, Loss: 0.4046\n",
      "Epoch 72/10000, Batch 170/188, Loss: 0.4714\n",
      "Epoch 72/10000, Batch 180/188, Loss: 0.3914\n",
      "Epoch 72 Finished - Train Loss: 0.4172, Train IoU: 0.5846, Train F1-Score: 0.7316\n",
      "Validation Loss: 0.5684, Validation IoU: 0.4331, Validation F1-Score: 0.5743\n",
      "Current Learning Rate: 0.00000313\n",
      "Validation loss did not improve. Patience: 2/30\n",
      "Epoch 73/10000 starting...\n",
      "Epoch 73/10000, Batch 10/188, Loss: 0.5751\n",
      "Epoch 73/10000, Batch 20/188, Loss: 0.3544\n",
      "Epoch 73/10000, Batch 30/188, Loss: 0.6567\n",
      "Epoch 73/10000, Batch 40/188, Loss: 0.5863\n",
      "Epoch 73/10000, Batch 50/188, Loss: 0.4244\n",
      "Epoch 73/10000, Batch 60/188, Loss: 0.4535\n",
      "Epoch 73/10000, Batch 70/188, Loss: 0.2748\n",
      "Epoch 73/10000, Batch 80/188, Loss: 0.5009\n",
      "Epoch 73/10000, Batch 90/188, Loss: 0.2067\n",
      "Epoch 73/10000, Batch 100/188, Loss: 0.4273\n",
      "Epoch 73/10000, Batch 110/188, Loss: 0.4116\n",
      "Epoch 73/10000, Batch 120/188, Loss: 0.4443\n",
      "Epoch 73/10000, Batch 130/188, Loss: 0.4022\n",
      "Epoch 73/10000, Batch 140/188, Loss: 0.3970\n",
      "Epoch 73/10000, Batch 150/188, Loss: 0.2387\n",
      "Epoch 73/10000, Batch 160/188, Loss: 0.5134\n",
      "Epoch 73/10000, Batch 170/188, Loss: 0.2323\n",
      "Epoch 73/10000, Batch 180/188, Loss: 0.3460\n",
      "Epoch 73 Finished - Train Loss: 0.4134, Train IoU: 0.5884, Train F1-Score: 0.7344\n",
      "Validation Loss: 0.5681, Validation IoU: 0.4332, Validation F1-Score: 0.5748\n",
      "Current Learning Rate: 0.00000313\n",
      "Validation loss did not improve. Patience: 3/30\n",
      "Epoch 74/10000 starting...\n",
      "Epoch 74/10000, Batch 10/188, Loss: 0.6000\n",
      "Epoch 74/10000, Batch 20/188, Loss: 0.5427\n",
      "Epoch 74/10000, Batch 30/188, Loss: 0.4015\n",
      "Epoch 74/10000, Batch 40/188, Loss: 0.5483\n",
      "Epoch 74/10000, Batch 50/188, Loss: 0.5446\n",
      "Epoch 74/10000, Batch 60/188, Loss: 0.3978\n",
      "Epoch 74/10000, Batch 70/188, Loss: 0.7599\n",
      "Epoch 74/10000, Batch 80/188, Loss: 0.4214\n",
      "Epoch 74/10000, Batch 90/188, Loss: 0.2975\n",
      "Epoch 74/10000, Batch 100/188, Loss: 0.5082\n",
      "Epoch 74/10000, Batch 110/188, Loss: 0.3868\n",
      "Epoch 74/10000, Batch 120/188, Loss: 0.2956\n",
      "Epoch 74/10000, Batch 130/188, Loss: 0.3986\n",
      "Epoch 74/10000, Batch 140/188, Loss: 0.2833\n",
      "Epoch 74/10000, Batch 150/188, Loss: 0.3955\n",
      "Epoch 74/10000, Batch 160/188, Loss: 0.3273\n",
      "Epoch 74/10000, Batch 170/188, Loss: 0.6088\n",
      "Epoch 74/10000, Batch 180/188, Loss: 0.3895\n",
      "Epoch 74 Finished - Train Loss: 0.4122, Train IoU: 0.5895, Train F1-Score: 0.7348\n",
      "Validation Loss: 0.5664, Validation IoU: 0.4349, Validation F1-Score: 0.5762\n",
      "Current Learning Rate: 0.00000156\n",
      "Validation loss did not improve. Patience: 4/30\n",
      "Epoch 75/10000 starting...\n",
      "Epoch 75/10000, Batch 10/188, Loss: 0.4410\n",
      "Epoch 75/10000, Batch 20/188, Loss: 0.4742\n",
      "Epoch 75/10000, Batch 30/188, Loss: 0.5114\n",
      "Epoch 75/10000, Batch 40/188, Loss: 0.4209\n",
      "Epoch 75/10000, Batch 50/188, Loss: 0.3726\n",
      "Epoch 75/10000, Batch 60/188, Loss: 0.5699\n",
      "Epoch 75/10000, Batch 70/188, Loss: 0.4318\n",
      "Epoch 75/10000, Batch 80/188, Loss: 0.2752\n",
      "Epoch 75/10000, Batch 90/188, Loss: 0.4003\n",
      "Epoch 75/10000, Batch 100/188, Loss: 0.3702\n",
      "Epoch 75/10000, Batch 110/188, Loss: 0.3933\n",
      "Epoch 75/10000, Batch 120/188, Loss: 0.2291\n",
      "Epoch 75/10000, Batch 130/188, Loss: 0.6294\n",
      "Epoch 75/10000, Batch 140/188, Loss: 0.5852\n",
      "Epoch 75/10000, Batch 150/188, Loss: 0.3523\n",
      "Epoch 75/10000, Batch 160/188, Loss: 0.3798\n",
      "Epoch 75/10000, Batch 170/188, Loss: 0.4815\n",
      "Epoch 75/10000, Batch 180/188, Loss: 0.3511\n",
      "Epoch 75 Finished - Train Loss: 0.4153, Train IoU: 0.5864, Train F1-Score: 0.7334\n",
      "Validation Loss: 0.5681, Validation IoU: 0.4332, Validation F1-Score: 0.5736\n",
      "Current Learning Rate: 0.00000156\n",
      "Validation loss did not improve. Patience: 5/30\n",
      "Epoch 76/10000 starting...\n",
      "Epoch 76/10000, Batch 10/188, Loss: 0.3637\n",
      "Epoch 76/10000, Batch 20/188, Loss: 0.3804\n",
      "Epoch 76/10000, Batch 30/188, Loss: 0.4234\n",
      "Epoch 76/10000, Batch 40/188, Loss: 0.3012\n",
      "Epoch 76/10000, Batch 50/188, Loss: 0.4982\n",
      "Epoch 76/10000, Batch 60/188, Loss: 0.3033\n",
      "Epoch 76/10000, Batch 70/188, Loss: 0.4520\n",
      "Epoch 76/10000, Batch 80/188, Loss: 0.6318\n",
      "Epoch 76/10000, Batch 90/188, Loss: 0.3629\n",
      "Epoch 76/10000, Batch 100/188, Loss: 0.3297\n",
      "Epoch 76/10000, Batch 110/188, Loss: 0.3391\n",
      "Epoch 76/10000, Batch 120/188, Loss: 0.3115\n",
      "Epoch 76/10000, Batch 130/188, Loss: 0.3294\n",
      "Epoch 76/10000, Batch 140/188, Loss: 0.4005\n",
      "Epoch 76/10000, Batch 150/188, Loss: 0.3790\n",
      "Epoch 76/10000, Batch 160/188, Loss: 0.4429\n",
      "Epoch 76/10000, Batch 170/188, Loss: 0.5261\n",
      "Epoch 76/10000, Batch 180/188, Loss: 0.4207\n",
      "Epoch 76 Finished - Train Loss: 0.4155, Train IoU: 0.5862, Train F1-Score: 0.7334\n",
      "Validation Loss: 0.5642, Validation IoU: 0.4370, Validation F1-Score: 0.5784\n",
      "Current Learning Rate: 0.00000156\n",
      "Best validation loss updated: 0.5642. Saving model and state...\n",
      "Epoch 77/10000 starting...\n",
      "Epoch 77/10000, Batch 10/188, Loss: 0.7724\n",
      "Epoch 77/10000, Batch 20/188, Loss: 0.1903\n",
      "Epoch 77/10000, Batch 30/188, Loss: 0.3186\n",
      "Epoch 77/10000, Batch 40/188, Loss: 0.4115\n",
      "Epoch 77/10000, Batch 50/188, Loss: 0.5091\n",
      "Epoch 77/10000, Batch 60/188, Loss: 0.4469\n",
      "Epoch 77/10000, Batch 70/188, Loss: 0.6454\n",
      "Epoch 77/10000, Batch 80/188, Loss: 0.4148\n",
      "Epoch 77/10000, Batch 90/188, Loss: 0.2826\n",
      "Epoch 77/10000, Batch 100/188, Loss: 0.4054\n",
      "Epoch 77/10000, Batch 110/188, Loss: 0.4780\n",
      "Epoch 77/10000, Batch 120/188, Loss: 0.4041\n",
      "Epoch 77/10000, Batch 130/188, Loss: 0.5927\n",
      "Epoch 77/10000, Batch 140/188, Loss: 0.5557\n",
      "Epoch 77/10000, Batch 150/188, Loss: 0.4005\n",
      "Epoch 77/10000, Batch 160/188, Loss: 0.6659\n",
      "Epoch 77/10000, Batch 170/188, Loss: 0.6145\n",
      "Epoch 77/10000, Batch 180/188, Loss: 0.1844\n",
      "Epoch 77 Finished - Train Loss: 0.4188, Train IoU: 0.5830, Train F1-Score: 0.7300\n",
      "Validation Loss: 0.5647, Validation IoU: 0.4365, Validation F1-Score: 0.5783\n",
      "Current Learning Rate: 0.00000156\n",
      "Validation loss did not improve. Patience: 1/30\n",
      "Epoch 78/10000 starting...\n",
      "Epoch 78/10000, Batch 10/188, Loss: 0.5246\n",
      "Epoch 78/10000, Batch 20/188, Loss: 0.3286\n",
      "Epoch 78/10000, Batch 30/188, Loss: 0.3955\n",
      "Epoch 78/10000, Batch 40/188, Loss: 0.2539\n",
      "Epoch 78/10000, Batch 50/188, Loss: 0.3774\n",
      "Epoch 78/10000, Batch 60/188, Loss: 0.4486\n",
      "Epoch 78/10000, Batch 70/188, Loss: 0.5156\n",
      "Epoch 78/10000, Batch 80/188, Loss: 0.3560\n",
      "Epoch 78/10000, Batch 90/188, Loss: 0.3915\n",
      "Epoch 78/10000, Batch 100/188, Loss: 0.3696\n",
      "Epoch 78/10000, Batch 110/188, Loss: 0.4516\n",
      "Epoch 78/10000, Batch 120/188, Loss: 0.5809\n",
      "Epoch 78/10000, Batch 130/188, Loss: 0.3579\n",
      "Epoch 78/10000, Batch 140/188, Loss: 0.3749\n",
      "Epoch 78/10000, Batch 150/188, Loss: 0.3194\n",
      "Epoch 78/10000, Batch 160/188, Loss: 0.7811\n",
      "Epoch 78/10000, Batch 170/188, Loss: 0.2164\n",
      "Epoch 78/10000, Batch 180/188, Loss: 0.3728\n",
      "Epoch 78 Finished - Train Loss: 0.4139, Train IoU: 0.5878, Train F1-Score: 0.7335\n",
      "Validation Loss: 0.5674, Validation IoU: 0.4339, Validation F1-Score: 0.5749\n",
      "Current Learning Rate: 0.00000156\n",
      "Validation loss did not improve. Patience: 2/30\n",
      "Epoch 79/10000 starting...\n",
      "Epoch 79/10000, Batch 10/188, Loss: 0.3880\n",
      "Epoch 79/10000, Batch 20/188, Loss: 0.4062\n",
      "Epoch 79/10000, Batch 30/188, Loss: 0.5190\n",
      "Epoch 79/10000, Batch 40/188, Loss: 0.2666\n",
      "Epoch 79/10000, Batch 50/188, Loss: 0.3785\n",
      "Epoch 79/10000, Batch 60/188, Loss: 0.3921\n",
      "Epoch 79/10000, Batch 70/188, Loss: 0.4692\n",
      "Epoch 79/10000, Batch 80/188, Loss: 0.4449\n",
      "Epoch 79/10000, Batch 90/188, Loss: 0.4155\n",
      "Epoch 79/10000, Batch 100/188, Loss: 0.4162\n",
      "Epoch 79/10000, Batch 110/188, Loss: 0.3580\n",
      "Epoch 79/10000, Batch 120/188, Loss: 0.1971\n",
      "Epoch 79/10000, Batch 130/188, Loss: 0.4666\n",
      "Epoch 79/10000, Batch 140/188, Loss: 0.3540\n",
      "Epoch 79/10000, Batch 150/188, Loss: 0.3383\n",
      "Epoch 79/10000, Batch 160/188, Loss: 0.3800\n",
      "Epoch 79/10000, Batch 170/188, Loss: 0.3788\n",
      "Epoch 79/10000, Batch 180/188, Loss: 0.4419\n",
      "Epoch 79 Finished - Train Loss: 0.4130, Train IoU: 0.5887, Train F1-Score: 0.7361\n",
      "Validation Loss: 0.5658, Validation IoU: 0.4355, Validation F1-Score: 0.5764\n",
      "Current Learning Rate: 0.00000156\n",
      "Validation loss did not improve. Patience: 3/30\n",
      "Epoch 80/10000 starting...\n",
      "Epoch 80/10000, Batch 10/188, Loss: 0.3366\n",
      "Epoch 80/10000, Batch 20/188, Loss: 0.4636\n",
      "Epoch 80/10000, Batch 30/188, Loss: 0.2661\n",
      "Epoch 80/10000, Batch 40/188, Loss: 0.4992\n",
      "Epoch 80/10000, Batch 50/188, Loss: 0.3356\n",
      "Epoch 80/10000, Batch 60/188, Loss: 0.3264\n",
      "Epoch 80/10000, Batch 70/188, Loss: 0.3024\n",
      "Epoch 80/10000, Batch 80/188, Loss: 0.4230\n",
      "Epoch 80/10000, Batch 90/188, Loss: 0.3518\n",
      "Epoch 80/10000, Batch 100/188, Loss: 0.2970\n",
      "Epoch 80/10000, Batch 110/188, Loss: 0.2484\n",
      "Epoch 80/10000, Batch 120/188, Loss: 0.4689\n",
      "Epoch 80/10000, Batch 130/188, Loss: 0.6383\n",
      "Epoch 80/10000, Batch 140/188, Loss: 0.4770\n",
      "Epoch 80/10000, Batch 150/188, Loss: 0.2663\n",
      "Epoch 80/10000, Batch 160/188, Loss: 0.3651\n",
      "Epoch 80/10000, Batch 170/188, Loss: 0.4483\n",
      "Epoch 80/10000, Batch 180/188, Loss: 0.3241\n",
      "Epoch 80 Finished - Train Loss: 0.4136, Train IoU: 0.5881, Train F1-Score: 0.7343\n",
      "Validation Loss: 0.5682, Validation IoU: 0.4330, Validation F1-Score: 0.5739\n",
      "Current Learning Rate: 0.00000078\n",
      "Validation loss did not improve. Patience: 4/30\n",
      "Epoch 81/10000 starting...\n",
      "Epoch 81/10000, Batch 10/188, Loss: 0.5579\n",
      "Epoch 81/10000, Batch 20/188, Loss: 0.3812\n",
      "Epoch 81/10000, Batch 30/188, Loss: 0.5159\n",
      "Epoch 81/10000, Batch 40/188, Loss: 0.5541\n",
      "Epoch 81/10000, Batch 50/188, Loss: 0.4358\n",
      "Epoch 81/10000, Batch 60/188, Loss: 0.4111\n",
      "Epoch 81/10000, Batch 70/188, Loss: 0.2863\n",
      "Epoch 81/10000, Batch 80/188, Loss: 0.5897\n",
      "Epoch 81/10000, Batch 90/188, Loss: 0.5722\n",
      "Epoch 81/10000, Batch 100/188, Loss: 0.4734\n",
      "Epoch 81/10000, Batch 110/188, Loss: 0.3936\n",
      "Epoch 81/10000, Batch 120/188, Loss: 0.2239\n",
      "Epoch 81/10000, Batch 130/188, Loss: 0.3751\n",
      "Epoch 81/10000, Batch 140/188, Loss: 0.4737\n",
      "Epoch 81/10000, Batch 150/188, Loss: 0.5504\n",
      "Epoch 81/10000, Batch 160/188, Loss: 0.6696\n",
      "Epoch 81/10000, Batch 170/188, Loss: 0.3593\n",
      "Epoch 81/10000, Batch 180/188, Loss: 0.4232\n",
      "Epoch 81 Finished - Train Loss: 0.4132, Train IoU: 0.5885, Train F1-Score: 0.7342\n",
      "Validation Loss: 0.5674, Validation IoU: 0.4339, Validation F1-Score: 0.5745\n",
      "Current Learning Rate: 0.00000078\n",
      "Validation loss did not improve. Patience: 5/30\n",
      "Epoch 82/10000 starting...\n",
      "Epoch 82/10000, Batch 10/188, Loss: 0.5925\n",
      "Epoch 82/10000, Batch 20/188, Loss: 0.2991\n",
      "Epoch 82/10000, Batch 30/188, Loss: 0.2689\n",
      "Epoch 82/10000, Batch 40/188, Loss: 0.4407\n",
      "Epoch 82/10000, Batch 50/188, Loss: 0.4911\n",
      "Epoch 82/10000, Batch 60/188, Loss: 0.3078\n",
      "Epoch 82/10000, Batch 70/188, Loss: 0.6758\n",
      "Epoch 82/10000, Batch 80/188, Loss: 0.2673\n",
      "Epoch 82/10000, Batch 90/188, Loss: 0.6356\n",
      "Epoch 82/10000, Batch 100/188, Loss: 0.5239\n",
      "Epoch 82/10000, Batch 110/188, Loss: 0.2671\n",
      "Epoch 82/10000, Batch 120/188, Loss: 0.3472\n",
      "Epoch 82/10000, Batch 130/188, Loss: 0.4564\n",
      "Epoch 82/10000, Batch 140/188, Loss: 0.4607\n",
      "Epoch 82/10000, Batch 150/188, Loss: 0.3485\n",
      "Epoch 82/10000, Batch 160/188, Loss: 0.3357\n",
      "Epoch 82/10000, Batch 170/188, Loss: 0.3406\n",
      "Epoch 82/10000, Batch 180/188, Loss: 0.4194\n",
      "Epoch 82 Finished - Train Loss: 0.4149, Train IoU: 0.5868, Train F1-Score: 0.7335\n",
      "Validation Loss: 0.5658, Validation IoU: 0.4353, Validation F1-Score: 0.5771\n",
      "Current Learning Rate: 0.00000078\n",
      "Validation loss did not improve. Patience: 6/30\n",
      "Epoch 83/10000 starting...\n",
      "Epoch 83/10000, Batch 10/188, Loss: 0.4186\n",
      "Epoch 83/10000, Batch 20/188, Loss: 0.7832\n",
      "Epoch 83/10000, Batch 30/188, Loss: 0.5610\n",
      "Epoch 83/10000, Batch 40/188, Loss: 0.5005\n",
      "Epoch 83/10000, Batch 50/188, Loss: 0.3907\n",
      "Epoch 83/10000, Batch 60/188, Loss: 0.4410\n",
      "Epoch 83/10000, Batch 70/188, Loss: 0.2988\n",
      "Epoch 83/10000, Batch 80/188, Loss: 0.2310\n",
      "Epoch 83/10000, Batch 90/188, Loss: 0.6540\n",
      "Epoch 83/10000, Batch 100/188, Loss: 0.4466\n",
      "Epoch 83/10000, Batch 110/188, Loss: 0.4012\n",
      "Epoch 83/10000, Batch 120/188, Loss: 0.4686\n",
      "Epoch 83/10000, Batch 130/188, Loss: 0.4256\n",
      "Epoch 83/10000, Batch 140/188, Loss: 0.5012\n",
      "Epoch 83/10000, Batch 150/188, Loss: 0.4147\n",
      "Epoch 83/10000, Batch 160/188, Loss: 0.4999\n",
      "Epoch 83/10000, Batch 170/188, Loss: 0.4784\n",
      "Epoch 83/10000, Batch 180/188, Loss: 0.4562\n",
      "Epoch 83 Finished - Train Loss: 0.4138, Train IoU: 0.5880, Train F1-Score: 0.7345\n",
      "Validation Loss: 0.5652, Validation IoU: 0.4359, Validation F1-Score: 0.5773\n",
      "Current Learning Rate: 0.00000078\n",
      "Validation loss did not improve. Patience: 7/30\n",
      "Epoch 84/10000 starting...\n",
      "Epoch 84/10000, Batch 10/188, Loss: 0.4371\n",
      "Epoch 84/10000, Batch 20/188, Loss: 0.4774\n",
      "Epoch 84/10000, Batch 30/188, Loss: 0.2636\n",
      "Epoch 84/10000, Batch 40/188, Loss: 0.4380\n",
      "Epoch 84/10000, Batch 50/188, Loss: 0.4464\n",
      "Epoch 84/10000, Batch 60/188, Loss: 0.3175\n",
      "Epoch 84/10000, Batch 70/188, Loss: 0.6089\n",
      "Epoch 84/10000, Batch 80/188, Loss: 0.3430\n",
      "Epoch 84/10000, Batch 90/188, Loss: 0.4504\n",
      "Epoch 84/10000, Batch 100/188, Loss: 0.2777\n",
      "Epoch 84/10000, Batch 110/188, Loss: 0.3228\n",
      "Epoch 84/10000, Batch 120/188, Loss: 0.3618\n",
      "Epoch 84/10000, Batch 130/188, Loss: 0.3905\n",
      "Epoch 84/10000, Batch 140/188, Loss: 0.5474\n",
      "Epoch 84/10000, Batch 150/188, Loss: 0.5082\n",
      "Epoch 84/10000, Batch 160/188, Loss: 0.2613\n",
      "Epoch 84/10000, Batch 170/188, Loss: 0.4102\n",
      "Epoch 84/10000, Batch 180/188, Loss: 0.3420\n",
      "Epoch 84 Finished - Train Loss: 0.4157, Train IoU: 0.5860, Train F1-Score: 0.7322\n",
      "Validation Loss: 0.5681, Validation IoU: 0.4331, Validation F1-Score: 0.5740\n",
      "Current Learning Rate: 0.00000039\n",
      "Validation loss did not improve. Patience: 8/30\n",
      "Epoch 85/10000 starting...\n",
      "Epoch 85/10000, Batch 10/188, Loss: 0.4355\n",
      "Epoch 85/10000, Batch 20/188, Loss: 0.4365\n",
      "Epoch 85/10000, Batch 30/188, Loss: 0.4493\n",
      "Epoch 85/10000, Batch 40/188, Loss: 0.2104\n",
      "Epoch 85/10000, Batch 50/188, Loss: 0.4032\n",
      "Epoch 85/10000, Batch 60/188, Loss: 0.5287\n",
      "Epoch 85/10000, Batch 70/188, Loss: 0.4398\n",
      "Epoch 85/10000, Batch 80/188, Loss: 0.4822\n",
      "Epoch 85/10000, Batch 90/188, Loss: 0.5285\n",
      "Epoch 85/10000, Batch 100/188, Loss: 0.4936\n",
      "Epoch 85/10000, Batch 110/188, Loss: 0.5471\n",
      "Epoch 85/10000, Batch 120/188, Loss: 0.4144\n",
      "Epoch 85/10000, Batch 130/188, Loss: 0.3843\n",
      "Epoch 85/10000, Batch 140/188, Loss: 0.6214\n",
      "Epoch 85/10000, Batch 150/188, Loss: 0.3846\n",
      "Epoch 85/10000, Batch 160/188, Loss: 0.4735\n",
      "Epoch 85/10000, Batch 170/188, Loss: 0.5047\n",
      "Epoch 85/10000, Batch 180/188, Loss: 0.3221\n",
      "Epoch 85 Finished - Train Loss: 0.4185, Train IoU: 0.5833, Train F1-Score: 0.7302\n",
      "Validation Loss: 0.5669, Validation IoU: 0.4342, Validation F1-Score: 0.5757\n",
      "Current Learning Rate: 0.00000039\n",
      "Validation loss did not improve. Patience: 9/30\n",
      "Epoch 86/10000 starting...\n",
      "Epoch 86/10000, Batch 10/188, Loss: 0.1804\n",
      "Epoch 86/10000, Batch 20/188, Loss: 0.3607\n",
      "Epoch 86/10000, Batch 30/188, Loss: 0.2315\n",
      "Epoch 86/10000, Batch 40/188, Loss: 0.3511\n",
      "Epoch 86/10000, Batch 50/188, Loss: 0.3700\n",
      "Epoch 86/10000, Batch 60/188, Loss: 0.2957\n",
      "Epoch 86/10000, Batch 70/188, Loss: 0.3865\n",
      "Epoch 86/10000, Batch 80/188, Loss: 0.5592\n",
      "Epoch 86/10000, Batch 90/188, Loss: 0.6137\n",
      "Epoch 86/10000, Batch 100/188, Loss: 0.5067\n",
      "Epoch 86/10000, Batch 110/188, Loss: 0.3935\n",
      "Epoch 86/10000, Batch 120/188, Loss: 0.3302\n",
      "Epoch 86/10000, Batch 130/188, Loss: 0.5403\n",
      "Epoch 86/10000, Batch 140/188, Loss: 0.5104\n",
      "Epoch 86/10000, Batch 150/188, Loss: 0.4339\n",
      "Epoch 86/10000, Batch 160/188, Loss: 0.3253\n",
      "Epoch 86/10000, Batch 170/188, Loss: 0.4009\n",
      "Epoch 86/10000, Batch 180/188, Loss: 0.4737\n",
      "Epoch 86 Finished - Train Loss: 0.4136, Train IoU: 0.5881, Train F1-Score: 0.7345\n",
      "Validation Loss: 0.5659, Validation IoU: 0.4353, Validation F1-Score: 0.5767\n",
      "Current Learning Rate: 0.00000039\n",
      "Validation loss did not improve. Patience: 10/30\n",
      "Epoch 87/10000 starting...\n",
      "Epoch 87/10000, Batch 10/188, Loss: 0.3307\n",
      "Epoch 87/10000, Batch 20/188, Loss: 0.3280\n",
      "Epoch 87/10000, Batch 30/188, Loss: 0.4041\n",
      "Epoch 87/10000, Batch 40/188, Loss: 0.3998\n",
      "Epoch 87/10000, Batch 50/188, Loss: 0.5310\n",
      "Epoch 87/10000, Batch 60/188, Loss: 0.3875\n",
      "Epoch 87/10000, Batch 70/188, Loss: 0.3619\n",
      "Epoch 87/10000, Batch 80/188, Loss: 0.5322\n",
      "Epoch 87/10000, Batch 90/188, Loss: 0.4810\n",
      "Epoch 87/10000, Batch 100/188, Loss: 0.2972\n",
      "Epoch 87/10000, Batch 110/188, Loss: 0.4727\n",
      "Epoch 87/10000, Batch 120/188, Loss: 0.4517\n",
      "Epoch 87/10000, Batch 130/188, Loss: 0.4230\n",
      "Epoch 87/10000, Batch 140/188, Loss: 0.4871\n",
      "Epoch 87/10000, Batch 150/188, Loss: 0.3812\n",
      "Epoch 87/10000, Batch 160/188, Loss: 0.4891\n",
      "Epoch 87/10000, Batch 170/188, Loss: 0.5543\n",
      "Epoch 87/10000, Batch 180/188, Loss: 0.3354\n",
      "Epoch 87 Finished - Train Loss: 0.4127, Train IoU: 0.5890, Train F1-Score: 0.7348\n",
      "Validation Loss: 0.5650, Validation IoU: 0.4361, Validation F1-Score: 0.5776\n",
      "Current Learning Rate: 0.00000039\n",
      "Validation loss did not improve. Patience: 11/30\n",
      "Epoch 88/10000 starting...\n",
      "Epoch 88/10000, Batch 10/188, Loss: 0.2249\n",
      "Epoch 88/10000, Batch 20/188, Loss: 0.3033\n",
      "Epoch 88/10000, Batch 30/188, Loss: 0.5683\n",
      "Epoch 88/10000, Batch 40/188, Loss: 0.2932\n",
      "Epoch 88/10000, Batch 50/188, Loss: 0.3550\n",
      "Epoch 88/10000, Batch 60/188, Loss: 0.3579\n",
      "Epoch 88/10000, Batch 70/188, Loss: 0.2124\n",
      "Epoch 88/10000, Batch 80/188, Loss: 0.4143\n",
      "Epoch 88/10000, Batch 90/188, Loss: 0.6243\n",
      "Epoch 88/10000, Batch 100/188, Loss: 0.3537\n",
      "Epoch 88/10000, Batch 110/188, Loss: 0.3112\n",
      "Epoch 88/10000, Batch 120/188, Loss: 0.2654\n",
      "Epoch 88/10000, Batch 130/188, Loss: 0.3078\n",
      "Epoch 88/10000, Batch 140/188, Loss: 0.4759\n",
      "Epoch 88/10000, Batch 150/188, Loss: 0.2905\n",
      "Epoch 88/10000, Batch 160/188, Loss: 0.4055\n",
      "Epoch 88/10000, Batch 170/188, Loss: 0.3934\n",
      "Epoch 88/10000, Batch 180/188, Loss: 0.3449\n",
      "Epoch 88 Finished - Train Loss: 0.4167, Train IoU: 0.5851, Train F1-Score: 0.7313\n",
      "Validation Loss: 0.5651, Validation IoU: 0.4361, Validation F1-Score: 0.5780\n",
      "Current Learning Rate: 0.00000020\n",
      "Validation loss did not improve. Patience: 12/30\n",
      "Epoch 89/10000 starting...\n",
      "Epoch 89/10000, Batch 10/188, Loss: 0.4124\n",
      "Epoch 89/10000, Batch 20/188, Loss: 0.4093\n",
      "Epoch 89/10000, Batch 30/188, Loss: 0.2605\n",
      "Epoch 89/10000, Batch 40/188, Loss: 0.5267\n",
      "Epoch 89/10000, Batch 50/188, Loss: 0.2722\n",
      "Epoch 89/10000, Batch 60/188, Loss: 0.3478\n",
      "Epoch 89/10000, Batch 70/188, Loss: 0.3654\n",
      "Epoch 89/10000, Batch 80/188, Loss: 0.3746\n",
      "Epoch 89/10000, Batch 90/188, Loss: 0.1797\n",
      "Epoch 89/10000, Batch 100/188, Loss: 0.3949\n",
      "Epoch 89/10000, Batch 110/188, Loss: 0.5202\n",
      "Epoch 89/10000, Batch 120/188, Loss: 0.4723\n",
      "Epoch 89/10000, Batch 130/188, Loss: 0.5527\n",
      "Epoch 89/10000, Batch 140/188, Loss: 0.4550\n",
      "Epoch 89/10000, Batch 150/188, Loss: 0.3554\n",
      "Epoch 89/10000, Batch 160/188, Loss: 0.3929\n",
      "Epoch 89/10000, Batch 170/188, Loss: 0.2613\n",
      "Epoch 89/10000, Batch 180/188, Loss: 0.2516\n",
      "Epoch 89 Finished - Train Loss: 0.4137, Train IoU: 0.5880, Train F1-Score: 0.7348\n",
      "Validation Loss: 0.5649, Validation IoU: 0.4364, Validation F1-Score: 0.5777\n",
      "Current Learning Rate: 0.00000020\n",
      "Validation loss did not improve. Patience: 13/30\n",
      "Epoch 90/10000 starting...\n",
      "Epoch 90/10000, Batch 10/188, Loss: 0.3513\n",
      "Epoch 90/10000, Batch 20/188, Loss: 0.4858\n",
      "Epoch 90/10000, Batch 30/188, Loss: 0.3350\n",
      "Epoch 90/10000, Batch 40/188, Loss: 0.4149\n",
      "Epoch 90/10000, Batch 50/188, Loss: 0.5512\n",
      "Epoch 90/10000, Batch 60/188, Loss: 0.2314\n",
      "Epoch 90/10000, Batch 70/188, Loss: 0.3339\n",
      "Epoch 90/10000, Batch 80/188, Loss: 0.3748\n",
      "Epoch 90/10000, Batch 90/188, Loss: 0.2119\n",
      "Epoch 90/10000, Batch 100/188, Loss: 0.3844\n",
      "Epoch 90/10000, Batch 110/188, Loss: 0.4425\n",
      "Epoch 90/10000, Batch 120/188, Loss: 0.4169\n",
      "Epoch 90/10000, Batch 130/188, Loss: 0.4026\n",
      "Epoch 90/10000, Batch 140/188, Loss: 0.3469\n",
      "Epoch 90/10000, Batch 150/188, Loss: 0.5358\n",
      "Epoch 90/10000, Batch 160/188, Loss: 0.4152\n",
      "Epoch 90/10000, Batch 170/188, Loss: 0.5356\n",
      "Epoch 90/10000, Batch 180/188, Loss: 0.3364\n",
      "Epoch 90 Finished - Train Loss: 0.4146, Train IoU: 0.5872, Train F1-Score: 0.7328\n",
      "Validation Loss: 0.5657, Validation IoU: 0.4354, Validation F1-Score: 0.5767\n",
      "Current Learning Rate: 0.00000020\n",
      "Validation loss did not improve. Patience: 14/30\n",
      "Epoch 91/10000 starting...\n",
      "Epoch 91/10000, Batch 10/188, Loss: 0.5437\n",
      "Epoch 91/10000, Batch 20/188, Loss: 0.2681\n",
      "Epoch 91/10000, Batch 30/188, Loss: 0.4893\n",
      "Epoch 91/10000, Batch 40/188, Loss: 0.3931\n",
      "Epoch 91/10000, Batch 50/188, Loss: 0.3404\n",
      "Epoch 91/10000, Batch 60/188, Loss: 0.5063\n",
      "Epoch 91/10000, Batch 70/188, Loss: 0.4355\n",
      "Epoch 91/10000, Batch 80/188, Loss: 0.4906\n",
      "Epoch 91/10000, Batch 90/188, Loss: 0.4642\n",
      "Epoch 91/10000, Batch 100/188, Loss: 0.3256\n",
      "Epoch 91/10000, Batch 110/188, Loss: 0.4161\n",
      "Epoch 91/10000, Batch 120/188, Loss: 0.3515\n",
      "Epoch 91/10000, Batch 130/188, Loss: 0.3800\n",
      "Epoch 91/10000, Batch 140/188, Loss: 0.5665\n",
      "Epoch 91/10000, Batch 150/188, Loss: 0.3550\n",
      "Epoch 91/10000, Batch 160/188, Loss: 0.3007\n",
      "Epoch 91/10000, Batch 170/188, Loss: 0.3932\n",
      "Epoch 91/10000, Batch 180/188, Loss: 0.3343\n",
      "Epoch 91 Finished - Train Loss: 0.4129, Train IoU: 0.5888, Train F1-Score: 0.7353\n",
      "Validation Loss: 0.5661, Validation IoU: 0.4351, Validation F1-Score: 0.5764\n",
      "Current Learning Rate: 0.00000020\n",
      "Validation loss did not improve. Patience: 15/30\n",
      "Epoch 92/10000 starting...\n",
      "Epoch 92/10000, Batch 10/188, Loss: 0.3261\n",
      "Epoch 92/10000, Batch 20/188, Loss: 0.3787\n",
      "Epoch 92/10000, Batch 30/188, Loss: 0.4270\n",
      "Epoch 92/10000, Batch 40/188, Loss: 0.6345\n",
      "Epoch 92/10000, Batch 50/188, Loss: 0.4581\n",
      "Epoch 92/10000, Batch 60/188, Loss: 0.4571\n",
      "Epoch 92/10000, Batch 70/188, Loss: 0.3180\n",
      "Epoch 92/10000, Batch 80/188, Loss: 0.5814\n",
      "Epoch 92/10000, Batch 90/188, Loss: 0.4641\n",
      "Epoch 92/10000, Batch 100/188, Loss: 0.3344\n",
      "Epoch 92/10000, Batch 110/188, Loss: 0.2608\n",
      "Epoch 92/10000, Batch 120/188, Loss: 0.2329\n",
      "Epoch 92/10000, Batch 130/188, Loss: 0.4845\n",
      "Epoch 92/10000, Batch 140/188, Loss: 0.5587\n",
      "Epoch 92/10000, Batch 150/188, Loss: 0.3810\n",
      "Epoch 92/10000, Batch 160/188, Loss: 0.4398\n",
      "Epoch 92/10000, Batch 170/188, Loss: 0.1222\n",
      "Epoch 92/10000, Batch 180/188, Loss: 0.2392\n",
      "Epoch 92 Finished - Train Loss: 0.4146, Train IoU: 0.5871, Train F1-Score: 0.7324\n",
      "Validation Loss: 0.5666, Validation IoU: 0.4346, Validation F1-Score: 0.5752\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 16/30\n",
      "Epoch 93/10000 starting...\n",
      "Epoch 93/10000, Batch 10/188, Loss: 0.5081\n",
      "Epoch 93/10000, Batch 20/188, Loss: 0.3334\n",
      "Epoch 93/10000, Batch 30/188, Loss: 0.2274\n",
      "Epoch 93/10000, Batch 40/188, Loss: 0.4269\n",
      "Epoch 93/10000, Batch 50/188, Loss: 0.3678\n",
      "Epoch 93/10000, Batch 60/188, Loss: 0.4336\n",
      "Epoch 93/10000, Batch 70/188, Loss: 0.4995\n",
      "Epoch 93/10000, Batch 80/188, Loss: 0.5080\n",
      "Epoch 93/10000, Batch 90/188, Loss: 0.4629\n",
      "Epoch 93/10000, Batch 100/188, Loss: 0.4318\n",
      "Epoch 93/10000, Batch 110/188, Loss: 0.2661\n",
      "Epoch 93/10000, Batch 120/188, Loss: 0.3751\n",
      "Epoch 93/10000, Batch 130/188, Loss: 0.4913\n",
      "Epoch 93/10000, Batch 140/188, Loss: 0.2846\n",
      "Epoch 93/10000, Batch 150/188, Loss: 0.2743\n",
      "Epoch 93/10000, Batch 160/188, Loss: 0.3433\n",
      "Epoch 93/10000, Batch 170/188, Loss: 0.5769\n",
      "Epoch 93/10000, Batch 180/188, Loss: 0.4171\n",
      "Epoch 93 Finished - Train Loss: 0.4129, Train IoU: 0.5888, Train F1-Score: 0.7353\n",
      "Validation Loss: 0.5663, Validation IoU: 0.4349, Validation F1-Score: 0.5762\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 17/30\n",
      "Epoch 94/10000 starting...\n",
      "Epoch 94/10000, Batch 10/188, Loss: 0.3057\n",
      "Epoch 94/10000, Batch 20/188, Loss: 0.3943\n",
      "Epoch 94/10000, Batch 30/188, Loss: 0.3257\n",
      "Epoch 94/10000, Batch 40/188, Loss: 0.2300\n",
      "Epoch 94/10000, Batch 50/188, Loss: 0.3759\n",
      "Epoch 94/10000, Batch 60/188, Loss: 0.3948\n",
      "Epoch 94/10000, Batch 70/188, Loss: 0.2733\n",
      "Epoch 94/10000, Batch 80/188, Loss: 0.3624\n",
      "Epoch 94/10000, Batch 90/188, Loss: 0.6238\n",
      "Epoch 94/10000, Batch 100/188, Loss: 0.5119\n",
      "Epoch 94/10000, Batch 110/188, Loss: 0.4594\n",
      "Epoch 94/10000, Batch 120/188, Loss: 0.2723\n",
      "Epoch 94/10000, Batch 130/188, Loss: 0.3365\n",
      "Epoch 94/10000, Batch 140/188, Loss: 0.3003\n",
      "Epoch 94/10000, Batch 150/188, Loss: 0.2857\n",
      "Epoch 94/10000, Batch 160/188, Loss: 0.3034\n",
      "Epoch 94/10000, Batch 170/188, Loss: 0.3112\n",
      "Epoch 94/10000, Batch 180/188, Loss: 0.3906\n",
      "Epoch 94 Finished - Train Loss: 0.4114, Train IoU: 0.5903, Train F1-Score: 0.7360\n",
      "Validation Loss: 0.5653, Validation IoU: 0.4358, Validation F1-Score: 0.5772\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 18/30\n",
      "Epoch 95/10000 starting...\n",
      "Epoch 95/10000, Batch 10/188, Loss: 0.4839\n",
      "Epoch 95/10000, Batch 20/188, Loss: 0.3992\n",
      "Epoch 95/10000, Batch 30/188, Loss: 0.6106\n",
      "Epoch 95/10000, Batch 40/188, Loss: 0.3027\n",
      "Epoch 95/10000, Batch 50/188, Loss: 0.3358\n",
      "Epoch 95/10000, Batch 60/188, Loss: 0.3863\n",
      "Epoch 95/10000, Batch 70/188, Loss: 0.4333\n",
      "Epoch 95/10000, Batch 80/188, Loss: 0.3372\n",
      "Epoch 95/10000, Batch 90/188, Loss: 0.5592\n",
      "Epoch 95/10000, Batch 100/188, Loss: 0.4557\n",
      "Epoch 95/10000, Batch 110/188, Loss: 0.2319\n",
      "Epoch 95/10000, Batch 120/188, Loss: 0.3234\n",
      "Epoch 95/10000, Batch 130/188, Loss: 0.4979\n",
      "Epoch 95/10000, Batch 140/188, Loss: 0.4654\n",
      "Epoch 95/10000, Batch 150/188, Loss: 0.5346\n",
      "Epoch 95/10000, Batch 160/188, Loss: 0.6021\n",
      "Epoch 95/10000, Batch 170/188, Loss: 0.5893\n",
      "Epoch 95/10000, Batch 180/188, Loss: 0.2407\n",
      "Epoch 95 Finished - Train Loss: 0.4123, Train IoU: 0.5894, Train F1-Score: 0.7362\n",
      "Validation Loss: 0.5661, Validation IoU: 0.4351, Validation F1-Score: 0.5766\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 19/30\n",
      "Epoch 96/10000 starting...\n",
      "Epoch 96/10000, Batch 10/188, Loss: 0.5858\n",
      "Epoch 96/10000, Batch 20/188, Loss: 0.2356\n",
      "Epoch 96/10000, Batch 30/188, Loss: 0.2767\n",
      "Epoch 96/10000, Batch 40/188, Loss: 0.5484\n",
      "Epoch 96/10000, Batch 50/188, Loss: 0.3678\n",
      "Epoch 96/10000, Batch 60/188, Loss: 0.4183\n",
      "Epoch 96/10000, Batch 70/188, Loss: 0.4466\n",
      "Epoch 96/10000, Batch 80/188, Loss: 0.4426\n",
      "Epoch 96/10000, Batch 90/188, Loss: 0.4256\n",
      "Epoch 96/10000, Batch 100/188, Loss: 0.3060\n",
      "Epoch 96/10000, Batch 110/188, Loss: 0.3757\n",
      "Epoch 96/10000, Batch 120/188, Loss: 0.3385\n",
      "Epoch 96/10000, Batch 130/188, Loss: 0.3658\n",
      "Epoch 96/10000, Batch 140/188, Loss: 0.4853\n",
      "Epoch 96/10000, Batch 150/188, Loss: 0.4255\n",
      "Epoch 96/10000, Batch 160/188, Loss: 0.2773\n",
      "Epoch 96/10000, Batch 170/188, Loss: 0.3142\n",
      "Epoch 96/10000, Batch 180/188, Loss: 0.4373\n",
      "Epoch 96 Finished - Train Loss: 0.4134, Train IoU: 0.5883, Train F1-Score: 0.7345\n",
      "Validation Loss: 0.5670, Validation IoU: 0.4342, Validation F1-Score: 0.5753\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 20/30\n",
      "Epoch 97/10000 starting...\n",
      "Epoch 97/10000, Batch 10/188, Loss: 0.4922\n",
      "Epoch 97/10000, Batch 20/188, Loss: 0.3129\n",
      "Epoch 97/10000, Batch 30/188, Loss: 0.3295\n",
      "Epoch 97/10000, Batch 40/188, Loss: 0.5992\n",
      "Epoch 97/10000, Batch 50/188, Loss: 0.2561\n",
      "Epoch 97/10000, Batch 60/188, Loss: 0.5085\n",
      "Epoch 97/10000, Batch 70/188, Loss: 0.3734\n",
      "Epoch 97/10000, Batch 80/188, Loss: 0.3009\n",
      "Epoch 97/10000, Batch 90/188, Loss: 0.4032\n",
      "Epoch 97/10000, Batch 100/188, Loss: 0.4465\n",
      "Epoch 97/10000, Batch 110/188, Loss: 0.4510\n",
      "Epoch 97/10000, Batch 120/188, Loss: 0.2791\n",
      "Epoch 97/10000, Batch 130/188, Loss: 0.3347\n",
      "Epoch 97/10000, Batch 140/188, Loss: 0.4562\n",
      "Epoch 97/10000, Batch 150/188, Loss: 0.3881\n",
      "Epoch 97/10000, Batch 160/188, Loss: 0.3640\n",
      "Epoch 97/10000, Batch 170/188, Loss: 0.4579\n",
      "Epoch 97/10000, Batch 180/188, Loss: 0.3204\n",
      "Epoch 97 Finished - Train Loss: 0.4121, Train IoU: 0.5896, Train F1-Score: 0.7353\n",
      "Validation Loss: 0.5655, Validation IoU: 0.4356, Validation F1-Score: 0.5769\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 21/30\n",
      "Epoch 98/10000 starting...\n",
      "Epoch 98/10000, Batch 10/188, Loss: 0.2817\n",
      "Epoch 98/10000, Batch 20/188, Loss: 0.4065\n",
      "Epoch 98/10000, Batch 30/188, Loss: 0.2600\n",
      "Epoch 98/10000, Batch 40/188, Loss: 0.5123\n",
      "Epoch 98/10000, Batch 50/188, Loss: 0.2044\n",
      "Epoch 98/10000, Batch 60/188, Loss: 0.3396\n",
      "Epoch 98/10000, Batch 70/188, Loss: 0.5756\n",
      "Epoch 98/10000, Batch 80/188, Loss: 0.4683\n",
      "Epoch 98/10000, Batch 90/188, Loss: 0.3088\n",
      "Epoch 98/10000, Batch 100/188, Loss: 0.4003\n",
      "Epoch 98/10000, Batch 110/188, Loss: 0.3155\n",
      "Epoch 98/10000, Batch 120/188, Loss: 0.3281\n",
      "Epoch 98/10000, Batch 130/188, Loss: 0.4227\n",
      "Epoch 98/10000, Batch 140/188, Loss: 0.2807\n",
      "Epoch 98/10000, Batch 150/188, Loss: 0.5717\n",
      "Epoch 98/10000, Batch 160/188, Loss: 0.3239\n",
      "Epoch 98/10000, Batch 170/188, Loss: 0.3355\n",
      "Epoch 98/10000, Batch 180/188, Loss: 0.5800\n",
      "Epoch 98 Finished - Train Loss: 0.4159, Train IoU: 0.5858, Train F1-Score: 0.7316\n",
      "Validation Loss: 0.5660, Validation IoU: 0.4352, Validation F1-Score: 0.5770\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 22/30\n",
      "Epoch 99/10000 starting...\n",
      "Epoch 99/10000, Batch 10/188, Loss: 0.4352\n",
      "Epoch 99/10000, Batch 20/188, Loss: 0.4925\n",
      "Epoch 99/10000, Batch 30/188, Loss: 0.3332\n",
      "Epoch 99/10000, Batch 40/188, Loss: 0.3063\n",
      "Epoch 99/10000, Batch 50/188, Loss: 0.4252\n",
      "Epoch 99/10000, Batch 60/188, Loss: 0.4566\n",
      "Epoch 99/10000, Batch 70/188, Loss: 0.4041\n",
      "Epoch 99/10000, Batch 80/188, Loss: 0.5271\n",
      "Epoch 99/10000, Batch 90/188, Loss: 0.3382\n",
      "Epoch 99/10000, Batch 100/188, Loss: 0.4706\n",
      "Epoch 99/10000, Batch 110/188, Loss: 0.2182\n",
      "Epoch 99/10000, Batch 120/188, Loss: 0.3972\n",
      "Epoch 99/10000, Batch 130/188, Loss: 0.2626\n",
      "Epoch 99/10000, Batch 140/188, Loss: 0.5878\n",
      "Epoch 99/10000, Batch 150/188, Loss: 0.3478\n",
      "Epoch 99/10000, Batch 160/188, Loss: 0.3565\n",
      "Epoch 99/10000, Batch 170/188, Loss: 0.2667\n",
      "Epoch 99/10000, Batch 180/188, Loss: 0.3757\n",
      "Epoch 99 Finished - Train Loss: 0.4140, Train IoU: 0.5878, Train F1-Score: 0.7336\n",
      "Validation Loss: 0.5666, Validation IoU: 0.4346, Validation F1-Score: 0.5756\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 23/30\n",
      "Epoch 100/10000 starting...\n",
      "Epoch 100/10000, Batch 10/188, Loss: 0.3203\n",
      "Epoch 100/10000, Batch 20/188, Loss: 0.6538\n",
      "Epoch 100/10000, Batch 30/188, Loss: 0.3599\n",
      "Epoch 100/10000, Batch 40/188, Loss: 0.6747\n",
      "Epoch 100/10000, Batch 50/188, Loss: 0.2889\n",
      "Epoch 100/10000, Batch 60/188, Loss: 0.3061\n",
      "Epoch 100/10000, Batch 70/188, Loss: 0.4815\n",
      "Epoch 100/10000, Batch 80/188, Loss: 0.3400\n",
      "Epoch 100/10000, Batch 90/188, Loss: 0.4656\n",
      "Epoch 100/10000, Batch 100/188, Loss: 0.4753\n",
      "Epoch 100/10000, Batch 110/188, Loss: 0.5416\n",
      "Epoch 100/10000, Batch 120/188, Loss: 0.2325\n",
      "Epoch 100/10000, Batch 130/188, Loss: 0.5722\n",
      "Epoch 100/10000, Batch 140/188, Loss: 0.4664\n",
      "Epoch 100/10000, Batch 150/188, Loss: 0.5562\n",
      "Epoch 100/10000, Batch 160/188, Loss: 0.3343\n",
      "Epoch 100/10000, Batch 170/188, Loss: 0.3459\n",
      "Epoch 100/10000, Batch 180/188, Loss: 0.3604\n",
      "Epoch 100 Finished - Train Loss: 0.4122, Train IoU: 0.5895, Train F1-Score: 0.7355\n",
      "Validation Loss: 0.5660, Validation IoU: 0.4352, Validation F1-Score: 0.5760\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 24/30\n",
      "Epoch 101/10000 starting...\n",
      "Epoch 101/10000, Batch 10/188, Loss: 0.3003\n",
      "Epoch 101/10000, Batch 20/188, Loss: 0.5395\n",
      "Epoch 101/10000, Batch 30/188, Loss: 0.4157\n",
      "Epoch 101/10000, Batch 40/188, Loss: 0.5388\n",
      "Epoch 101/10000, Batch 50/188, Loss: 0.3677\n",
      "Epoch 101/10000, Batch 60/188, Loss: 0.4683\n",
      "Epoch 101/10000, Batch 70/188, Loss: 0.4614\n",
      "Epoch 101/10000, Batch 80/188, Loss: 0.4576\n",
      "Epoch 101/10000, Batch 90/188, Loss: 0.2388\n",
      "Epoch 101/10000, Batch 100/188, Loss: 0.6064\n",
      "Epoch 101/10000, Batch 110/188, Loss: 0.3999\n",
      "Epoch 101/10000, Batch 120/188, Loss: 0.3708\n",
      "Epoch 101/10000, Batch 130/188, Loss: 0.4364\n",
      "Epoch 101/10000, Batch 140/188, Loss: 0.2037\n",
      "Epoch 101/10000, Batch 150/188, Loss: 0.4027\n",
      "Epoch 101/10000, Batch 160/188, Loss: 0.4770\n",
      "Epoch 101/10000, Batch 170/188, Loss: 0.3425\n",
      "Epoch 101/10000, Batch 180/188, Loss: 0.5395\n",
      "Epoch 101 Finished - Train Loss: 0.4133, Train IoU: 0.5884, Train F1-Score: 0.7346\n",
      "Validation Loss: 0.5671, Validation IoU: 0.4341, Validation F1-Score: 0.5755\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 25/30\n",
      "Epoch 102/10000 starting...\n",
      "Epoch 102/10000, Batch 10/188, Loss: 0.4733\n",
      "Epoch 102/10000, Batch 20/188, Loss: 0.2865\n",
      "Epoch 102/10000, Batch 30/188, Loss: 0.3523\n",
      "Epoch 102/10000, Batch 40/188, Loss: 0.6118\n",
      "Epoch 102/10000, Batch 50/188, Loss: 0.4148\n",
      "Epoch 102/10000, Batch 60/188, Loss: 0.3774\n",
      "Epoch 102/10000, Batch 70/188, Loss: 0.3659\n",
      "Epoch 102/10000, Batch 80/188, Loss: 0.2696\n",
      "Epoch 102/10000, Batch 90/188, Loss: 0.3908\n",
      "Epoch 102/10000, Batch 100/188, Loss: 0.3767\n",
      "Epoch 102/10000, Batch 110/188, Loss: 0.2993\n",
      "Epoch 102/10000, Batch 120/188, Loss: 0.3457\n",
      "Epoch 102/10000, Batch 130/188, Loss: 0.4231\n",
      "Epoch 102/10000, Batch 140/188, Loss: 0.5092\n",
      "Epoch 102/10000, Batch 150/188, Loss: 0.4682\n",
      "Epoch 102/10000, Batch 160/188, Loss: 0.4333\n",
      "Epoch 102/10000, Batch 170/188, Loss: 0.4764\n",
      "Epoch 102/10000, Batch 180/188, Loss: 0.2477\n",
      "Epoch 102 Finished - Train Loss: 0.4149, Train IoU: 0.5868, Train F1-Score: 0.7334\n",
      "Validation Loss: 0.5640, Validation IoU: 0.4371, Validation F1-Score: 0.5789\n",
      "Current Learning Rate: 0.00000010\n",
      "Best validation loss updated: 0.5640. Saving model and state...\n",
      "Epoch 103/10000 starting...\n",
      "Epoch 103/10000, Batch 10/188, Loss: 0.7379\n",
      "Epoch 103/10000, Batch 20/188, Loss: 0.3915\n",
      "Epoch 103/10000, Batch 30/188, Loss: 0.5733\n",
      "Epoch 103/10000, Batch 40/188, Loss: 0.6206\n",
      "Epoch 103/10000, Batch 50/188, Loss: 0.4677\n",
      "Epoch 103/10000, Batch 60/188, Loss: 0.3587\n",
      "Epoch 103/10000, Batch 70/188, Loss: 0.3468\n",
      "Epoch 103/10000, Batch 80/188, Loss: 0.4329\n",
      "Epoch 103/10000, Batch 90/188, Loss: 0.4490\n",
      "Epoch 103/10000, Batch 100/188, Loss: 0.2419\n",
      "Epoch 103/10000, Batch 110/188, Loss: 0.5744\n",
      "Epoch 103/10000, Batch 120/188, Loss: 0.4557\n",
      "Epoch 103/10000, Batch 130/188, Loss: 0.6042\n",
      "Epoch 103/10000, Batch 140/188, Loss: 0.2689\n",
      "Epoch 103/10000, Batch 150/188, Loss: 0.4073\n",
      "Epoch 103/10000, Batch 160/188, Loss: 0.6057\n",
      "Epoch 103/10000, Batch 170/188, Loss: 0.4524\n",
      "Epoch 103/10000, Batch 180/188, Loss: 0.4550\n",
      "Epoch 103 Finished - Train Loss: 0.4124, Train IoU: 0.5893, Train F1-Score: 0.7360\n",
      "Validation Loss: 0.5653, Validation IoU: 0.4358, Validation F1-Score: 0.5771\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 1/30\n",
      "Epoch 104/10000 starting...\n",
      "Epoch 104/10000, Batch 10/188, Loss: 0.5103\n",
      "Epoch 104/10000, Batch 20/188, Loss: 0.6067\n",
      "Epoch 104/10000, Batch 30/188, Loss: 0.4687\n",
      "Epoch 104/10000, Batch 40/188, Loss: 0.2461\n",
      "Epoch 104/10000, Batch 50/188, Loss: 0.4200\n",
      "Epoch 104/10000, Batch 60/188, Loss: 0.4997\n",
      "Epoch 104/10000, Batch 70/188, Loss: 0.5766\n",
      "Epoch 104/10000, Batch 80/188, Loss: 0.5288\n",
      "Epoch 104/10000, Batch 90/188, Loss: 0.2567\n",
      "Epoch 104/10000, Batch 100/188, Loss: 0.6715\n",
      "Epoch 104/10000, Batch 110/188, Loss: 0.3290\n",
      "Epoch 104/10000, Batch 120/188, Loss: 0.5701\n",
      "Epoch 104/10000, Batch 130/188, Loss: 0.3866\n",
      "Epoch 104/10000, Batch 140/188, Loss: 0.3518\n",
      "Epoch 104/10000, Batch 150/188, Loss: 0.4015\n",
      "Epoch 104/10000, Batch 160/188, Loss: 0.3788\n",
      "Epoch 104/10000, Batch 170/188, Loss: 0.4928\n",
      "Epoch 104/10000, Batch 180/188, Loss: 0.4917\n",
      "Epoch 104 Finished - Train Loss: 0.4134, Train IoU: 0.5883, Train F1-Score: 0.7344\n",
      "Validation Loss: 0.5665, Validation IoU: 0.4347, Validation F1-Score: 0.5756\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 2/30\n",
      "Epoch 105/10000 starting...\n",
      "Epoch 105/10000, Batch 10/188, Loss: 0.3801\n",
      "Epoch 105/10000, Batch 20/188, Loss: 0.4464\n",
      "Epoch 105/10000, Batch 30/188, Loss: 0.3102\n",
      "Epoch 105/10000, Batch 40/188, Loss: 0.6040\n",
      "Epoch 105/10000, Batch 50/188, Loss: 0.3688\n",
      "Epoch 105/10000, Batch 60/188, Loss: 0.3764\n",
      "Epoch 105/10000, Batch 70/188, Loss: 0.3778\n",
      "Epoch 105/10000, Batch 80/188, Loss: 0.5887\n",
      "Epoch 105/10000, Batch 90/188, Loss: 0.2119\n",
      "Epoch 105/10000, Batch 100/188, Loss: 0.5241\n",
      "Epoch 105/10000, Batch 110/188, Loss: 0.4310\n",
      "Epoch 105/10000, Batch 120/188, Loss: 0.4042\n",
      "Epoch 105/10000, Batch 130/188, Loss: 0.7731\n",
      "Epoch 105/10000, Batch 140/188, Loss: 0.2339\n",
      "Epoch 105/10000, Batch 150/188, Loss: 0.2965\n",
      "Epoch 105/10000, Batch 160/188, Loss: 0.4666\n",
      "Epoch 105/10000, Batch 170/188, Loss: 0.5821\n",
      "Epoch 105/10000, Batch 180/188, Loss: 0.3980\n",
      "Epoch 105 Finished - Train Loss: 0.4159, Train IoU: 0.5858, Train F1-Score: 0.7330\n",
      "Validation Loss: 0.5669, Validation IoU: 0.4343, Validation F1-Score: 0.5753\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 3/30\n",
      "Epoch 106/10000 starting...\n",
      "Epoch 106/10000, Batch 10/188, Loss: 0.5581\n",
      "Epoch 106/10000, Batch 20/188, Loss: 0.5184\n",
      "Epoch 106/10000, Batch 30/188, Loss: 0.4851\n",
      "Epoch 106/10000, Batch 40/188, Loss: 0.4548\n",
      "Epoch 106/10000, Batch 50/188, Loss: 0.4790\n",
      "Epoch 106/10000, Batch 60/188, Loss: 0.4595\n",
      "Epoch 106/10000, Batch 70/188, Loss: 0.3757\n",
      "Epoch 106/10000, Batch 80/188, Loss: 0.3822\n",
      "Epoch 106/10000, Batch 90/188, Loss: 0.3322\n",
      "Epoch 106/10000, Batch 100/188, Loss: 0.4188\n",
      "Epoch 106/10000, Batch 110/188, Loss: 0.4261\n",
      "Epoch 106/10000, Batch 120/188, Loss: 0.4011\n",
      "Epoch 106/10000, Batch 130/188, Loss: 0.2666\n",
      "Epoch 106/10000, Batch 140/188, Loss: 0.4631\n",
      "Epoch 106/10000, Batch 150/188, Loss: 0.2672\n",
      "Epoch 106/10000, Batch 160/188, Loss: 0.5148\n",
      "Epoch 106/10000, Batch 170/188, Loss: 0.3749\n",
      "Epoch 106/10000, Batch 180/188, Loss: 0.6319\n",
      "Epoch 106 Finished - Train Loss: 0.4109, Train IoU: 0.5908, Train F1-Score: 0.7362\n",
      "Validation Loss: 0.5656, Validation IoU: 0.4356, Validation F1-Score: 0.5773\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 4/30\n",
      "Epoch 107/10000 starting...\n",
      "Epoch 107/10000, Batch 10/188, Loss: 0.3957\n",
      "Epoch 107/10000, Batch 20/188, Loss: 0.3579\n",
      "Epoch 107/10000, Batch 30/188, Loss: 0.4532\n",
      "Epoch 107/10000, Batch 40/188, Loss: 0.4900\n",
      "Epoch 107/10000, Batch 50/188, Loss: 0.4007\n",
      "Epoch 107/10000, Batch 60/188, Loss: 0.5042\n",
      "Epoch 107/10000, Batch 70/188, Loss: 0.3883\n",
      "Epoch 107/10000, Batch 80/188, Loss: 0.4225\n",
      "Epoch 107/10000, Batch 90/188, Loss: 0.3691\n",
      "Epoch 107/10000, Batch 100/188, Loss: 0.4490\n",
      "Epoch 107/10000, Batch 110/188, Loss: 0.4353\n",
      "Epoch 107/10000, Batch 120/188, Loss: 0.3251\n",
      "Epoch 107/10000, Batch 130/188, Loss: 0.6288\n",
      "Epoch 107/10000, Batch 140/188, Loss: 0.3847\n",
      "Epoch 107/10000, Batch 150/188, Loss: 0.4242\n",
      "Epoch 107/10000, Batch 160/188, Loss: 0.3040\n",
      "Epoch 107/10000, Batch 170/188, Loss: 0.2743\n",
      "Epoch 107/10000, Batch 180/188, Loss: 0.4326\n",
      "Epoch 107 Finished - Train Loss: 0.4138, Train IoU: 0.5878, Train F1-Score: 0.7344\n",
      "Validation Loss: 0.5658, Validation IoU: 0.4354, Validation F1-Score: 0.5765\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 5/30\n",
      "Epoch 108/10000 starting...\n",
      "Epoch 108/10000, Batch 10/188, Loss: 0.4904\n",
      "Epoch 108/10000, Batch 20/188, Loss: 0.4174\n",
      "Epoch 108/10000, Batch 30/188, Loss: 0.3386\n",
      "Epoch 108/10000, Batch 40/188, Loss: 0.3828\n",
      "Epoch 108/10000, Batch 50/188, Loss: 0.3594\n",
      "Epoch 108/10000, Batch 60/188, Loss: 0.3885\n",
      "Epoch 108/10000, Batch 70/188, Loss: 0.5670\n",
      "Epoch 108/10000, Batch 80/188, Loss: 0.3951\n",
      "Epoch 108/10000, Batch 90/188, Loss: 0.4184\n",
      "Epoch 108/10000, Batch 100/188, Loss: 0.3755\n",
      "Epoch 108/10000, Batch 110/188, Loss: 0.2641\n",
      "Epoch 108/10000, Batch 120/188, Loss: 0.4115\n",
      "Epoch 108/10000, Batch 130/188, Loss: 0.2571\n",
      "Epoch 108/10000, Batch 140/188, Loss: 0.3661\n",
      "Epoch 108/10000, Batch 150/188, Loss: 0.3236\n",
      "Epoch 108/10000, Batch 160/188, Loss: 0.5375\n",
      "Epoch 108/10000, Batch 170/188, Loss: 0.3542\n",
      "Epoch 108/10000, Batch 180/188, Loss: 0.3551\n",
      "Epoch 108 Finished - Train Loss: 0.4140, Train IoU: 0.5877, Train F1-Score: 0.7342\n",
      "Validation Loss: 0.5646, Validation IoU: 0.4366, Validation F1-Score: 0.5785\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 6/30\n",
      "Epoch 109/10000 starting...\n",
      "Epoch 109/10000, Batch 10/188, Loss: 0.2623\n",
      "Epoch 109/10000, Batch 20/188, Loss: 0.4869\n",
      "Epoch 109/10000, Batch 30/188, Loss: 0.6126\n",
      "Epoch 109/10000, Batch 40/188, Loss: 0.5643\n",
      "Epoch 109/10000, Batch 50/188, Loss: 0.3611\n",
      "Epoch 109/10000, Batch 60/188, Loss: 0.4716\n",
      "Epoch 109/10000, Batch 70/188, Loss: 0.4432\n",
      "Epoch 109/10000, Batch 80/188, Loss: 0.4651\n",
      "Epoch 109/10000, Batch 90/188, Loss: 0.4088\n",
      "Epoch 109/10000, Batch 100/188, Loss: 0.3115\n",
      "Epoch 109/10000, Batch 110/188, Loss: 0.4046\n",
      "Epoch 109/10000, Batch 120/188, Loss: 0.5628\n",
      "Epoch 109/10000, Batch 130/188, Loss: 0.3184\n",
      "Epoch 109/10000, Batch 140/188, Loss: 0.4203\n",
      "Epoch 109/10000, Batch 150/188, Loss: 0.2542\n",
      "Epoch 109/10000, Batch 160/188, Loss: 0.2857\n",
      "Epoch 109/10000, Batch 170/188, Loss: 0.3249\n",
      "Epoch 109/10000, Batch 180/188, Loss: 0.3684\n",
      "Epoch 109 Finished - Train Loss: 0.4105, Train IoU: 0.5912, Train F1-Score: 0.7376\n",
      "Validation Loss: 0.5656, Validation IoU: 0.4357, Validation F1-Score: 0.5771\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 7/30\n",
      "Epoch 110/10000 starting...\n",
      "Epoch 110/10000, Batch 10/188, Loss: 0.4726\n",
      "Epoch 110/10000, Batch 20/188, Loss: 0.5482\n",
      "Epoch 110/10000, Batch 30/188, Loss: 0.3131\n",
      "Epoch 110/10000, Batch 40/188, Loss: 0.3898\n",
      "Epoch 110/10000, Batch 50/188, Loss: 0.2475\n",
      "Epoch 110/10000, Batch 60/188, Loss: 0.5341\n",
      "Epoch 110/10000, Batch 70/188, Loss: 0.4717\n",
      "Epoch 110/10000, Batch 80/188, Loss: 0.2985\n",
      "Epoch 110/10000, Batch 90/188, Loss: 0.4435\n",
      "Epoch 110/10000, Batch 100/188, Loss: 0.2208\n",
      "Epoch 110/10000, Batch 110/188, Loss: 0.5747\n",
      "Epoch 110/10000, Batch 120/188, Loss: 0.4316\n",
      "Epoch 110/10000, Batch 130/188, Loss: 0.4359\n",
      "Epoch 110/10000, Batch 140/188, Loss: 0.4297\n",
      "Epoch 110/10000, Batch 150/188, Loss: 0.3086\n",
      "Epoch 110/10000, Batch 160/188, Loss: 0.5472\n",
      "Epoch 110/10000, Batch 170/188, Loss: 0.2556\n",
      "Epoch 110/10000, Batch 180/188, Loss: 0.3668\n",
      "Epoch 110 Finished - Train Loss: 0.4097, Train IoU: 0.5920, Train F1-Score: 0.7383\n",
      "Validation Loss: 0.5645, Validation IoU: 0.4367, Validation F1-Score: 0.5789\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 8/30\n",
      "Epoch 111/10000 starting...\n",
      "Epoch 111/10000, Batch 10/188, Loss: 0.5052\n",
      "Epoch 111/10000, Batch 20/188, Loss: 0.4558\n",
      "Epoch 111/10000, Batch 30/188, Loss: 0.3801\n",
      "Epoch 111/10000, Batch 40/188, Loss: 0.5176\n",
      "Epoch 111/10000, Batch 50/188, Loss: 0.3372\n",
      "Epoch 111/10000, Batch 60/188, Loss: 0.1985\n",
      "Epoch 111/10000, Batch 70/188, Loss: 0.6456\n",
      "Epoch 111/10000, Batch 80/188, Loss: 0.3609\n",
      "Epoch 111/10000, Batch 90/188, Loss: 0.4616\n",
      "Epoch 111/10000, Batch 100/188, Loss: 0.3538\n",
      "Epoch 111/10000, Batch 110/188, Loss: 0.3456\n",
      "Epoch 111/10000, Batch 120/188, Loss: 0.4855\n",
      "Epoch 111/10000, Batch 130/188, Loss: 0.6734\n",
      "Epoch 111/10000, Batch 140/188, Loss: 0.5352\n",
      "Epoch 111/10000, Batch 150/188, Loss: 0.6284\n",
      "Epoch 111/10000, Batch 160/188, Loss: 0.3165\n",
      "Epoch 111/10000, Batch 170/188, Loss: 0.3141\n",
      "Epoch 111/10000, Batch 180/188, Loss: 0.5329\n",
      "Epoch 111 Finished - Train Loss: 0.4126, Train IoU: 0.5890, Train F1-Score: 0.7356\n",
      "Validation Loss: 0.5683, Validation IoU: 0.4329, Validation F1-Score: 0.5732\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 9/30\n",
      "Epoch 112/10000 starting...\n",
      "Epoch 112/10000, Batch 10/188, Loss: 0.3401\n",
      "Epoch 112/10000, Batch 20/188, Loss: 0.4305\n",
      "Epoch 112/10000, Batch 30/188, Loss: 0.1971\n",
      "Epoch 112/10000, Batch 40/188, Loss: 0.2977\n",
      "Epoch 112/10000, Batch 50/188, Loss: 0.3814\n",
      "Epoch 112/10000, Batch 60/188, Loss: 0.3043\n",
      "Epoch 112/10000, Batch 70/188, Loss: 0.4119\n",
      "Epoch 112/10000, Batch 80/188, Loss: 0.5009\n",
      "Epoch 112/10000, Batch 90/188, Loss: 0.5639\n",
      "Epoch 112/10000, Batch 100/188, Loss: 0.5464\n",
      "Epoch 112/10000, Batch 110/188, Loss: 0.3980\n",
      "Epoch 112/10000, Batch 120/188, Loss: 0.4053\n",
      "Epoch 112/10000, Batch 130/188, Loss: 0.3344\n",
      "Epoch 112/10000, Batch 140/188, Loss: 0.3016\n",
      "Epoch 112/10000, Batch 150/188, Loss: 0.5550\n",
      "Epoch 112/10000, Batch 160/188, Loss: 0.3759\n",
      "Epoch 112/10000, Batch 170/188, Loss: 0.4022\n",
      "Epoch 112/10000, Batch 180/188, Loss: 0.1357\n",
      "Epoch 112 Finished - Train Loss: 0.4148, Train IoU: 0.5870, Train F1-Score: 0.7322\n",
      "Validation Loss: 0.5650, Validation IoU: 0.4361, Validation F1-Score: 0.5774\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 10/30\n",
      "Epoch 113/10000 starting...\n",
      "Epoch 113/10000, Batch 10/188, Loss: 0.4032\n",
      "Epoch 113/10000, Batch 20/188, Loss: 0.4726\n",
      "Epoch 113/10000, Batch 30/188, Loss: 0.2976\n",
      "Epoch 113/10000, Batch 40/188, Loss: 0.3831\n",
      "Epoch 113/10000, Batch 50/188, Loss: 0.5774\n",
      "Epoch 113/10000, Batch 60/188, Loss: 0.4709\n",
      "Epoch 113/10000, Batch 70/188, Loss: 0.2968\n",
      "Epoch 113/10000, Batch 80/188, Loss: 0.5038\n",
      "Epoch 113/10000, Batch 90/188, Loss: 0.5189\n",
      "Epoch 113/10000, Batch 100/188, Loss: 0.3861\n",
      "Epoch 113/10000, Batch 110/188, Loss: 0.4527\n",
      "Epoch 113/10000, Batch 120/188, Loss: 0.5086\n",
      "Epoch 113/10000, Batch 130/188, Loss: 0.2385\n",
      "Epoch 113/10000, Batch 140/188, Loss: 0.3586\n",
      "Epoch 113/10000, Batch 150/188, Loss: 0.2676\n",
      "Epoch 113/10000, Batch 160/188, Loss: 0.4037\n",
      "Epoch 113/10000, Batch 170/188, Loss: 0.3579\n",
      "Epoch 113/10000, Batch 180/188, Loss: 0.6439\n",
      "Epoch 113 Finished - Train Loss: 0.4113, Train IoU: 0.5903, Train F1-Score: 0.7364\n",
      "Validation Loss: 0.5648, Validation IoU: 0.4364, Validation F1-Score: 0.5780\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 11/30\n",
      "Epoch 114/10000 starting...\n",
      "Epoch 114/10000, Batch 10/188, Loss: 0.4563\n",
      "Epoch 114/10000, Batch 20/188, Loss: 0.4103\n",
      "Epoch 114/10000, Batch 30/188, Loss: 0.5880\n",
      "Epoch 114/10000, Batch 40/188, Loss: 0.3284\n",
      "Epoch 114/10000, Batch 50/188, Loss: 0.5436\n",
      "Epoch 114/10000, Batch 60/188, Loss: 0.3165\n",
      "Epoch 114/10000, Batch 70/188, Loss: 0.4785\n",
      "Epoch 114/10000, Batch 80/188, Loss: 0.6242\n",
      "Epoch 114/10000, Batch 90/188, Loss: 0.4958\n",
      "Epoch 114/10000, Batch 100/188, Loss: 0.3202\n",
      "Epoch 114/10000, Batch 110/188, Loss: 0.3679\n",
      "Epoch 114/10000, Batch 120/188, Loss: 0.4074\n",
      "Epoch 114/10000, Batch 130/188, Loss: 0.4341\n",
      "Epoch 114/10000, Batch 140/188, Loss: 0.1583\n",
      "Epoch 114/10000, Batch 150/188, Loss: 0.4250\n",
      "Epoch 114/10000, Batch 160/188, Loss: 0.4544\n",
      "Epoch 114/10000, Batch 170/188, Loss: 0.3301\n",
      "Epoch 114/10000, Batch 180/188, Loss: 0.2461\n",
      "Epoch 114 Finished - Train Loss: 0.4140, Train IoU: 0.5877, Train F1-Score: 0.7345\n",
      "Validation Loss: 0.5642, Validation IoU: 0.4370, Validation F1-Score: 0.5789\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 12/30\n",
      "Epoch 115/10000 starting...\n",
      "Epoch 115/10000, Batch 10/188, Loss: 0.3564\n",
      "Epoch 115/10000, Batch 20/188, Loss: 0.2119\n",
      "Epoch 115/10000, Batch 30/188, Loss: 0.6029\n",
      "Epoch 115/10000, Batch 40/188, Loss: 0.4789\n",
      "Epoch 115/10000, Batch 50/188, Loss: 0.5791\n",
      "Epoch 115/10000, Batch 60/188, Loss: 0.4401\n",
      "Epoch 115/10000, Batch 70/188, Loss: 0.4113\n",
      "Epoch 115/10000, Batch 80/188, Loss: 0.4281\n",
      "Epoch 115/10000, Batch 90/188, Loss: 0.3086\n",
      "Epoch 115/10000, Batch 100/188, Loss: 0.3831\n",
      "Epoch 115/10000, Batch 110/188, Loss: 0.3059\n",
      "Epoch 115/10000, Batch 120/188, Loss: 0.4866\n",
      "Epoch 115/10000, Batch 130/188, Loss: 0.3335\n",
      "Epoch 115/10000, Batch 140/188, Loss: 0.3810\n",
      "Epoch 115/10000, Batch 150/188, Loss: 0.3722\n",
      "Epoch 115/10000, Batch 160/188, Loss: 0.3988\n",
      "Epoch 115/10000, Batch 170/188, Loss: 0.4301\n",
      "Epoch 115/10000, Batch 180/188, Loss: 0.4824\n",
      "Epoch 115 Finished - Train Loss: 0.4157, Train IoU: 0.5860, Train F1-Score: 0.7326\n",
      "Validation Loss: 0.5664, Validation IoU: 0.4349, Validation F1-Score: 0.5760\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 13/30\n",
      "Epoch 116/10000 starting...\n",
      "Epoch 116/10000, Batch 10/188, Loss: 0.2538\n",
      "Epoch 116/10000, Batch 20/188, Loss: 0.4709\n",
      "Epoch 116/10000, Batch 30/188, Loss: 0.3566\n",
      "Epoch 116/10000, Batch 40/188, Loss: 0.3257\n",
      "Epoch 116/10000, Batch 50/188, Loss: 0.5764\n",
      "Epoch 116/10000, Batch 60/188, Loss: 0.5894\n",
      "Epoch 116/10000, Batch 70/188, Loss: 0.3303\n",
      "Epoch 116/10000, Batch 80/188, Loss: 0.3982\n",
      "Epoch 116/10000, Batch 90/188, Loss: 0.5366\n",
      "Epoch 116/10000, Batch 100/188, Loss: 0.4326\n",
      "Epoch 116/10000, Batch 110/188, Loss: 0.3205\n",
      "Epoch 116/10000, Batch 120/188, Loss: 0.4286\n",
      "Epoch 116/10000, Batch 130/188, Loss: 0.5437\n",
      "Epoch 116/10000, Batch 140/188, Loss: 0.4650\n",
      "Epoch 116/10000, Batch 150/188, Loss: 0.3577\n",
      "Epoch 116/10000, Batch 160/188, Loss: 0.3054\n",
      "Epoch 116/10000, Batch 170/188, Loss: 0.3203\n",
      "Epoch 116/10000, Batch 180/188, Loss: 0.4644\n",
      "Epoch 116 Finished - Train Loss: 0.4125, Train IoU: 0.5892, Train F1-Score: 0.7355\n",
      "Validation Loss: 0.5667, Validation IoU: 0.4346, Validation F1-Score: 0.5756\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 14/30\n",
      "Epoch 117/10000 starting...\n",
      "Epoch 117/10000, Batch 10/188, Loss: 0.4058\n",
      "Epoch 117/10000, Batch 20/188, Loss: 0.2579\n",
      "Epoch 117/10000, Batch 30/188, Loss: 0.7092\n",
      "Epoch 117/10000, Batch 40/188, Loss: 0.3671\n",
      "Epoch 117/10000, Batch 50/188, Loss: 0.2316\n",
      "Epoch 117/10000, Batch 60/188, Loss: 0.4592\n",
      "Epoch 117/10000, Batch 70/188, Loss: 0.3643\n",
      "Epoch 117/10000, Batch 80/188, Loss: 0.5266\n",
      "Epoch 117/10000, Batch 90/188, Loss: 0.5937\n",
      "Epoch 117/10000, Batch 100/188, Loss: 0.4021\n",
      "Epoch 117/10000, Batch 110/188, Loss: 0.4758\n",
      "Epoch 117/10000, Batch 120/188, Loss: 0.4894\n",
      "Epoch 117/10000, Batch 130/188, Loss: 0.3080\n",
      "Epoch 117/10000, Batch 140/188, Loss: 0.2279\n",
      "Epoch 117/10000, Batch 150/188, Loss: 0.4155\n",
      "Epoch 117/10000, Batch 160/188, Loss: 0.4849\n",
      "Epoch 117/10000, Batch 170/188, Loss: 0.6758\n",
      "Epoch 117/10000, Batch 180/188, Loss: 0.4518\n",
      "Epoch 117 Finished - Train Loss: 0.4127, Train IoU: 0.5891, Train F1-Score: 0.7343\n",
      "Validation Loss: 0.5655, Validation IoU: 0.4356, Validation F1-Score: 0.5768\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 15/30\n",
      "Epoch 118/10000 starting...\n",
      "Epoch 118/10000, Batch 10/188, Loss: 0.5313\n",
      "Epoch 118/10000, Batch 20/188, Loss: 0.5948\n",
      "Epoch 118/10000, Batch 30/188, Loss: 0.4225\n",
      "Epoch 118/10000, Batch 40/188, Loss: 0.4438\n",
      "Epoch 118/10000, Batch 50/188, Loss: 0.2925\n",
      "Epoch 118/10000, Batch 60/188, Loss: 0.3985\n",
      "Epoch 118/10000, Batch 70/188, Loss: 0.4123\n",
      "Epoch 118/10000, Batch 80/188, Loss: 0.2792\n",
      "Epoch 118/10000, Batch 90/188, Loss: 0.6099\n",
      "Epoch 118/10000, Batch 100/188, Loss: 0.2719\n",
      "Epoch 118/10000, Batch 110/188, Loss: 0.2355\n",
      "Epoch 118/10000, Batch 120/188, Loss: 0.3860\n",
      "Epoch 118/10000, Batch 130/188, Loss: 0.3431\n",
      "Epoch 118/10000, Batch 140/188, Loss: 0.4186\n",
      "Epoch 118/10000, Batch 150/188, Loss: 0.3357\n",
      "Epoch 118/10000, Batch 160/188, Loss: 0.5021\n",
      "Epoch 118/10000, Batch 170/188, Loss: 0.5987\n",
      "Epoch 118/10000, Batch 180/188, Loss: 0.2638\n",
      "Epoch 118 Finished - Train Loss: 0.4196, Train IoU: 0.5821, Train F1-Score: 0.7279\n",
      "Validation Loss: 0.5670, Validation IoU: 0.4343, Validation F1-Score: 0.5748\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 16/30\n",
      "Epoch 119/10000 starting...\n",
      "Epoch 119/10000, Batch 10/188, Loss: 0.4170\n",
      "Epoch 119/10000, Batch 20/188, Loss: 0.4299\n",
      "Epoch 119/10000, Batch 30/188, Loss: 0.4257\n",
      "Epoch 119/10000, Batch 40/188, Loss: 0.4292\n",
      "Epoch 119/10000, Batch 50/188, Loss: 0.3763\n",
      "Epoch 119/10000, Batch 60/188, Loss: 0.5014\n",
      "Epoch 119/10000, Batch 70/188, Loss: 0.5135\n",
      "Epoch 119/10000, Batch 80/188, Loss: 0.3468\n",
      "Epoch 119/10000, Batch 90/188, Loss: 0.3976\n",
      "Epoch 119/10000, Batch 100/188, Loss: 0.2281\n",
      "Epoch 119/10000, Batch 110/188, Loss: 0.3236\n",
      "Epoch 119/10000, Batch 120/188, Loss: 0.3953\n",
      "Epoch 119/10000, Batch 130/188, Loss: 0.4094\n",
      "Epoch 119/10000, Batch 140/188, Loss: 0.2810\n",
      "Epoch 119/10000, Batch 150/188, Loss: 0.4157\n",
      "Epoch 119/10000, Batch 160/188, Loss: 0.3197\n",
      "Epoch 119/10000, Batch 170/188, Loss: 0.4981\n",
      "Epoch 119/10000, Batch 180/188, Loss: 0.4281\n",
      "Epoch 119 Finished - Train Loss: 0.4108, Train IoU: 0.5909, Train F1-Score: 0.7360\n",
      "Validation Loss: 0.5640, Validation IoU: 0.4372, Validation F1-Score: 0.5787\n",
      "Current Learning Rate: 0.00000010\n",
      "Best validation loss updated: 0.5640. Saving model and state...\n",
      "Epoch 120/10000 starting...\n",
      "Epoch 120/10000, Batch 10/188, Loss: 0.4199\n",
      "Epoch 120/10000, Batch 20/188, Loss: 0.5293\n",
      "Epoch 120/10000, Batch 30/188, Loss: 0.2764\n",
      "Epoch 120/10000, Batch 40/188, Loss: 0.4612\n",
      "Epoch 120/10000, Batch 50/188, Loss: 0.3521\n",
      "Epoch 120/10000, Batch 60/188, Loss: 0.4340\n",
      "Epoch 120/10000, Batch 70/188, Loss: 0.4412\n",
      "Epoch 120/10000, Batch 80/188, Loss: 0.3958\n",
      "Epoch 120/10000, Batch 90/188, Loss: 0.3410\n",
      "Epoch 120/10000, Batch 100/188, Loss: 0.4491\n",
      "Epoch 120/10000, Batch 110/188, Loss: 0.2292\n",
      "Epoch 120/10000, Batch 120/188, Loss: 0.3918\n",
      "Epoch 120/10000, Batch 130/188, Loss: 0.2936\n",
      "Epoch 120/10000, Batch 140/188, Loss: 0.4901\n",
      "Epoch 120/10000, Batch 150/188, Loss: 0.2492\n",
      "Epoch 120/10000, Batch 160/188, Loss: 0.4236\n",
      "Epoch 120/10000, Batch 170/188, Loss: 0.4454\n",
      "Epoch 120/10000, Batch 180/188, Loss: 0.2391\n",
      "Epoch 120 Finished - Train Loss: 0.4155, Train IoU: 0.5862, Train F1-Score: 0.7333\n",
      "Validation Loss: 0.5644, Validation IoU: 0.4367, Validation F1-Score: 0.5783\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 1/30\n",
      "Epoch 121/10000 starting...\n",
      "Epoch 121/10000, Batch 10/188, Loss: 0.4705\n",
      "Epoch 121/10000, Batch 20/188, Loss: 0.2825\n",
      "Epoch 121/10000, Batch 30/188, Loss: 0.4263\n",
      "Epoch 121/10000, Batch 40/188, Loss: 0.2968\n",
      "Epoch 121/10000, Batch 50/188, Loss: 0.4469\n",
      "Epoch 121/10000, Batch 60/188, Loss: 0.4277\n",
      "Epoch 121/10000, Batch 70/188, Loss: 0.3254\n",
      "Epoch 121/10000, Batch 80/188, Loss: 0.4493\n",
      "Epoch 121/10000, Batch 90/188, Loss: 0.4291\n",
      "Epoch 121/10000, Batch 100/188, Loss: 0.4258\n",
      "Epoch 121/10000, Batch 110/188, Loss: 0.4225\n",
      "Epoch 121/10000, Batch 120/188, Loss: 0.4078\n",
      "Epoch 121/10000, Batch 130/188, Loss: 0.1431\n",
      "Epoch 121/10000, Batch 140/188, Loss: 0.2868\n",
      "Epoch 121/10000, Batch 150/188, Loss: 0.5184\n",
      "Epoch 121/10000, Batch 160/188, Loss: 0.5004\n",
      "Epoch 121/10000, Batch 170/188, Loss: 0.4670\n",
      "Epoch 121/10000, Batch 180/188, Loss: 0.3783\n",
      "Epoch 121 Finished - Train Loss: 0.4131, Train IoU: 0.5886, Train F1-Score: 0.7347\n",
      "Validation Loss: 0.5653, Validation IoU: 0.4360, Validation F1-Score: 0.5774\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 2/30\n",
      "Epoch 122/10000 starting...\n",
      "Epoch 122/10000, Batch 10/188, Loss: 0.3457\n",
      "Epoch 122/10000, Batch 20/188, Loss: 0.5070\n",
      "Epoch 122/10000, Batch 30/188, Loss: 0.4554\n",
      "Epoch 122/10000, Batch 40/188, Loss: 0.3574\n",
      "Epoch 122/10000, Batch 50/188, Loss: 0.3946\n",
      "Epoch 122/10000, Batch 60/188, Loss: 0.6166\n",
      "Epoch 122/10000, Batch 70/188, Loss: 0.4127\n",
      "Epoch 122/10000, Batch 80/188, Loss: 0.3788\n",
      "Epoch 122/10000, Batch 90/188, Loss: 0.4526\n",
      "Epoch 122/10000, Batch 100/188, Loss: 0.4259\n",
      "Epoch 122/10000, Batch 110/188, Loss: 0.5878\n",
      "Epoch 122/10000, Batch 120/188, Loss: 0.6791\n",
      "Epoch 122/10000, Batch 130/188, Loss: 0.2574\n",
      "Epoch 122/10000, Batch 140/188, Loss: 0.3934\n",
      "Epoch 122/10000, Batch 150/188, Loss: 0.2245\n",
      "Epoch 122/10000, Batch 160/188, Loss: 0.3847\n",
      "Epoch 122/10000, Batch 170/188, Loss: 0.4641\n",
      "Epoch 122/10000, Batch 180/188, Loss: 0.5945\n",
      "Epoch 122 Finished - Train Loss: 0.4156, Train IoU: 0.5861, Train F1-Score: 0.7325\n",
      "Validation Loss: 0.5674, Validation IoU: 0.4338, Validation F1-Score: 0.5746\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 3/30\n",
      "Epoch 123/10000 starting...\n",
      "Epoch 123/10000, Batch 10/188, Loss: 0.6351\n",
      "Epoch 123/10000, Batch 20/188, Loss: 0.3780\n",
      "Epoch 123/10000, Batch 30/188, Loss: 0.5676\n",
      "Epoch 123/10000, Batch 40/188, Loss: 0.3141\n",
      "Epoch 123/10000, Batch 50/188, Loss: 0.4670\n",
      "Epoch 123/10000, Batch 60/188, Loss: 0.3987\n",
      "Epoch 123/10000, Batch 70/188, Loss: 0.3589\n",
      "Epoch 123/10000, Batch 80/188, Loss: 0.4248\n",
      "Epoch 123/10000, Batch 90/188, Loss: 0.3926\n",
      "Epoch 123/10000, Batch 100/188, Loss: 0.2861\n",
      "Epoch 123/10000, Batch 110/188, Loss: 0.3226\n",
      "Epoch 123/10000, Batch 120/188, Loss: 0.4631\n",
      "Epoch 123/10000, Batch 130/188, Loss: 0.6020\n",
      "Epoch 123/10000, Batch 140/188, Loss: 0.3258\n",
      "Epoch 123/10000, Batch 150/188, Loss: 0.3937\n",
      "Epoch 123/10000, Batch 160/188, Loss: 0.5235\n",
      "Epoch 123/10000, Batch 170/188, Loss: 0.2468\n",
      "Epoch 123/10000, Batch 180/188, Loss: 0.3690\n",
      "Epoch 123 Finished - Train Loss: 0.4109, Train IoU: 0.5908, Train F1-Score: 0.7366\n",
      "Validation Loss: 0.5655, Validation IoU: 0.4357, Validation F1-Score: 0.5772\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 4/30\n",
      "Epoch 124/10000 starting...\n",
      "Epoch 124/10000, Batch 10/188, Loss: 0.5231\n",
      "Epoch 124/10000, Batch 20/188, Loss: 0.4623\n",
      "Epoch 124/10000, Batch 30/188, Loss: 0.3647\n",
      "Epoch 124/10000, Batch 40/188, Loss: 0.4340\n",
      "Epoch 124/10000, Batch 50/188, Loss: 0.2995\n",
      "Epoch 124/10000, Batch 60/188, Loss: 0.4088\n",
      "Epoch 124/10000, Batch 70/188, Loss: 0.4374\n",
      "Epoch 124/10000, Batch 80/188, Loss: 0.3873\n",
      "Epoch 124/10000, Batch 90/188, Loss: 0.6115\n",
      "Epoch 124/10000, Batch 100/188, Loss: 0.4236\n",
      "Epoch 124/10000, Batch 110/188, Loss: 0.5380\n",
      "Epoch 124/10000, Batch 120/188, Loss: 0.4777\n",
      "Epoch 124/10000, Batch 130/188, Loss: 0.4167\n",
      "Epoch 124/10000, Batch 140/188, Loss: 0.3058\n",
      "Epoch 124/10000, Batch 150/188, Loss: 0.3738\n",
      "Epoch 124/10000, Batch 160/188, Loss: 0.5085\n",
      "Epoch 124/10000, Batch 170/188, Loss: 0.4730\n",
      "Epoch 124/10000, Batch 180/188, Loss: 0.3956\n",
      "Epoch 124 Finished - Train Loss: 0.4093, Train IoU: 0.5924, Train F1-Score: 0.7385\n",
      "Validation Loss: 0.5676, Validation IoU: 0.4337, Validation F1-Score: 0.5742\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 5/30\n",
      "Epoch 125/10000 starting...\n",
      "Epoch 125/10000, Batch 10/188, Loss: 0.3602\n",
      "Epoch 125/10000, Batch 20/188, Loss: 0.5884\n",
      "Epoch 125/10000, Batch 30/188, Loss: 0.3616\n",
      "Epoch 125/10000, Batch 40/188, Loss: 0.4505\n",
      "Epoch 125/10000, Batch 50/188, Loss: 0.2643\n",
      "Epoch 125/10000, Batch 60/188, Loss: 0.4368\n",
      "Epoch 125/10000, Batch 70/188, Loss: 0.3261\n",
      "Epoch 125/10000, Batch 80/188, Loss: 0.4147\n",
      "Epoch 125/10000, Batch 90/188, Loss: 0.4152\n",
      "Epoch 125/10000, Batch 100/188, Loss: 0.6203\n",
      "Epoch 125/10000, Batch 110/188, Loss: 0.4056\n",
      "Epoch 125/10000, Batch 120/188, Loss: 0.4985\n",
      "Epoch 125/10000, Batch 130/188, Loss: 0.4101\n",
      "Epoch 125/10000, Batch 140/188, Loss: 0.3306\n",
      "Epoch 125/10000, Batch 150/188, Loss: 0.3133\n",
      "Epoch 125/10000, Batch 160/188, Loss: 0.3588\n",
      "Epoch 125/10000, Batch 170/188, Loss: 0.5651\n",
      "Epoch 125/10000, Batch 180/188, Loss: 0.3184\n",
      "Epoch 125 Finished - Train Loss: 0.4178, Train IoU: 0.5839, Train F1-Score: 0.7313\n",
      "Validation Loss: 0.5654, Validation IoU: 0.4358, Validation F1-Score: 0.5771\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 6/30\n",
      "Epoch 126/10000 starting...\n",
      "Epoch 126/10000, Batch 10/188, Loss: 0.2391\n",
      "Epoch 126/10000, Batch 20/188, Loss: 0.4146\n",
      "Epoch 126/10000, Batch 30/188, Loss: 0.3804\n",
      "Epoch 126/10000, Batch 40/188, Loss: 0.3824\n",
      "Epoch 126/10000, Batch 50/188, Loss: 0.4750\n",
      "Epoch 126/10000, Batch 60/188, Loss: 0.4829\n",
      "Epoch 126/10000, Batch 70/188, Loss: 0.4361\n",
      "Epoch 126/10000, Batch 80/188, Loss: 0.4349\n",
      "Epoch 126/10000, Batch 90/188, Loss: 0.2929\n",
      "Epoch 126/10000, Batch 100/188, Loss: 0.4834\n",
      "Epoch 126/10000, Batch 110/188, Loss: 0.5705\n",
      "Epoch 126/10000, Batch 120/188, Loss: 0.3212\n",
      "Epoch 126/10000, Batch 130/188, Loss: 0.4209\n",
      "Epoch 126/10000, Batch 140/188, Loss: 0.4486\n",
      "Epoch 126/10000, Batch 150/188, Loss: 0.6407\n",
      "Epoch 126/10000, Batch 160/188, Loss: 0.4384\n",
      "Epoch 126/10000, Batch 170/188, Loss: 0.4477\n",
      "Epoch 126/10000, Batch 180/188, Loss: 0.6127\n",
      "Epoch 126 Finished - Train Loss: 0.4113, Train IoU: 0.5904, Train F1-Score: 0.7363\n",
      "Validation Loss: 0.5666, Validation IoU: 0.4346, Validation F1-Score: 0.5762\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 7/30\n",
      "Epoch 127/10000 starting...\n",
      "Epoch 127/10000, Batch 10/188, Loss: 0.4659\n",
      "Epoch 127/10000, Batch 20/188, Loss: 0.4122\n",
      "Epoch 127/10000, Batch 30/188, Loss: 0.5508\n",
      "Epoch 127/10000, Batch 40/188, Loss: 0.3063\n",
      "Epoch 127/10000, Batch 50/188, Loss: 0.3523\n",
      "Epoch 127/10000, Batch 60/188, Loss: 0.3849\n",
      "Epoch 127/10000, Batch 70/188, Loss: 0.4165\n",
      "Epoch 127/10000, Batch 80/188, Loss: 0.2305\n",
      "Epoch 127/10000, Batch 90/188, Loss: 0.2486\n",
      "Epoch 127/10000, Batch 100/188, Loss: 0.4757\n",
      "Epoch 127/10000, Batch 110/188, Loss: 0.4255\n",
      "Epoch 127/10000, Batch 120/188, Loss: 0.5048\n",
      "Epoch 127/10000, Batch 130/188, Loss: 0.3572\n",
      "Epoch 127/10000, Batch 140/188, Loss: 0.6602\n",
      "Epoch 127/10000, Batch 150/188, Loss: 0.4665\n",
      "Epoch 127/10000, Batch 160/188, Loss: 0.3647\n",
      "Epoch 127/10000, Batch 170/188, Loss: 0.3325\n",
      "Epoch 127/10000, Batch 180/188, Loss: 0.4461\n",
      "Epoch 127 Finished - Train Loss: 0.4171, Train IoU: 0.5846, Train F1-Score: 0.7310\n",
      "Validation Loss: 0.5682, Validation IoU: 0.4331, Validation F1-Score: 0.5733\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 8/30\n",
      "Epoch 128/10000 starting...\n",
      "Epoch 128/10000, Batch 10/188, Loss: 0.4324\n",
      "Epoch 128/10000, Batch 20/188, Loss: 0.3523\n",
      "Epoch 128/10000, Batch 30/188, Loss: 0.4783\n",
      "Epoch 128/10000, Batch 40/188, Loss: 0.3271\n",
      "Epoch 128/10000, Batch 50/188, Loss: 0.3858\n",
      "Epoch 128/10000, Batch 60/188, Loss: 0.4466\n",
      "Epoch 128/10000, Batch 70/188, Loss: 0.3946\n",
      "Epoch 128/10000, Batch 80/188, Loss: 0.3742\n",
      "Epoch 128/10000, Batch 90/188, Loss: 0.2761\n",
      "Epoch 128/10000, Batch 100/188, Loss: 0.4719\n",
      "Epoch 128/10000, Batch 110/188, Loss: 0.2441\n",
      "Epoch 128/10000, Batch 120/188, Loss: 0.4374\n",
      "Epoch 128/10000, Batch 130/188, Loss: 0.4130\n",
      "Epoch 128/10000, Batch 140/188, Loss: 0.4099\n",
      "Epoch 128/10000, Batch 150/188, Loss: 0.2631\n",
      "Epoch 128/10000, Batch 160/188, Loss: 0.3848\n",
      "Epoch 128/10000, Batch 170/188, Loss: 0.6557\n",
      "Epoch 128/10000, Batch 180/188, Loss: 0.1633\n",
      "Epoch 128 Finished - Train Loss: 0.4154, Train IoU: 0.5863, Train F1-Score: 0.7322\n",
      "Validation Loss: 0.5662, Validation IoU: 0.4351, Validation F1-Score: 0.5763\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 9/30\n",
      "Epoch 129/10000 starting...\n",
      "Epoch 129/10000, Batch 10/188, Loss: 0.3038\n",
      "Epoch 129/10000, Batch 20/188, Loss: 0.4696\n",
      "Epoch 129/10000, Batch 30/188, Loss: 0.4099\n",
      "Epoch 129/10000, Batch 40/188, Loss: 0.4465\n",
      "Epoch 129/10000, Batch 50/188, Loss: 0.2941\n",
      "Epoch 129/10000, Batch 60/188, Loss: 0.3777\n",
      "Epoch 129/10000, Batch 70/188, Loss: 0.4268\n",
      "Epoch 129/10000, Batch 80/188, Loss: 0.3458\n",
      "Epoch 129/10000, Batch 90/188, Loss: 0.4349\n",
      "Epoch 129/10000, Batch 100/188, Loss: 0.3355\n",
      "Epoch 129/10000, Batch 110/188, Loss: 0.3873\n",
      "Epoch 129/10000, Batch 120/188, Loss: 0.3119\n",
      "Epoch 129/10000, Batch 130/188, Loss: 0.4565\n",
      "Epoch 129/10000, Batch 140/188, Loss: 0.4426\n",
      "Epoch 129/10000, Batch 150/188, Loss: 0.3453\n",
      "Epoch 129/10000, Batch 160/188, Loss: 0.4111\n",
      "Epoch 129/10000, Batch 170/188, Loss: 0.3417\n",
      "Epoch 129/10000, Batch 180/188, Loss: 0.5450\n",
      "Epoch 129 Finished - Train Loss: 0.4117, Train IoU: 0.5900, Train F1-Score: 0.7355\n",
      "Validation Loss: 0.5663, Validation IoU: 0.4349, Validation F1-Score: 0.5755\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 10/30\n",
      "Epoch 130/10000 starting...\n",
      "Epoch 130/10000, Batch 10/188, Loss: 0.5148\n",
      "Epoch 130/10000, Batch 20/188, Loss: 0.2743\n",
      "Epoch 130/10000, Batch 30/188, Loss: 0.3416\n",
      "Epoch 130/10000, Batch 40/188, Loss: 0.2997\n",
      "Epoch 130/10000, Batch 50/188, Loss: 0.5319\n",
      "Epoch 130/10000, Batch 60/188, Loss: 0.4192\n",
      "Epoch 130/10000, Batch 70/188, Loss: 0.4073\n",
      "Epoch 130/10000, Batch 80/188, Loss: 0.3869\n",
      "Epoch 130/10000, Batch 90/188, Loss: 0.3180\n",
      "Epoch 130/10000, Batch 100/188, Loss: 0.4311\n",
      "Epoch 130/10000, Batch 110/188, Loss: 0.4025\n",
      "Epoch 130/10000, Batch 120/188, Loss: 0.3919\n",
      "Epoch 130/10000, Batch 130/188, Loss: 0.4658\n",
      "Epoch 130/10000, Batch 140/188, Loss: 0.3527\n",
      "Epoch 130/10000, Batch 150/188, Loss: 0.2778\n",
      "Epoch 130/10000, Batch 160/188, Loss: 0.5988\n",
      "Epoch 130/10000, Batch 170/188, Loss: 0.4408\n",
      "Epoch 130/10000, Batch 180/188, Loss: 0.4534\n",
      "Epoch 130 Finished - Train Loss: 0.4133, Train IoU: 0.5884, Train F1-Score: 0.7348\n",
      "Validation Loss: 0.5658, Validation IoU: 0.4354, Validation F1-Score: 0.5769\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 11/30\n",
      "Epoch 131/10000 starting...\n",
      "Epoch 131/10000, Batch 10/188, Loss: 0.3046\n",
      "Epoch 131/10000, Batch 20/188, Loss: 0.3472\n",
      "Epoch 131/10000, Batch 30/188, Loss: 0.5531\n",
      "Epoch 131/10000, Batch 40/188, Loss: 0.3398\n",
      "Epoch 131/10000, Batch 50/188, Loss: 0.1850\n",
      "Epoch 131/10000, Batch 60/188, Loss: 0.4840\n",
      "Epoch 131/10000, Batch 70/188, Loss: 0.4086\n",
      "Epoch 131/10000, Batch 80/188, Loss: 0.3480\n",
      "Epoch 131/10000, Batch 90/188, Loss: 0.3185\n",
      "Epoch 131/10000, Batch 100/188, Loss: 0.3716\n",
      "Epoch 131/10000, Batch 110/188, Loss: 0.2939\n",
      "Epoch 131/10000, Batch 120/188, Loss: 0.3996\n",
      "Epoch 131/10000, Batch 130/188, Loss: 0.5917\n",
      "Epoch 131/10000, Batch 140/188, Loss: 0.3736\n",
      "Epoch 131/10000, Batch 150/188, Loss: 0.2767\n",
      "Epoch 131/10000, Batch 160/188, Loss: 0.7043\n",
      "Epoch 131/10000, Batch 170/188, Loss: 0.4746\n",
      "Epoch 131/10000, Batch 180/188, Loss: 0.3052\n",
      "Epoch 131 Finished - Train Loss: 0.4158, Train IoU: 0.5859, Train F1-Score: 0.7319\n",
      "Validation Loss: 0.5635, Validation IoU: 0.4377, Validation F1-Score: 0.5800\n",
      "Current Learning Rate: 0.00000010\n",
      "Best validation loss updated: 0.5635. Saving model and state...\n",
      "Epoch 132/10000 starting...\n",
      "Epoch 132/10000, Batch 10/188, Loss: 0.5688\n",
      "Epoch 132/10000, Batch 20/188, Loss: 0.4712\n",
      "Epoch 132/10000, Batch 30/188, Loss: 0.3554\n",
      "Epoch 132/10000, Batch 40/188, Loss: 0.6311\n",
      "Epoch 132/10000, Batch 50/188, Loss: 0.4627\n",
      "Epoch 132/10000, Batch 60/188, Loss: 0.3441\n",
      "Epoch 132/10000, Batch 70/188, Loss: 0.6212\n",
      "Epoch 132/10000, Batch 80/188, Loss: 0.4080\n",
      "Epoch 132/10000, Batch 90/188, Loss: 0.2874\n",
      "Epoch 132/10000, Batch 100/188, Loss: 0.3518\n",
      "Epoch 132/10000, Batch 110/188, Loss: 0.6005\n",
      "Epoch 132/10000, Batch 120/188, Loss: 0.3276\n",
      "Epoch 132/10000, Batch 130/188, Loss: 0.2752\n",
      "Epoch 132/10000, Batch 140/188, Loss: 0.3753\n",
      "Epoch 132/10000, Batch 150/188, Loss: 0.4278\n",
      "Epoch 132/10000, Batch 160/188, Loss: 0.4024\n",
      "Epoch 132/10000, Batch 170/188, Loss: 0.4879\n",
      "Epoch 132/10000, Batch 180/188, Loss: 0.6108\n",
      "Epoch 132 Finished - Train Loss: 0.4143, Train IoU: 0.5875, Train F1-Score: 0.7341\n",
      "Validation Loss: 0.5666, Validation IoU: 0.4346, Validation F1-Score: 0.5757\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 1/30\n",
      "Epoch 133/10000 starting...\n",
      "Epoch 133/10000, Batch 10/188, Loss: 0.4555\n",
      "Epoch 133/10000, Batch 20/188, Loss: 0.1901\n",
      "Epoch 133/10000, Batch 30/188, Loss: 0.3215\n",
      "Epoch 133/10000, Batch 40/188, Loss: 0.4826\n",
      "Epoch 133/10000, Batch 50/188, Loss: 0.5943\n",
      "Epoch 133/10000, Batch 60/188, Loss: 0.5388\n",
      "Epoch 133/10000, Batch 70/188, Loss: 0.4883\n",
      "Epoch 133/10000, Batch 80/188, Loss: 0.4936\n",
      "Epoch 133/10000, Batch 90/188, Loss: 0.3350\n",
      "Epoch 133/10000, Batch 100/188, Loss: 0.6915\n",
      "Epoch 133/10000, Batch 110/188, Loss: 0.2561\n",
      "Epoch 133/10000, Batch 120/188, Loss: 0.4779\n",
      "Epoch 133/10000, Batch 130/188, Loss: 0.3929\n",
      "Epoch 133/10000, Batch 140/188, Loss: 0.4347\n",
      "Epoch 133/10000, Batch 150/188, Loss: 0.4311\n",
      "Epoch 133/10000, Batch 160/188, Loss: 0.2870\n",
      "Epoch 133/10000, Batch 170/188, Loss: 0.4192\n",
      "Epoch 133/10000, Batch 180/188, Loss: 0.3218\n",
      "Epoch 133 Finished - Train Loss: 0.4142, Train IoU: 0.5875, Train F1-Score: 0.7335\n",
      "Validation Loss: 0.5657, Validation IoU: 0.4355, Validation F1-Score: 0.5768\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 2/30\n",
      "Epoch 134/10000 starting...\n",
      "Epoch 134/10000, Batch 10/188, Loss: 0.2593\n",
      "Epoch 134/10000, Batch 20/188, Loss: 0.5616\n",
      "Epoch 134/10000, Batch 30/188, Loss: 0.4436\n",
      "Epoch 134/10000, Batch 40/188, Loss: 0.2801\n",
      "Epoch 134/10000, Batch 50/188, Loss: 0.4680\n",
      "Epoch 134/10000, Batch 60/188, Loss: 0.5348\n",
      "Epoch 134/10000, Batch 70/188, Loss: 0.3233\n",
      "Epoch 134/10000, Batch 80/188, Loss: 0.3709\n",
      "Epoch 134/10000, Batch 90/188, Loss: 0.4262\n",
      "Epoch 134/10000, Batch 100/188, Loss: 0.2815\n",
      "Epoch 134/10000, Batch 110/188, Loss: 0.2364\n",
      "Epoch 134/10000, Batch 120/188, Loss: 0.3365\n",
      "Epoch 134/10000, Batch 130/188, Loss: 0.3454\n",
      "Epoch 134/10000, Batch 140/188, Loss: 0.3758\n",
      "Epoch 134/10000, Batch 150/188, Loss: 0.3743\n",
      "Epoch 134/10000, Batch 160/188, Loss: 0.8280\n",
      "Epoch 134/10000, Batch 170/188, Loss: 0.4296\n",
      "Epoch 134/10000, Batch 180/188, Loss: 0.6583\n",
      "Epoch 134 Finished - Train Loss: 0.4108, Train IoU: 0.5910, Train F1-Score: 0.7367\n",
      "Validation Loss: 0.5668, Validation IoU: 0.4343, Validation F1-Score: 0.5758\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 3/30\n",
      "Epoch 135/10000 starting...\n",
      "Epoch 135/10000, Batch 10/188, Loss: 0.6026\n",
      "Epoch 135/10000, Batch 20/188, Loss: 0.4013\n",
      "Epoch 135/10000, Batch 30/188, Loss: 0.4723\n",
      "Epoch 135/10000, Batch 40/188, Loss: 0.4114\n",
      "Epoch 135/10000, Batch 50/188, Loss: 0.2538\n",
      "Epoch 135/10000, Batch 60/188, Loss: 0.3597\n",
      "Epoch 135/10000, Batch 70/188, Loss: 0.4630\n",
      "Epoch 135/10000, Batch 80/188, Loss: 0.4232\n",
      "Epoch 135/10000, Batch 90/188, Loss: 0.3949\n",
      "Epoch 135/10000, Batch 100/188, Loss: 0.4925\n",
      "Epoch 135/10000, Batch 110/188, Loss: 0.4642\n",
      "Epoch 135/10000, Batch 120/188, Loss: 0.4850\n",
      "Epoch 135/10000, Batch 130/188, Loss: 0.5660\n",
      "Epoch 135/10000, Batch 140/188, Loss: 0.2972\n",
      "Epoch 135/10000, Batch 150/188, Loss: 0.3247\n",
      "Epoch 135/10000, Batch 160/188, Loss: 0.5061\n",
      "Epoch 135/10000, Batch 170/188, Loss: 0.4423\n",
      "Epoch 135/10000, Batch 180/188, Loss: 0.4961\n",
      "Epoch 135 Finished - Train Loss: 0.4135, Train IoU: 0.5883, Train F1-Score: 0.7348\n",
      "Validation Loss: 0.5647, Validation IoU: 0.4366, Validation F1-Score: 0.5781\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 4/30\n",
      "Epoch 136/10000 starting...\n",
      "Epoch 136/10000, Batch 10/188, Loss: 0.4389\n",
      "Epoch 136/10000, Batch 20/188, Loss: 0.4585\n",
      "Epoch 136/10000, Batch 30/188, Loss: 0.2298\n",
      "Epoch 136/10000, Batch 40/188, Loss: 0.4342\n",
      "Epoch 136/10000, Batch 50/188, Loss: 0.4605\n",
      "Epoch 136/10000, Batch 60/188, Loss: 0.5724\n",
      "Epoch 136/10000, Batch 70/188, Loss: 0.7059\n",
      "Epoch 136/10000, Batch 80/188, Loss: 0.5002\n",
      "Epoch 136/10000, Batch 90/188, Loss: 0.5357\n",
      "Epoch 136/10000, Batch 100/188, Loss: 0.3323\n",
      "Epoch 136/10000, Batch 110/188, Loss: 0.3821\n",
      "Epoch 136/10000, Batch 120/188, Loss: 0.4250\n",
      "Epoch 136/10000, Batch 130/188, Loss: 0.4810\n",
      "Epoch 136/10000, Batch 140/188, Loss: 0.4881\n",
      "Epoch 136/10000, Batch 150/188, Loss: 0.3515\n",
      "Epoch 136/10000, Batch 160/188, Loss: 0.2936\n",
      "Epoch 136/10000, Batch 170/188, Loss: 0.2640\n",
      "Epoch 136/10000, Batch 180/188, Loss: 0.4273\n",
      "Epoch 136 Finished - Train Loss: 0.4120, Train IoU: 0.5897, Train F1-Score: 0.7352\n",
      "Validation Loss: 0.5667, Validation IoU: 0.4346, Validation F1-Score: 0.5756\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 5/30\n",
      "Epoch 137/10000 starting...\n",
      "Epoch 137/10000, Batch 10/188, Loss: 0.3141\n",
      "Epoch 137/10000, Batch 20/188, Loss: 0.4078\n",
      "Epoch 137/10000, Batch 30/188, Loss: 0.3117\n",
      "Epoch 137/10000, Batch 40/188, Loss: 0.2351\n",
      "Epoch 137/10000, Batch 50/188, Loss: 0.4395\n",
      "Epoch 137/10000, Batch 60/188, Loss: 0.5932\n",
      "Epoch 137/10000, Batch 70/188, Loss: 0.6019\n",
      "Epoch 137/10000, Batch 80/188, Loss: 0.2474\n",
      "Epoch 137/10000, Batch 90/188, Loss: 0.4837\n",
      "Epoch 137/10000, Batch 100/188, Loss: 0.4345\n",
      "Epoch 137/10000, Batch 110/188, Loss: 0.6106\n",
      "Epoch 137/10000, Batch 120/188, Loss: 0.2820\n",
      "Epoch 137/10000, Batch 130/188, Loss: 0.5180\n",
      "Epoch 137/10000, Batch 140/188, Loss: 0.3072\n",
      "Epoch 137/10000, Batch 150/188, Loss: 0.6849\n",
      "Epoch 137/10000, Batch 160/188, Loss: 0.4937\n",
      "Epoch 137/10000, Batch 170/188, Loss: 0.5283\n",
      "Epoch 137/10000, Batch 180/188, Loss: 0.3018\n",
      "Epoch 137 Finished - Train Loss: 0.4148, Train IoU: 0.5868, Train F1-Score: 0.7316\n",
      "Validation Loss: 0.5652, Validation IoU: 0.4360, Validation F1-Score: 0.5777\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 6/30\n",
      "Epoch 138/10000 starting...\n",
      "Epoch 138/10000, Batch 10/188, Loss: 0.3798\n",
      "Epoch 138/10000, Batch 20/188, Loss: 0.2180\n",
      "Epoch 138/10000, Batch 30/188, Loss: 0.3752\n",
      "Epoch 138/10000, Batch 40/188, Loss: 0.4165\n",
      "Epoch 138/10000, Batch 50/188, Loss: 0.2074\n",
      "Epoch 138/10000, Batch 60/188, Loss: 0.5197\n",
      "Epoch 138/10000, Batch 70/188, Loss: 0.4424\n",
      "Epoch 138/10000, Batch 80/188, Loss: 0.2622\n",
      "Epoch 138/10000, Batch 90/188, Loss: 0.3149\n",
      "Epoch 138/10000, Batch 100/188, Loss: 0.6238\n",
      "Epoch 138/10000, Batch 110/188, Loss: 0.3877\n",
      "Epoch 138/10000, Batch 120/188, Loss: 0.3035\n",
      "Epoch 138/10000, Batch 130/188, Loss: 0.3473\n",
      "Epoch 138/10000, Batch 140/188, Loss: 0.5204\n",
      "Epoch 138/10000, Batch 150/188, Loss: 0.3361\n",
      "Epoch 138/10000, Batch 160/188, Loss: 0.4923\n",
      "Epoch 138/10000, Batch 170/188, Loss: 0.4133\n",
      "Epoch 138/10000, Batch 180/188, Loss: 0.4159\n",
      "Epoch 138 Finished - Train Loss: 0.4144, Train IoU: 0.5873, Train F1-Score: 0.7338\n",
      "Validation Loss: 0.5676, Validation IoU: 0.4336, Validation F1-Score: 0.5746\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 7/30\n",
      "Epoch 139/10000 starting...\n",
      "Epoch 139/10000, Batch 10/188, Loss: 0.4450\n",
      "Epoch 139/10000, Batch 20/188, Loss: 0.2656\n",
      "Epoch 139/10000, Batch 30/188, Loss: 0.4665\n",
      "Epoch 139/10000, Batch 40/188, Loss: 0.5079\n",
      "Epoch 139/10000, Batch 50/188, Loss: 0.3843\n",
      "Epoch 139/10000, Batch 60/188, Loss: 0.6006\n",
      "Epoch 139/10000, Batch 70/188, Loss: 0.2215\n",
      "Epoch 139/10000, Batch 80/188, Loss: 0.4338\n",
      "Epoch 139/10000, Batch 90/188, Loss: 0.4611\n",
      "Epoch 139/10000, Batch 100/188, Loss: 0.4165\n",
      "Epoch 139/10000, Batch 110/188, Loss: 0.6238\n",
      "Epoch 139/10000, Batch 120/188, Loss: 0.2801\n",
      "Epoch 139/10000, Batch 130/188, Loss: 0.4799\n",
      "Epoch 139/10000, Batch 140/188, Loss: 0.2346\n",
      "Epoch 139/10000, Batch 150/188, Loss: 0.2523\n",
      "Epoch 139/10000, Batch 160/188, Loss: 0.3125\n",
      "Epoch 139/10000, Batch 170/188, Loss: 0.3473\n",
      "Epoch 139/10000, Batch 180/188, Loss: 0.4637\n",
      "Epoch 139 Finished - Train Loss: 0.4101, Train IoU: 0.5915, Train F1-Score: 0.7367\n",
      "Validation Loss: 0.5643, Validation IoU: 0.4368, Validation F1-Score: 0.5789\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 8/30\n",
      "Epoch 140/10000 starting...\n",
      "Epoch 140/10000, Batch 10/188, Loss: 0.4426\n",
      "Epoch 140/10000, Batch 20/188, Loss: 0.3115\n",
      "Epoch 140/10000, Batch 30/188, Loss: 0.4377\n",
      "Epoch 140/10000, Batch 40/188, Loss: 0.3687\n",
      "Epoch 140/10000, Batch 50/188, Loss: 0.4463\n",
      "Epoch 140/10000, Batch 60/188, Loss: 0.4559\n",
      "Epoch 140/10000, Batch 70/188, Loss: 0.4305\n",
      "Epoch 140/10000, Batch 80/188, Loss: 0.5521\n",
      "Epoch 140/10000, Batch 90/188, Loss: 0.7104\n",
      "Epoch 140/10000, Batch 100/188, Loss: 0.3520\n",
      "Epoch 140/10000, Batch 110/188, Loss: 0.4413\n",
      "Epoch 140/10000, Batch 120/188, Loss: 0.4508\n",
      "Epoch 140/10000, Batch 130/188, Loss: 0.1723\n",
      "Epoch 140/10000, Batch 140/188, Loss: 0.6173\n",
      "Epoch 140/10000, Batch 150/188, Loss: 0.5733\n",
      "Epoch 140/10000, Batch 160/188, Loss: 0.4214\n",
      "Epoch 140/10000, Batch 170/188, Loss: 0.4969\n",
      "Epoch 140/10000, Batch 180/188, Loss: 0.6506\n",
      "Epoch 140 Finished - Train Loss: 0.4125, Train IoU: 0.5892, Train F1-Score: 0.7345\n",
      "Validation Loss: 0.5645, Validation IoU: 0.4366, Validation F1-Score: 0.5783\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 9/30\n",
      "Epoch 141/10000 starting...\n",
      "Epoch 141/10000, Batch 10/188, Loss: 0.6546\n",
      "Epoch 141/10000, Batch 20/188, Loss: 0.6444\n",
      "Epoch 141/10000, Batch 30/188, Loss: 0.5706\n",
      "Epoch 141/10000, Batch 40/188, Loss: 0.4322\n",
      "Epoch 141/10000, Batch 50/188, Loss: 0.2866\n",
      "Epoch 141/10000, Batch 60/188, Loss: 0.4572\n",
      "Epoch 141/10000, Batch 70/188, Loss: 0.4722\n",
      "Epoch 141/10000, Batch 80/188, Loss: 0.2395\n",
      "Epoch 141/10000, Batch 90/188, Loss: 0.4061\n",
      "Epoch 141/10000, Batch 100/188, Loss: 0.4291\n",
      "Epoch 141/10000, Batch 110/188, Loss: 0.5930\n",
      "Epoch 141/10000, Batch 120/188, Loss: 0.3617\n",
      "Epoch 141/10000, Batch 130/188, Loss: 0.3920\n",
      "Epoch 141/10000, Batch 140/188, Loss: 0.3299\n",
      "Epoch 141/10000, Batch 150/188, Loss: 0.3069\n",
      "Epoch 141/10000, Batch 160/188, Loss: 0.6396\n",
      "Epoch 141/10000, Batch 170/188, Loss: 0.3997\n",
      "Epoch 141/10000, Batch 180/188, Loss: 0.2490\n",
      "Epoch 141 Finished - Train Loss: 0.4167, Train IoU: 0.5850, Train F1-Score: 0.7313\n",
      "Validation Loss: 0.5656, Validation IoU: 0.4357, Validation F1-Score: 0.5768\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 10/30\n",
      "Epoch 142/10000 starting...\n",
      "Epoch 142/10000, Batch 10/188, Loss: 0.7499\n",
      "Epoch 142/10000, Batch 20/188, Loss: 0.2995\n",
      "Epoch 142/10000, Batch 30/188, Loss: 0.6430\n",
      "Epoch 142/10000, Batch 40/188, Loss: 0.3188\n",
      "Epoch 142/10000, Batch 50/188, Loss: 0.4330\n",
      "Epoch 142/10000, Batch 60/188, Loss: 0.4849\n",
      "Epoch 142/10000, Batch 70/188, Loss: 0.6354\n",
      "Epoch 142/10000, Batch 80/188, Loss: 0.3793\n",
      "Epoch 142/10000, Batch 90/188, Loss: 0.6152\n",
      "Epoch 142/10000, Batch 100/188, Loss: 0.3752\n",
      "Epoch 142/10000, Batch 110/188, Loss: 0.5617\n",
      "Epoch 142/10000, Batch 120/188, Loss: 0.6058\n",
      "Epoch 142/10000, Batch 130/188, Loss: 0.4418\n",
      "Epoch 142/10000, Batch 140/188, Loss: 0.4480\n",
      "Epoch 142/10000, Batch 150/188, Loss: 0.4605\n",
      "Epoch 142/10000, Batch 160/188, Loss: 0.2103\n",
      "Epoch 142/10000, Batch 170/188, Loss: 0.2468\n",
      "Epoch 142/10000, Batch 180/188, Loss: 0.3714\n",
      "Epoch 142 Finished - Train Loss: 0.4086, Train IoU: 0.5931, Train F1-Score: 0.7386\n",
      "Validation Loss: 0.5650, Validation IoU: 0.4362, Validation F1-Score: 0.5777\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 11/30\n",
      "Epoch 143/10000 starting...\n",
      "Epoch 143/10000, Batch 10/188, Loss: 0.3902\n",
      "Epoch 143/10000, Batch 20/188, Loss: 0.4819\n",
      "Epoch 143/10000, Batch 30/188, Loss: 0.5884\n",
      "Epoch 143/10000, Batch 40/188, Loss: 0.2815\n",
      "Epoch 143/10000, Batch 50/188, Loss: 0.3676\n",
      "Epoch 143/10000, Batch 60/188, Loss: 0.5336\n",
      "Epoch 143/10000, Batch 70/188, Loss: 0.4570\n",
      "Epoch 143/10000, Batch 80/188, Loss: 0.4153\n",
      "Epoch 143/10000, Batch 90/188, Loss: 0.4729\n",
      "Epoch 143/10000, Batch 100/188, Loss: 0.6373\n",
      "Epoch 143/10000, Batch 110/188, Loss: 0.3220\n",
      "Epoch 143/10000, Batch 120/188, Loss: 0.3555\n",
      "Epoch 143/10000, Batch 130/188, Loss: 0.5679\n",
      "Epoch 143/10000, Batch 140/188, Loss: 0.2736\n",
      "Epoch 143/10000, Batch 150/188, Loss: 0.5003\n",
      "Epoch 143/10000, Batch 160/188, Loss: 0.3863\n",
      "Epoch 143/10000, Batch 170/188, Loss: 0.3408\n",
      "Epoch 143/10000, Batch 180/188, Loss: 0.3762\n",
      "Epoch 143 Finished - Train Loss: 0.4147, Train IoU: 0.5870, Train F1-Score: 0.7334\n",
      "Validation Loss: 0.5637, Validation IoU: 0.4376, Validation F1-Score: 0.5800\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 12/30\n",
      "Epoch 144/10000 starting...\n",
      "Epoch 144/10000, Batch 10/188, Loss: 0.4946\n",
      "Epoch 144/10000, Batch 20/188, Loss: 0.4449\n",
      "Epoch 144/10000, Batch 30/188, Loss: 0.4112\n",
      "Epoch 144/10000, Batch 40/188, Loss: 0.4690\n",
      "Epoch 144/10000, Batch 50/188, Loss: 0.4815\n",
      "Epoch 144/10000, Batch 60/188, Loss: 0.4642\n",
      "Epoch 144/10000, Batch 70/188, Loss: 0.7365\n",
      "Epoch 144/10000, Batch 80/188, Loss: 0.4603\n",
      "Epoch 144/10000, Batch 90/188, Loss: 0.6535\n",
      "Epoch 144/10000, Batch 100/188, Loss: 0.3857\n",
      "Epoch 144/10000, Batch 110/188, Loss: 0.3485\n",
      "Epoch 144/10000, Batch 120/188, Loss: 0.3558\n",
      "Epoch 144/10000, Batch 130/188, Loss: 0.3256\n",
      "Epoch 144/10000, Batch 140/188, Loss: 0.2478\n",
      "Epoch 144/10000, Batch 150/188, Loss: 0.5323\n",
      "Epoch 144/10000, Batch 160/188, Loss: 0.4148\n",
      "Epoch 144/10000, Batch 170/188, Loss: 0.3261\n",
      "Epoch 144/10000, Batch 180/188, Loss: 0.2762\n",
      "Epoch 144 Finished - Train Loss: 0.4130, Train IoU: 0.5887, Train F1-Score: 0.7346\n",
      "Validation Loss: 0.5658, Validation IoU: 0.4354, Validation F1-Score: 0.5766\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 13/30\n",
      "Epoch 145/10000 starting...\n",
      "Epoch 145/10000, Batch 10/188, Loss: 0.4089\n",
      "Epoch 145/10000, Batch 20/188, Loss: 0.4239\n",
      "Epoch 145/10000, Batch 30/188, Loss: 0.4148\n",
      "Epoch 145/10000, Batch 40/188, Loss: 0.5003\n",
      "Epoch 145/10000, Batch 50/188, Loss: 0.2687\n",
      "Epoch 145/10000, Batch 60/188, Loss: 0.3283\n",
      "Epoch 145/10000, Batch 70/188, Loss: 0.3657\n",
      "Epoch 145/10000, Batch 80/188, Loss: 0.4191\n",
      "Epoch 145/10000, Batch 90/188, Loss: 0.2992\n",
      "Epoch 145/10000, Batch 100/188, Loss: 0.5127\n",
      "Epoch 145/10000, Batch 110/188, Loss: 0.5478\n",
      "Epoch 145/10000, Batch 120/188, Loss: 0.3669\n",
      "Epoch 145/10000, Batch 130/188, Loss: 0.3666\n",
      "Epoch 145/10000, Batch 140/188, Loss: 0.3513\n",
      "Epoch 145/10000, Batch 150/188, Loss: 0.6109\n",
      "Epoch 145/10000, Batch 160/188, Loss: 0.3792\n",
      "Epoch 145/10000, Batch 170/188, Loss: 0.2058\n",
      "Epoch 145/10000, Batch 180/188, Loss: 0.3760\n",
      "Epoch 145 Finished - Train Loss: 0.4077, Train IoU: 0.5940, Train F1-Score: 0.7396\n",
      "Validation Loss: 0.5650, Validation IoU: 0.4362, Validation F1-Score: 0.5776\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 14/30\n",
      "Epoch 146/10000 starting...\n",
      "Epoch 146/10000, Batch 10/188, Loss: 0.3595\n",
      "Epoch 146/10000, Batch 20/188, Loss: 0.2929\n",
      "Epoch 146/10000, Batch 30/188, Loss: 0.5128\n",
      "Epoch 146/10000, Batch 40/188, Loss: 0.3106\n",
      "Epoch 146/10000, Batch 50/188, Loss: 0.4663\n",
      "Epoch 146/10000, Batch 60/188, Loss: 0.4494\n",
      "Epoch 146/10000, Batch 70/188, Loss: 0.4020\n",
      "Epoch 146/10000, Batch 80/188, Loss: 0.3542\n",
      "Epoch 146/10000, Batch 90/188, Loss: 0.5320\n",
      "Epoch 146/10000, Batch 100/188, Loss: 0.5920\n",
      "Epoch 146/10000, Batch 110/188, Loss: 0.3914\n",
      "Epoch 146/10000, Batch 120/188, Loss: 0.5756\n",
      "Epoch 146/10000, Batch 130/188, Loss: 0.4287\n",
      "Epoch 146/10000, Batch 140/188, Loss: 0.5475\n",
      "Epoch 146/10000, Batch 150/188, Loss: 0.2641\n",
      "Epoch 146/10000, Batch 160/188, Loss: 0.7239\n",
      "Epoch 146/10000, Batch 170/188, Loss: 0.4280\n",
      "Epoch 146/10000, Batch 180/188, Loss: 0.4002\n",
      "Epoch 146 Finished - Train Loss: 0.4092, Train IoU: 0.5924, Train F1-Score: 0.7379\n",
      "Validation Loss: 0.5652, Validation IoU: 0.4360, Validation F1-Score: 0.5775\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 15/30\n",
      "Epoch 147/10000 starting...\n",
      "Epoch 147/10000, Batch 10/188, Loss: 0.3973\n",
      "Epoch 147/10000, Batch 20/188, Loss: 0.4124\n",
      "Epoch 147/10000, Batch 30/188, Loss: 0.4743\n",
      "Epoch 147/10000, Batch 40/188, Loss: 0.4194\n",
      "Epoch 147/10000, Batch 50/188, Loss: 0.4385\n",
      "Epoch 147/10000, Batch 60/188, Loss: 0.2972\n",
      "Epoch 147/10000, Batch 70/188, Loss: 0.3508\n",
      "Epoch 147/10000, Batch 80/188, Loss: 0.3730\n",
      "Epoch 147/10000, Batch 90/188, Loss: 0.4539\n",
      "Epoch 147/10000, Batch 100/188, Loss: 0.3230\n",
      "Epoch 147/10000, Batch 110/188, Loss: 0.3157\n",
      "Epoch 147/10000, Batch 120/188, Loss: 0.4831\n",
      "Epoch 147/10000, Batch 130/188, Loss: 0.2328\n",
      "Epoch 147/10000, Batch 140/188, Loss: 0.2658\n",
      "Epoch 147/10000, Batch 150/188, Loss: 0.4943\n",
      "Epoch 147/10000, Batch 160/188, Loss: 0.4605\n",
      "Epoch 147/10000, Batch 170/188, Loss: 0.4469\n",
      "Epoch 147/10000, Batch 180/188, Loss: 0.3665\n",
      "Epoch 147 Finished - Train Loss: 0.4183, Train IoU: 0.5834, Train F1-Score: 0.7304\n",
      "Validation Loss: 0.5637, Validation IoU: 0.4374, Validation F1-Score: 0.5795\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 16/30\n",
      "Epoch 148/10000 starting...\n",
      "Epoch 148/10000, Batch 10/188, Loss: 0.2045\n",
      "Epoch 148/10000, Batch 20/188, Loss: 0.5173\n",
      "Epoch 148/10000, Batch 30/188, Loss: 0.1689\n",
      "Epoch 148/10000, Batch 40/188, Loss: 0.2456\n",
      "Epoch 148/10000, Batch 50/188, Loss: 0.3875\n",
      "Epoch 148/10000, Batch 60/188, Loss: 0.3997\n",
      "Epoch 148/10000, Batch 70/188, Loss: 0.5026\n",
      "Epoch 148/10000, Batch 80/188, Loss: 0.2886\n",
      "Epoch 148/10000, Batch 90/188, Loss: 0.3991\n",
      "Epoch 148/10000, Batch 100/188, Loss: 0.2438\n",
      "Epoch 148/10000, Batch 110/188, Loss: 0.3831\n",
      "Epoch 148/10000, Batch 120/188, Loss: 0.2956\n",
      "Epoch 148/10000, Batch 130/188, Loss: 0.5668\n",
      "Epoch 148/10000, Batch 140/188, Loss: 0.4715\n",
      "Epoch 148/10000, Batch 150/188, Loss: 0.5887\n",
      "Epoch 148/10000, Batch 160/188, Loss: 0.2466\n",
      "Epoch 148/10000, Batch 170/188, Loss: 0.3828\n",
      "Epoch 148/10000, Batch 180/188, Loss: 0.5262\n",
      "Epoch 148 Finished - Train Loss: 0.4111, Train IoU: 0.5906, Train F1-Score: 0.7366\n",
      "Validation Loss: 0.5651, Validation IoU: 0.4359, Validation F1-Score: 0.5779\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 17/30\n",
      "Epoch 149/10000 starting...\n",
      "Epoch 149/10000, Batch 10/188, Loss: 0.3502\n",
      "Epoch 149/10000, Batch 20/188, Loss: 0.2782\n",
      "Epoch 149/10000, Batch 30/188, Loss: 0.3315\n",
      "Epoch 149/10000, Batch 40/188, Loss: 0.5939\n",
      "Epoch 149/10000, Batch 50/188, Loss: 0.6132\n",
      "Epoch 149/10000, Batch 60/188, Loss: 0.4516\n",
      "Epoch 149/10000, Batch 70/188, Loss: 0.3824\n",
      "Epoch 149/10000, Batch 80/188, Loss: 0.5294\n",
      "Epoch 149/10000, Batch 90/188, Loss: 0.4535\n",
      "Epoch 149/10000, Batch 100/188, Loss: 0.4056\n",
      "Epoch 149/10000, Batch 110/188, Loss: 0.2176\n",
      "Epoch 149/10000, Batch 120/188, Loss: 0.2587\n",
      "Epoch 149/10000, Batch 130/188, Loss: 0.2758\n",
      "Epoch 149/10000, Batch 140/188, Loss: 0.4598\n",
      "Epoch 149/10000, Batch 150/188, Loss: 0.3384\n",
      "Epoch 149/10000, Batch 160/188, Loss: 0.4652\n",
      "Epoch 149/10000, Batch 170/188, Loss: 0.3675\n",
      "Epoch 149/10000, Batch 180/188, Loss: 0.3577\n",
      "Epoch 149 Finished - Train Loss: 0.4130, Train IoU: 0.5888, Train F1-Score: 0.7337\n",
      "Validation Loss: 0.5640, Validation IoU: 0.4371, Validation F1-Score: 0.5789\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 18/30\n",
      "Epoch 150/10000 starting...\n",
      "Epoch 150/10000, Batch 10/188, Loss: 0.4293\n",
      "Epoch 150/10000, Batch 20/188, Loss: 0.3342\n",
      "Epoch 150/10000, Batch 30/188, Loss: 0.5276\n",
      "Epoch 150/10000, Batch 40/188, Loss: 0.5399\n",
      "Epoch 150/10000, Batch 50/188, Loss: 0.4779\n",
      "Epoch 150/10000, Batch 60/188, Loss: 0.4745\n",
      "Epoch 150/10000, Batch 70/188, Loss: 0.4077\n",
      "Epoch 150/10000, Batch 80/188, Loss: 0.3309\n",
      "Epoch 150/10000, Batch 90/188, Loss: 0.3516\n",
      "Epoch 150/10000, Batch 100/188, Loss: 0.5221\n",
      "Epoch 150/10000, Batch 110/188, Loss: 0.4873\n",
      "Epoch 150/10000, Batch 120/188, Loss: 0.2592\n",
      "Epoch 150/10000, Batch 130/188, Loss: 0.5754\n",
      "Epoch 150/10000, Batch 140/188, Loss: 0.3119\n",
      "Epoch 150/10000, Batch 150/188, Loss: 0.4667\n",
      "Epoch 150/10000, Batch 160/188, Loss: 0.4255\n",
      "Epoch 150/10000, Batch 170/188, Loss: 0.5122\n",
      "Epoch 150/10000, Batch 180/188, Loss: 0.4478\n",
      "Epoch 150 Finished - Train Loss: 0.4126, Train IoU: 0.5890, Train F1-Score: 0.7346\n",
      "Validation Loss: 0.5651, Validation IoU: 0.4359, Validation F1-Score: 0.5781\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 19/30\n",
      "Epoch 151/10000 starting...\n",
      "Epoch 151/10000, Batch 10/188, Loss: 0.2843\n",
      "Epoch 151/10000, Batch 20/188, Loss: 0.2324\n",
      "Epoch 151/10000, Batch 30/188, Loss: 0.3455\n",
      "Epoch 151/10000, Batch 40/188, Loss: 0.4357\n",
      "Epoch 151/10000, Batch 50/188, Loss: 0.4660\n",
      "Epoch 151/10000, Batch 60/188, Loss: 0.2416\n",
      "Epoch 151/10000, Batch 70/188, Loss: 0.4944\n",
      "Epoch 151/10000, Batch 80/188, Loss: 0.2709\n",
      "Epoch 151/10000, Batch 90/188, Loss: 0.5217\n",
      "Epoch 151/10000, Batch 100/188, Loss: 0.3172\n",
      "Epoch 151/10000, Batch 110/188, Loss: 0.3605\n",
      "Epoch 151/10000, Batch 120/188, Loss: 0.5767\n",
      "Epoch 151/10000, Batch 130/188, Loss: 0.3833\n",
      "Epoch 151/10000, Batch 140/188, Loss: 0.4074\n",
      "Epoch 151/10000, Batch 150/188, Loss: 0.3275\n",
      "Epoch 151/10000, Batch 160/188, Loss: 0.3984\n",
      "Epoch 151/10000, Batch 170/188, Loss: 0.5330\n",
      "Epoch 151/10000, Batch 180/188, Loss: 0.4205\n",
      "Epoch 151 Finished - Train Loss: 0.4141, Train IoU: 0.5876, Train F1-Score: 0.7343\n",
      "Validation Loss: 0.5670, Validation IoU: 0.4343, Validation F1-Score: 0.5755\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 20/30\n",
      "Epoch 152/10000 starting...\n",
      "Epoch 152/10000, Batch 10/188, Loss: 0.4039\n",
      "Epoch 152/10000, Batch 20/188, Loss: 0.7017\n",
      "Epoch 152/10000, Batch 30/188, Loss: 0.3683\n",
      "Epoch 152/10000, Batch 40/188, Loss: 0.5595\n",
      "Epoch 152/10000, Batch 50/188, Loss: 0.3843\n",
      "Epoch 152/10000, Batch 60/188, Loss: 0.6132\n",
      "Epoch 152/10000, Batch 70/188, Loss: 0.6210\n",
      "Epoch 152/10000, Batch 80/188, Loss: 0.3982\n",
      "Epoch 152/10000, Batch 90/188, Loss: 0.5437\n",
      "Epoch 152/10000, Batch 100/188, Loss: 0.3287\n",
      "Epoch 152/10000, Batch 110/188, Loss: 0.3468\n",
      "Epoch 152/10000, Batch 120/188, Loss: 0.5228\n",
      "Epoch 152/10000, Batch 130/188, Loss: 0.3616\n",
      "Epoch 152/10000, Batch 140/188, Loss: 0.4315\n",
      "Epoch 152/10000, Batch 150/188, Loss: 0.5916\n",
      "Epoch 152/10000, Batch 160/188, Loss: 0.2850\n",
      "Epoch 152/10000, Batch 170/188, Loss: 0.6141\n",
      "Epoch 152/10000, Batch 180/188, Loss: 0.2943\n",
      "Epoch 152 Finished - Train Loss: 0.4149, Train IoU: 0.5868, Train F1-Score: 0.7323\n",
      "Validation Loss: 0.5654, Validation IoU: 0.4357, Validation F1-Score: 0.5771\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 21/30\n",
      "Epoch 153/10000 starting...\n",
      "Epoch 153/10000, Batch 10/188, Loss: 0.2851\n",
      "Epoch 153/10000, Batch 20/188, Loss: 0.5827\n",
      "Epoch 153/10000, Batch 30/188, Loss: 0.4324\n",
      "Epoch 153/10000, Batch 40/188, Loss: 0.4641\n",
      "Epoch 153/10000, Batch 50/188, Loss: 0.4119\n",
      "Epoch 153/10000, Batch 60/188, Loss: 0.4949\n",
      "Epoch 153/10000, Batch 70/188, Loss: 0.3995\n",
      "Epoch 153/10000, Batch 80/188, Loss: 0.3327\n",
      "Epoch 153/10000, Batch 90/188, Loss: 0.3620\n",
      "Epoch 153/10000, Batch 100/188, Loss: 0.2955\n",
      "Epoch 153/10000, Batch 110/188, Loss: 0.3588\n",
      "Epoch 153/10000, Batch 120/188, Loss: 0.3956\n",
      "Epoch 153/10000, Batch 130/188, Loss: 0.4253\n",
      "Epoch 153/10000, Batch 140/188, Loss: 0.5721\n",
      "Epoch 153/10000, Batch 150/188, Loss: 0.5059\n",
      "Epoch 153/10000, Batch 160/188, Loss: 0.4361\n",
      "Epoch 153/10000, Batch 170/188, Loss: 0.4479\n",
      "Epoch 153/10000, Batch 180/188, Loss: 0.3980\n",
      "Epoch 153 Finished - Train Loss: 0.4130, Train IoU: 0.5886, Train F1-Score: 0.7350\n",
      "Validation Loss: 0.5654, Validation IoU: 0.4357, Validation F1-Score: 0.5774\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 22/30\n",
      "Epoch 154/10000 starting...\n",
      "Epoch 154/10000, Batch 10/188, Loss: 0.5343\n",
      "Epoch 154/10000, Batch 20/188, Loss: 0.6540\n",
      "Epoch 154/10000, Batch 30/188, Loss: 0.4068\n",
      "Epoch 154/10000, Batch 40/188, Loss: 0.3411\n",
      "Epoch 154/10000, Batch 50/188, Loss: 0.4629\n",
      "Epoch 154/10000, Batch 60/188, Loss: 0.6680\n",
      "Epoch 154/10000, Batch 70/188, Loss: 0.4899\n",
      "Epoch 154/10000, Batch 80/188, Loss: 0.5108\n",
      "Epoch 154/10000, Batch 90/188, Loss: 0.3375\n",
      "Epoch 154/10000, Batch 100/188, Loss: 0.4854\n",
      "Epoch 154/10000, Batch 110/188, Loss: 0.3224\n",
      "Epoch 154/10000, Batch 120/188, Loss: 0.6408\n",
      "Epoch 154/10000, Batch 130/188, Loss: 0.6174\n",
      "Epoch 154/10000, Batch 140/188, Loss: 0.5879\n",
      "Epoch 154/10000, Batch 150/188, Loss: 0.5556\n",
      "Epoch 154/10000, Batch 160/188, Loss: 0.4093\n",
      "Epoch 154/10000, Batch 170/188, Loss: 0.4115\n",
      "Epoch 154/10000, Batch 180/188, Loss: 0.4503\n",
      "Epoch 154 Finished - Train Loss: 0.4190, Train IoU: 0.5828, Train F1-Score: 0.7296\n",
      "Validation Loss: 0.5662, Validation IoU: 0.4351, Validation F1-Score: 0.5763\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 23/30\n",
      "Epoch 155/10000 starting...\n",
      "Epoch 155/10000, Batch 10/188, Loss: 0.2034\n",
      "Epoch 155/10000, Batch 20/188, Loss: 0.4729\n",
      "Epoch 155/10000, Batch 30/188, Loss: 0.4016\n",
      "Epoch 155/10000, Batch 40/188, Loss: 0.4364\n",
      "Epoch 155/10000, Batch 50/188, Loss: 0.2541\n",
      "Epoch 155/10000, Batch 60/188, Loss: 0.3434\n",
      "Epoch 155/10000, Batch 70/188, Loss: 0.3251\n",
      "Epoch 155/10000, Batch 80/188, Loss: 0.5991\n",
      "Epoch 155/10000, Batch 90/188, Loss: 0.2934\n",
      "Epoch 155/10000, Batch 100/188, Loss: 0.2871\n",
      "Epoch 155/10000, Batch 110/188, Loss: 0.6775\n",
      "Epoch 155/10000, Batch 120/188, Loss: 0.4517\n",
      "Epoch 155/10000, Batch 130/188, Loss: 0.4503\n",
      "Epoch 155/10000, Batch 140/188, Loss: 0.3687\n",
      "Epoch 155/10000, Batch 150/188, Loss: 0.5135\n",
      "Epoch 155/10000, Batch 160/188, Loss: 0.3330\n",
      "Epoch 155/10000, Batch 170/188, Loss: 0.4597\n",
      "Epoch 155/10000, Batch 180/188, Loss: 0.5703\n",
      "Epoch 155 Finished - Train Loss: 0.4124, Train IoU: 0.5893, Train F1-Score: 0.7358\n",
      "Validation Loss: 0.5635, Validation IoU: 0.4376, Validation F1-Score: 0.5798\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 24/30\n",
      "Epoch 156/10000 starting...\n",
      "Epoch 156/10000, Batch 10/188, Loss: 0.4433\n",
      "Epoch 156/10000, Batch 20/188, Loss: 0.4612\n",
      "Epoch 156/10000, Batch 30/188, Loss: 0.4514\n",
      "Epoch 156/10000, Batch 40/188, Loss: 0.2300\n",
      "Epoch 156/10000, Batch 50/188, Loss: 0.5565\n",
      "Epoch 156/10000, Batch 60/188, Loss: 0.3843\n",
      "Epoch 156/10000, Batch 70/188, Loss: 0.3592\n",
      "Epoch 156/10000, Batch 80/188, Loss: 0.3758\n",
      "Epoch 156/10000, Batch 90/188, Loss: 0.2492\n",
      "Epoch 156/10000, Batch 100/188, Loss: 0.3586\n",
      "Epoch 156/10000, Batch 110/188, Loss: 0.3539\n",
      "Epoch 156/10000, Batch 120/188, Loss: 0.4726\n",
      "Epoch 156/10000, Batch 130/188, Loss: 0.2953\n",
      "Epoch 156/10000, Batch 140/188, Loss: 0.6569\n",
      "Epoch 156/10000, Batch 150/188, Loss: 0.3852\n",
      "Epoch 156/10000, Batch 160/188, Loss: 0.3355\n",
      "Epoch 156/10000, Batch 170/188, Loss: 0.6012\n",
      "Epoch 156/10000, Batch 180/188, Loss: 0.3743\n",
      "Epoch 156 Finished - Train Loss: 0.4162, Train IoU: 0.5855, Train F1-Score: 0.7309\n",
      "Validation Loss: 0.5633, Validation IoU: 0.4380, Validation F1-Score: 0.5803\n",
      "Current Learning Rate: 0.00000010\n",
      "Best validation loss updated: 0.5633. Saving model and state...\n",
      "Epoch 157/10000 starting...\n",
      "Epoch 157/10000, Batch 10/188, Loss: 0.4133\n",
      "Epoch 157/10000, Batch 20/188, Loss: 0.4816\n",
      "Epoch 157/10000, Batch 30/188, Loss: 0.5327\n",
      "Epoch 157/10000, Batch 40/188, Loss: 0.5704\n",
      "Epoch 157/10000, Batch 50/188, Loss: 0.4198\n",
      "Epoch 157/10000, Batch 60/188, Loss: 0.4406\n",
      "Epoch 157/10000, Batch 70/188, Loss: 0.5875\n",
      "Epoch 157/10000, Batch 80/188, Loss: 0.4891\n",
      "Epoch 157/10000, Batch 90/188, Loss: 0.2836\n",
      "Epoch 157/10000, Batch 100/188, Loss: 0.4155\n",
      "Epoch 157/10000, Batch 110/188, Loss: 0.4205\n",
      "Epoch 157/10000, Batch 120/188, Loss: 0.6367\n",
      "Epoch 157/10000, Batch 130/188, Loss: 0.3205\n",
      "Epoch 157/10000, Batch 140/188, Loss: 0.2838\n",
      "Epoch 157/10000, Batch 150/188, Loss: 0.4259\n",
      "Epoch 157/10000, Batch 160/188, Loss: 0.4956\n",
      "Epoch 157/10000, Batch 170/188, Loss: 0.4512\n",
      "Epoch 157/10000, Batch 180/188, Loss: 0.5001\n",
      "Epoch 157 Finished - Train Loss: 0.4155, Train IoU: 0.5862, Train F1-Score: 0.7332\n",
      "Validation Loss: 0.5653, Validation IoU: 0.4359, Validation F1-Score: 0.5772\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 1/30\n",
      "Epoch 158/10000 starting...\n",
      "Epoch 158/10000, Batch 10/188, Loss: 0.4336\n",
      "Epoch 158/10000, Batch 20/188, Loss: 0.5019\n",
      "Epoch 158/10000, Batch 30/188, Loss: 0.5057\n",
      "Epoch 158/10000, Batch 40/188, Loss: 0.5106\n",
      "Epoch 158/10000, Batch 50/188, Loss: 0.2723\n",
      "Epoch 158/10000, Batch 60/188, Loss: 0.5025\n",
      "Epoch 158/10000, Batch 70/188, Loss: 0.3845\n",
      "Epoch 158/10000, Batch 80/188, Loss: 0.4860\n",
      "Epoch 158/10000, Batch 90/188, Loss: 0.3330\n",
      "Epoch 158/10000, Batch 100/188, Loss: 0.5875\n",
      "Epoch 158/10000, Batch 110/188, Loss: 0.3095\n",
      "Epoch 158/10000, Batch 120/188, Loss: 0.3461\n",
      "Epoch 158/10000, Batch 130/188, Loss: 0.3686\n",
      "Epoch 158/10000, Batch 140/188, Loss: 0.3957\n",
      "Epoch 158/10000, Batch 150/188, Loss: 0.4310\n",
      "Epoch 158/10000, Batch 160/188, Loss: 0.3167\n",
      "Epoch 158/10000, Batch 170/188, Loss: 0.4164\n",
      "Epoch 158/10000, Batch 180/188, Loss: 0.4977\n",
      "Epoch 158 Finished - Train Loss: 0.4107, Train IoU: 0.5910, Train F1-Score: 0.7362\n",
      "Validation Loss: 0.5649, Validation IoU: 0.4362, Validation F1-Score: 0.5783\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 2/30\n",
      "Epoch 159/10000 starting...\n",
      "Epoch 159/10000, Batch 10/188, Loss: 0.4116\n",
      "Epoch 159/10000, Batch 20/188, Loss: 0.6617\n",
      "Epoch 159/10000, Batch 30/188, Loss: 0.3748\n",
      "Epoch 159/10000, Batch 40/188, Loss: 0.4146\n",
      "Epoch 159/10000, Batch 50/188, Loss: 0.4934\n",
      "Epoch 159/10000, Batch 60/188, Loss: 0.4100\n",
      "Epoch 159/10000, Batch 70/188, Loss: 0.2783\n",
      "Epoch 159/10000, Batch 80/188, Loss: 0.3820\n",
      "Epoch 159/10000, Batch 90/188, Loss: 0.3145\n",
      "Epoch 159/10000, Batch 100/188, Loss: 0.5742\n",
      "Epoch 159/10000, Batch 110/188, Loss: 0.2228\n",
      "Epoch 159/10000, Batch 120/188, Loss: 0.3380\n",
      "Epoch 159/10000, Batch 130/188, Loss: 0.5984\n",
      "Epoch 159/10000, Batch 140/188, Loss: 0.5587\n",
      "Epoch 159/10000, Batch 150/188, Loss: 0.2711\n",
      "Epoch 159/10000, Batch 160/188, Loss: 0.5182\n",
      "Epoch 159/10000, Batch 170/188, Loss: 0.4642\n",
      "Epoch 159/10000, Batch 180/188, Loss: 0.4755\n",
      "Epoch 159 Finished - Train Loss: 0.4135, Train IoU: 0.5882, Train F1-Score: 0.7339\n",
      "Validation Loss: 0.5658, Validation IoU: 0.4354, Validation F1-Score: 0.5772\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 3/30\n",
      "Epoch 160/10000 starting...\n",
      "Epoch 160/10000, Batch 10/188, Loss: 0.4536\n",
      "Epoch 160/10000, Batch 20/188, Loss: 0.3863\n",
      "Epoch 160/10000, Batch 30/188, Loss: 0.4738\n",
      "Epoch 160/10000, Batch 40/188, Loss: 0.4287\n",
      "Epoch 160/10000, Batch 50/188, Loss: 0.3900\n",
      "Epoch 160/10000, Batch 60/188, Loss: 0.5518\n",
      "Epoch 160/10000, Batch 70/188, Loss: 0.5005\n",
      "Epoch 160/10000, Batch 80/188, Loss: 0.6348\n",
      "Epoch 160/10000, Batch 90/188, Loss: 0.6206\n",
      "Epoch 160/10000, Batch 100/188, Loss: 0.2768\n",
      "Epoch 160/10000, Batch 110/188, Loss: 0.4373\n",
      "Epoch 160/10000, Batch 120/188, Loss: 0.3238\n",
      "Epoch 160/10000, Batch 130/188, Loss: 0.3862\n",
      "Epoch 160/10000, Batch 140/188, Loss: 0.2752\n",
      "Epoch 160/10000, Batch 150/188, Loss: 0.5444\n",
      "Epoch 160/10000, Batch 160/188, Loss: 0.4206\n",
      "Epoch 160/10000, Batch 170/188, Loss: 0.2793\n",
      "Epoch 160/10000, Batch 180/188, Loss: 0.3023\n",
      "Epoch 160 Finished - Train Loss: 0.4134, Train IoU: 0.5883, Train F1-Score: 0.7351\n",
      "Validation Loss: 0.5675, Validation IoU: 0.4338, Validation F1-Score: 0.5745\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 4/30\n",
      "Epoch 161/10000 starting...\n",
      "Epoch 161/10000, Batch 10/188, Loss: 0.2874\n",
      "Epoch 161/10000, Batch 20/188, Loss: 0.4305\n",
      "Epoch 161/10000, Batch 30/188, Loss: 0.5346\n",
      "Epoch 161/10000, Batch 40/188, Loss: 0.2275\n",
      "Epoch 161/10000, Batch 50/188, Loss: 0.2501\n",
      "Epoch 161/10000, Batch 60/188, Loss: 0.4090\n",
      "Epoch 161/10000, Batch 70/188, Loss: 0.2344\n",
      "Epoch 161/10000, Batch 80/188, Loss: 0.5589\n",
      "Epoch 161/10000, Batch 90/188, Loss: 0.1655\n",
      "Epoch 161/10000, Batch 100/188, Loss: 0.4724\n",
      "Epoch 161/10000, Batch 110/188, Loss: 0.3857\n",
      "Epoch 161/10000, Batch 120/188, Loss: 0.4375\n",
      "Epoch 161/10000, Batch 130/188, Loss: 0.3857\n",
      "Epoch 161/10000, Batch 140/188, Loss: 0.3938\n",
      "Epoch 161/10000, Batch 150/188, Loss: 0.3149\n",
      "Epoch 161/10000, Batch 160/188, Loss: 0.5256\n",
      "Epoch 161/10000, Batch 170/188, Loss: 0.5397\n",
      "Epoch 161/10000, Batch 180/188, Loss: 0.5645\n",
      "Epoch 161 Finished - Train Loss: 0.4122, Train IoU: 0.5895, Train F1-Score: 0.7359\n",
      "Validation Loss: 0.5656, Validation IoU: 0.4355, Validation F1-Score: 0.5773\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 5/30\n",
      "Epoch 162/10000 starting...\n",
      "Epoch 162/10000, Batch 10/188, Loss: 0.4834\n",
      "Epoch 162/10000, Batch 20/188, Loss: 0.3066\n",
      "Epoch 162/10000, Batch 30/188, Loss: 0.5539\n",
      "Epoch 162/10000, Batch 40/188, Loss: 0.4189\n",
      "Epoch 162/10000, Batch 50/188, Loss: 0.3994\n",
      "Epoch 162/10000, Batch 60/188, Loss: 0.3444\n",
      "Epoch 162/10000, Batch 70/188, Loss: 0.4605\n",
      "Epoch 162/10000, Batch 80/188, Loss: 0.5101\n",
      "Epoch 162/10000, Batch 90/188, Loss: 0.4731\n",
      "Epoch 162/10000, Batch 100/188, Loss: 0.4879\n",
      "Epoch 162/10000, Batch 110/188, Loss: 0.3677\n",
      "Epoch 162/10000, Batch 120/188, Loss: 0.2896\n",
      "Epoch 162/10000, Batch 130/188, Loss: 0.4501\n",
      "Epoch 162/10000, Batch 140/188, Loss: 0.5612\n",
      "Epoch 162/10000, Batch 150/188, Loss: 0.2050\n",
      "Epoch 162/10000, Batch 160/188, Loss: 0.3914\n",
      "Epoch 162/10000, Batch 170/188, Loss: 0.1683\n",
      "Epoch 162/10000, Batch 180/188, Loss: 0.2660\n",
      "Epoch 162 Finished - Train Loss: 0.4159, Train IoU: 0.5858, Train F1-Score: 0.7323\n",
      "Validation Loss: 0.5640, Validation IoU: 0.4371, Validation F1-Score: 0.5789\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 6/30\n",
      "Epoch 163/10000 starting...\n",
      "Epoch 163/10000, Batch 10/188, Loss: 0.4721\n",
      "Epoch 163/10000, Batch 20/188, Loss: 0.3676\n",
      "Epoch 163/10000, Batch 30/188, Loss: 0.2281\n",
      "Epoch 163/10000, Batch 40/188, Loss: 0.3525\n",
      "Epoch 163/10000, Batch 50/188, Loss: 0.4210\n",
      "Epoch 163/10000, Batch 60/188, Loss: 0.4039\n",
      "Epoch 163/10000, Batch 70/188, Loss: 0.3378\n",
      "Epoch 163/10000, Batch 80/188, Loss: 0.3170\n",
      "Epoch 163/10000, Batch 90/188, Loss: 0.4325\n",
      "Epoch 163/10000, Batch 100/188, Loss: 0.7160\n",
      "Epoch 163/10000, Batch 110/188, Loss: 0.3123\n",
      "Epoch 163/10000, Batch 120/188, Loss: 0.3465\n",
      "Epoch 163/10000, Batch 130/188, Loss: 0.4332\n",
      "Epoch 163/10000, Batch 140/188, Loss: 0.3592\n",
      "Epoch 163/10000, Batch 150/188, Loss: 0.2869\n",
      "Epoch 163/10000, Batch 160/188, Loss: 0.5084\n",
      "Epoch 163/10000, Batch 170/188, Loss: 0.2879\n",
      "Epoch 163/10000, Batch 180/188, Loss: 0.4821\n",
      "Epoch 163 Finished - Train Loss: 0.4129, Train IoU: 0.5888, Train F1-Score: 0.7343\n",
      "Validation Loss: 0.5639, Validation IoU: 0.4372, Validation F1-Score: 0.5789\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 7/30\n",
      "Epoch 164/10000 starting...\n",
      "Epoch 164/10000, Batch 10/188, Loss: 0.3713\n",
      "Epoch 164/10000, Batch 20/188, Loss: 0.3157\n",
      "Epoch 164/10000, Batch 30/188, Loss: 0.4762\n",
      "Epoch 164/10000, Batch 40/188, Loss: 0.3677\n",
      "Epoch 164/10000, Batch 50/188, Loss: 0.3486\n",
      "Epoch 164/10000, Batch 60/188, Loss: 0.7963\n",
      "Epoch 164/10000, Batch 70/188, Loss: 0.6104\n",
      "Epoch 164/10000, Batch 80/188, Loss: 0.2675\n",
      "Epoch 164/10000, Batch 90/188, Loss: 0.5039\n",
      "Epoch 164/10000, Batch 100/188, Loss: 0.3437\n",
      "Epoch 164/10000, Batch 110/188, Loss: 0.5273\n",
      "Epoch 164/10000, Batch 120/188, Loss: 0.3331\n",
      "Epoch 164/10000, Batch 130/188, Loss: 0.5106\n",
      "Epoch 164/10000, Batch 140/188, Loss: 0.3460\n",
      "Epoch 164/10000, Batch 150/188, Loss: 0.4720\n",
      "Epoch 164/10000, Batch 160/188, Loss: 0.6094\n",
      "Epoch 164/10000, Batch 170/188, Loss: 0.4717\n",
      "Epoch 164/10000, Batch 180/188, Loss: 0.4032\n",
      "Epoch 164 Finished - Train Loss: 0.4132, Train IoU: 0.5885, Train F1-Score: 0.7338\n",
      "Validation Loss: 0.5650, Validation IoU: 0.4362, Validation F1-Score: 0.5779\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 8/30\n",
      "Epoch 165/10000 starting...\n",
      "Epoch 165/10000, Batch 10/188, Loss: 0.3694\n",
      "Epoch 165/10000, Batch 20/188, Loss: 0.3838\n",
      "Epoch 165/10000, Batch 30/188, Loss: 0.2390\n",
      "Epoch 165/10000, Batch 40/188, Loss: 0.5763\n",
      "Epoch 165/10000, Batch 50/188, Loss: 0.4259\n",
      "Epoch 165/10000, Batch 60/188, Loss: 0.3584\n",
      "Epoch 165/10000, Batch 70/188, Loss: 0.4601\n",
      "Epoch 165/10000, Batch 80/188, Loss: 0.5008\n",
      "Epoch 165/10000, Batch 90/188, Loss: 0.4365\n",
      "Epoch 165/10000, Batch 100/188, Loss: 0.3536\n",
      "Epoch 165/10000, Batch 110/188, Loss: 0.4565\n",
      "Epoch 165/10000, Batch 120/188, Loss: 0.4942\n",
      "Epoch 165/10000, Batch 130/188, Loss: 0.6233\n",
      "Epoch 165/10000, Batch 140/188, Loss: 0.3480\n",
      "Epoch 165/10000, Batch 150/188, Loss: 0.2754\n",
      "Epoch 165/10000, Batch 160/188, Loss: 0.3504\n",
      "Epoch 165/10000, Batch 170/188, Loss: 0.4017\n",
      "Epoch 165/10000, Batch 180/188, Loss: 0.4323\n",
      "Epoch 165 Finished - Train Loss: 0.4127, Train IoU: 0.5889, Train F1-Score: 0.7348\n",
      "Validation Loss: 0.5646, Validation IoU: 0.4365, Validation F1-Score: 0.5781\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 9/30\n",
      "Epoch 166/10000 starting...\n",
      "Epoch 166/10000, Batch 10/188, Loss: 0.2007\n",
      "Epoch 166/10000, Batch 20/188, Loss: 0.4858\n",
      "Epoch 166/10000, Batch 30/188, Loss: 0.4492\n",
      "Epoch 166/10000, Batch 40/188, Loss: 0.4787\n",
      "Epoch 166/10000, Batch 50/188, Loss: 0.3580\n",
      "Epoch 166/10000, Batch 60/188, Loss: 0.2875\n",
      "Epoch 166/10000, Batch 70/188, Loss: 0.5119\n",
      "Epoch 166/10000, Batch 80/188, Loss: 0.5095\n",
      "Epoch 166/10000, Batch 90/188, Loss: 0.2988\n",
      "Epoch 166/10000, Batch 100/188, Loss: 0.3754\n",
      "Epoch 166/10000, Batch 110/188, Loss: 0.3114\n",
      "Epoch 166/10000, Batch 120/188, Loss: 0.3757\n",
      "Epoch 166/10000, Batch 130/188, Loss: 0.3653\n",
      "Epoch 166/10000, Batch 140/188, Loss: 0.5186\n",
      "Epoch 166/10000, Batch 150/188, Loss: 0.7412\n",
      "Epoch 166/10000, Batch 160/188, Loss: 0.4655\n",
      "Epoch 166/10000, Batch 170/188, Loss: 0.2187\n",
      "Epoch 166/10000, Batch 180/188, Loss: 0.4822\n",
      "Epoch 166 Finished - Train Loss: 0.4123, Train IoU: 0.5893, Train F1-Score: 0.7355\n",
      "Validation Loss: 0.5653, Validation IoU: 0.4358, Validation F1-Score: 0.5774\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 10/30\n",
      "Epoch 167/10000 starting...\n",
      "Epoch 167/10000, Batch 10/188, Loss: 0.2256\n",
      "Epoch 167/10000, Batch 20/188, Loss: 0.2824\n",
      "Epoch 167/10000, Batch 30/188, Loss: 0.3773\n",
      "Epoch 167/10000, Batch 40/188, Loss: 0.4413\n",
      "Epoch 167/10000, Batch 50/188, Loss: 0.3359\n",
      "Epoch 167/10000, Batch 60/188, Loss: 0.5286\n",
      "Epoch 167/10000, Batch 70/188, Loss: 0.2630\n",
      "Epoch 167/10000, Batch 80/188, Loss: 0.4327\n",
      "Epoch 167/10000, Batch 90/188, Loss: 0.3961\n",
      "Epoch 167/10000, Batch 100/188, Loss: 0.4358\n",
      "Epoch 167/10000, Batch 110/188, Loss: 0.4625\n",
      "Epoch 167/10000, Batch 120/188, Loss: 0.4090\n",
      "Epoch 167/10000, Batch 130/188, Loss: 0.4525\n",
      "Epoch 167/10000, Batch 140/188, Loss: 0.3522\n",
      "Epoch 167/10000, Batch 150/188, Loss: 0.3896\n",
      "Epoch 167/10000, Batch 160/188, Loss: 0.5569\n",
      "Epoch 167/10000, Batch 170/188, Loss: 0.3871\n",
      "Epoch 167/10000, Batch 180/188, Loss: 0.3631\n",
      "Epoch 167 Finished - Train Loss: 0.4122, Train IoU: 0.5895, Train F1-Score: 0.7359\n",
      "Validation Loss: 0.5667, Validation IoU: 0.4344, Validation F1-Score: 0.5753\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 11/30\n",
      "Epoch 168/10000 starting...\n",
      "Epoch 168/10000, Batch 10/188, Loss: 0.3852\n",
      "Epoch 168/10000, Batch 20/188, Loss: 0.4277\n",
      "Epoch 168/10000, Batch 30/188, Loss: 0.4271\n",
      "Epoch 168/10000, Batch 40/188, Loss: 0.4517\n",
      "Epoch 168/10000, Batch 50/188, Loss: 0.3597\n",
      "Epoch 168/10000, Batch 60/188, Loss: 0.3852\n",
      "Epoch 168/10000, Batch 70/188, Loss: 0.2934\n",
      "Epoch 168/10000, Batch 80/188, Loss: 0.6121\n",
      "Epoch 168/10000, Batch 90/188, Loss: 0.4372\n",
      "Epoch 168/10000, Batch 100/188, Loss: 0.2647\n",
      "Epoch 168/10000, Batch 110/188, Loss: 0.3945\n",
      "Epoch 168/10000, Batch 120/188, Loss: 0.2055\n",
      "Epoch 168/10000, Batch 130/188, Loss: 0.3608\n",
      "Epoch 168/10000, Batch 140/188, Loss: 0.5689\n",
      "Epoch 168/10000, Batch 150/188, Loss: 0.2621\n",
      "Epoch 168/10000, Batch 160/188, Loss: 0.3202\n",
      "Epoch 168/10000, Batch 170/188, Loss: 0.5745\n",
      "Epoch 168/10000, Batch 180/188, Loss: 0.3640\n",
      "Epoch 168 Finished - Train Loss: 0.4125, Train IoU: 0.5892, Train F1-Score: 0.7345\n",
      "Validation Loss: 0.5667, Validation IoU: 0.4344, Validation F1-Score: 0.5755\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 12/30\n",
      "Epoch 169/10000 starting...\n",
      "Epoch 169/10000, Batch 10/188, Loss: 0.2617\n",
      "Epoch 169/10000, Batch 20/188, Loss: 0.2262\n",
      "Epoch 169/10000, Batch 30/188, Loss: 0.5157\n",
      "Epoch 169/10000, Batch 40/188, Loss: 0.3026\n",
      "Epoch 169/10000, Batch 50/188, Loss: 0.5190\n",
      "Epoch 169/10000, Batch 60/188, Loss: 0.5485\n",
      "Epoch 169/10000, Batch 70/188, Loss: 0.5576\n",
      "Epoch 169/10000, Batch 80/188, Loss: 0.2811\n",
      "Epoch 169/10000, Batch 90/188, Loss: 0.3540\n",
      "Epoch 169/10000, Batch 100/188, Loss: 0.6768\n",
      "Epoch 169/10000, Batch 110/188, Loss: 0.3916\n",
      "Epoch 169/10000, Batch 120/188, Loss: 0.5984\n",
      "Epoch 169/10000, Batch 130/188, Loss: 0.6052\n",
      "Epoch 169/10000, Batch 140/188, Loss: 0.2904\n",
      "Epoch 169/10000, Batch 150/188, Loss: 0.4071\n",
      "Epoch 169/10000, Batch 160/188, Loss: 0.4269\n",
      "Epoch 169/10000, Batch 170/188, Loss: 0.5383\n",
      "Epoch 169/10000, Batch 180/188, Loss: 0.5495\n",
      "Epoch 169 Finished - Train Loss: 0.4179, Train IoU: 0.5839, Train F1-Score: 0.7294\n",
      "Validation Loss: 0.5673, Validation IoU: 0.4338, Validation F1-Score: 0.5751\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 13/30\n",
      "Epoch 170/10000 starting...\n",
      "Epoch 170/10000, Batch 10/188, Loss: 0.5311\n",
      "Epoch 170/10000, Batch 20/188, Loss: 0.3310\n",
      "Epoch 170/10000, Batch 30/188, Loss: 0.4532\n",
      "Epoch 170/10000, Batch 40/188, Loss: 0.2788\n",
      "Epoch 170/10000, Batch 50/188, Loss: 0.3554\n",
      "Epoch 170/10000, Batch 60/188, Loss: 0.3946\n",
      "Epoch 170/10000, Batch 70/188, Loss: 0.2816\n",
      "Epoch 170/10000, Batch 80/188, Loss: 0.3581\n",
      "Epoch 170/10000, Batch 90/188, Loss: 0.3220\n",
      "Epoch 170/10000, Batch 100/188, Loss: 0.6408\n",
      "Epoch 170/10000, Batch 110/188, Loss: 0.4683\n",
      "Epoch 170/10000, Batch 120/188, Loss: 0.5445\n",
      "Epoch 170/10000, Batch 130/188, Loss: 0.6411\n",
      "Epoch 170/10000, Batch 140/188, Loss: 0.5440\n",
      "Epoch 170/10000, Batch 150/188, Loss: 0.4551\n",
      "Epoch 170/10000, Batch 160/188, Loss: 0.4782\n",
      "Epoch 170/10000, Batch 170/188, Loss: 0.3649\n",
      "Epoch 170/10000, Batch 180/188, Loss: 0.4541\n",
      "Epoch 170 Finished - Train Loss: 0.4152, Train IoU: 0.5865, Train F1-Score: 0.7329\n",
      "Validation Loss: 0.5649, Validation IoU: 0.4361, Validation F1-Score: 0.5772\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 14/30\n",
      "Epoch 171/10000 starting...\n",
      "Epoch 171/10000, Batch 10/188, Loss: 0.4614\n",
      "Epoch 171/10000, Batch 20/188, Loss: 0.3626\n",
      "Epoch 171/10000, Batch 30/188, Loss: 0.3438\n",
      "Epoch 171/10000, Batch 40/188, Loss: 0.3122\n",
      "Epoch 171/10000, Batch 50/188, Loss: 0.6148\n",
      "Epoch 171/10000, Batch 60/188, Loss: 0.4640\n",
      "Epoch 171/10000, Batch 70/188, Loss: 0.2905\n",
      "Epoch 171/10000, Batch 80/188, Loss: 0.5547\n",
      "Epoch 171/10000, Batch 90/188, Loss: 0.3190\n",
      "Epoch 171/10000, Batch 100/188, Loss: 0.7070\n",
      "Epoch 171/10000, Batch 110/188, Loss: 0.3693\n",
      "Epoch 171/10000, Batch 120/188, Loss: 0.5114\n",
      "Epoch 171/10000, Batch 130/188, Loss: 0.3510\n",
      "Epoch 171/10000, Batch 140/188, Loss: 0.5624\n",
      "Epoch 171/10000, Batch 150/188, Loss: 0.3764\n",
      "Epoch 171/10000, Batch 160/188, Loss: 0.4596\n",
      "Epoch 171/10000, Batch 170/188, Loss: 0.3412\n",
      "Epoch 171/10000, Batch 180/188, Loss: 0.3921\n",
      "Epoch 171 Finished - Train Loss: 0.4169, Train IoU: 0.5848, Train F1-Score: 0.7309\n",
      "Validation Loss: 0.5654, Validation IoU: 0.4359, Validation F1-Score: 0.5772\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 15/30\n",
      "Epoch 172/10000 starting...\n",
      "Epoch 172/10000, Batch 10/188, Loss: 0.4782\n",
      "Epoch 172/10000, Batch 20/188, Loss: 0.2458\n",
      "Epoch 172/10000, Batch 30/188, Loss: 0.3001\n",
      "Epoch 172/10000, Batch 40/188, Loss: 0.4818\n",
      "Epoch 172/10000, Batch 50/188, Loss: 0.5437\n",
      "Epoch 172/10000, Batch 60/188, Loss: 0.2895\n",
      "Epoch 172/10000, Batch 70/188, Loss: 0.3301\n",
      "Epoch 172/10000, Batch 80/188, Loss: 0.2923\n",
      "Epoch 172/10000, Batch 90/188, Loss: 0.4916\n",
      "Epoch 172/10000, Batch 100/188, Loss: 0.4085\n",
      "Epoch 172/10000, Batch 110/188, Loss: 0.2065\n",
      "Epoch 172/10000, Batch 120/188, Loss: 0.3127\n",
      "Epoch 172/10000, Batch 130/188, Loss: 0.4342\n",
      "Epoch 172/10000, Batch 140/188, Loss: 0.5964\n",
      "Epoch 172/10000, Batch 150/188, Loss: 0.4969\n",
      "Epoch 172/10000, Batch 160/188, Loss: 0.3075\n",
      "Epoch 172/10000, Batch 170/188, Loss: 0.4139\n",
      "Epoch 172/10000, Batch 180/188, Loss: 0.5563\n",
      "Epoch 172 Finished - Train Loss: 0.4138, Train IoU: 0.5879, Train F1-Score: 0.7341\n",
      "Validation Loss: 0.5675, Validation IoU: 0.4337, Validation F1-Score: 0.5748\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 16/30\n",
      "Epoch 173/10000 starting...\n",
      "Epoch 173/10000, Batch 10/188, Loss: 0.4250\n",
      "Epoch 173/10000, Batch 20/188, Loss: 0.6017\n",
      "Epoch 173/10000, Batch 30/188, Loss: 0.4421\n",
      "Epoch 173/10000, Batch 40/188, Loss: 0.5090\n",
      "Epoch 173/10000, Batch 50/188, Loss: 0.5374\n",
      "Epoch 173/10000, Batch 60/188, Loss: 0.4998\n",
      "Epoch 173/10000, Batch 70/188, Loss: 0.4088\n",
      "Epoch 173/10000, Batch 80/188, Loss: 0.3604\n",
      "Epoch 173/10000, Batch 90/188, Loss: 0.2942\n",
      "Epoch 173/10000, Batch 100/188, Loss: 0.2413\n",
      "Epoch 173/10000, Batch 110/188, Loss: 0.1330\n",
      "Epoch 173/10000, Batch 120/188, Loss: 0.4803\n",
      "Epoch 173/10000, Batch 130/188, Loss: 0.4180\n",
      "Epoch 173/10000, Batch 140/188, Loss: 0.4136\n",
      "Epoch 173/10000, Batch 150/188, Loss: 0.4159\n",
      "Epoch 173/10000, Batch 160/188, Loss: 0.4560\n",
      "Epoch 173/10000, Batch 170/188, Loss: 0.3801\n",
      "Epoch 173/10000, Batch 180/188, Loss: 0.4874\n",
      "Epoch 173 Finished - Train Loss: 0.4124, Train IoU: 0.5893, Train F1-Score: 0.7352\n",
      "Validation Loss: 0.5658, Validation IoU: 0.4354, Validation F1-Score: 0.5771\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 17/30\n",
      "Epoch 174/10000 starting...\n",
      "Epoch 174/10000, Batch 10/188, Loss: 0.4464\n",
      "Epoch 174/10000, Batch 20/188, Loss: 0.5469\n",
      "Epoch 174/10000, Batch 30/188, Loss: 0.4891\n",
      "Epoch 174/10000, Batch 40/188, Loss: 0.3653\n",
      "Epoch 174/10000, Batch 50/188, Loss: 0.3475\n",
      "Epoch 174/10000, Batch 60/188, Loss: 0.4008\n",
      "Epoch 174/10000, Batch 70/188, Loss: 0.4028\n",
      "Epoch 174/10000, Batch 80/188, Loss: 0.4225\n",
      "Epoch 174/10000, Batch 90/188, Loss: 0.1306\n",
      "Epoch 174/10000, Batch 100/188, Loss: 0.2697\n",
      "Epoch 174/10000, Batch 110/188, Loss: 0.2729\n",
      "Epoch 174/10000, Batch 120/188, Loss: 0.5982\n",
      "Epoch 174/10000, Batch 130/188, Loss: 0.4214\n",
      "Epoch 174/10000, Batch 140/188, Loss: 0.5192\n",
      "Epoch 174/10000, Batch 150/188, Loss: 0.6157\n",
      "Epoch 174/10000, Batch 160/188, Loss: 0.6300\n",
      "Epoch 174/10000, Batch 170/188, Loss: 0.5154\n",
      "Epoch 174/10000, Batch 180/188, Loss: 0.4103\n",
      "Epoch 174 Finished - Train Loss: 0.4144, Train IoU: 0.5873, Train F1-Score: 0.7334\n",
      "Validation Loss: 0.5653, Validation IoU: 0.4358, Validation F1-Score: 0.5773\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 18/30\n",
      "Epoch 175/10000 starting...\n",
      "Epoch 175/10000, Batch 10/188, Loss: 0.4307\n",
      "Epoch 175/10000, Batch 20/188, Loss: 0.3934\n",
      "Epoch 175/10000, Batch 30/188, Loss: 0.1687\n",
      "Epoch 175/10000, Batch 40/188, Loss: 0.4285\n",
      "Epoch 175/10000, Batch 50/188, Loss: 0.5872\n",
      "Epoch 175/10000, Batch 60/188, Loss: 0.3909\n",
      "Epoch 175/10000, Batch 70/188, Loss: 0.4037\n",
      "Epoch 175/10000, Batch 80/188, Loss: 0.5534\n",
      "Epoch 175/10000, Batch 90/188, Loss: 0.3047\n",
      "Epoch 175/10000, Batch 100/188, Loss: 0.4872\n",
      "Epoch 175/10000, Batch 110/188, Loss: 0.5291\n",
      "Epoch 175/10000, Batch 120/188, Loss: 0.3743\n",
      "Epoch 175/10000, Batch 130/188, Loss: 0.4730\n",
      "Epoch 175/10000, Batch 140/188, Loss: 0.3647\n",
      "Epoch 175/10000, Batch 150/188, Loss: 0.3728\n",
      "Epoch 175/10000, Batch 160/188, Loss: 0.2355\n",
      "Epoch 175/10000, Batch 170/188, Loss: 0.3867\n",
      "Epoch 175/10000, Batch 180/188, Loss: 0.5644\n",
      "Epoch 175 Finished - Train Loss: 0.4100, Train IoU: 0.5917, Train F1-Score: 0.7373\n",
      "Validation Loss: 0.5651, Validation IoU: 0.4360, Validation F1-Score: 0.5780\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 19/30\n",
      "Epoch 176/10000 starting...\n",
      "Epoch 176/10000, Batch 10/188, Loss: 0.4734\n",
      "Epoch 176/10000, Batch 20/188, Loss: 0.2756\n",
      "Epoch 176/10000, Batch 30/188, Loss: 0.5397\n",
      "Epoch 176/10000, Batch 40/188, Loss: 0.3392\n",
      "Epoch 176/10000, Batch 50/188, Loss: 0.2863\n",
      "Epoch 176/10000, Batch 60/188, Loss: 0.3943\n",
      "Epoch 176/10000, Batch 70/188, Loss: 0.4830\n",
      "Epoch 176/10000, Batch 80/188, Loss: 0.1742\n",
      "Epoch 176/10000, Batch 90/188, Loss: 0.4144\n",
      "Epoch 176/10000, Batch 100/188, Loss: 0.4833\n",
      "Epoch 176/10000, Batch 110/188, Loss: 0.6594\n",
      "Epoch 176/10000, Batch 120/188, Loss: 0.3007\n",
      "Epoch 176/10000, Batch 130/188, Loss: 0.5510\n",
      "Epoch 176/10000, Batch 140/188, Loss: 0.5984\n",
      "Epoch 176/10000, Batch 150/188, Loss: 0.4926\n",
      "Epoch 176/10000, Batch 160/188, Loss: 0.2228\n",
      "Epoch 176/10000, Batch 170/188, Loss: 0.3081\n",
      "Epoch 176/10000, Batch 180/188, Loss: 0.4051\n",
      "Epoch 176 Finished - Train Loss: 0.4136, Train IoU: 0.5881, Train F1-Score: 0.7339\n",
      "Validation Loss: 0.5674, Validation IoU: 0.4338, Validation F1-Score: 0.5746\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 20/30\n",
      "Epoch 177/10000 starting...\n",
      "Epoch 177/10000, Batch 10/188, Loss: 0.5205\n",
      "Epoch 177/10000, Batch 20/188, Loss: 0.3558\n",
      "Epoch 177/10000, Batch 30/188, Loss: 0.2446\n",
      "Epoch 177/10000, Batch 40/188, Loss: 0.5656\n",
      "Epoch 177/10000, Batch 50/188, Loss: 0.3318\n",
      "Epoch 177/10000, Batch 60/188, Loss: 0.3534\n",
      "Epoch 177/10000, Batch 70/188, Loss: 0.4104\n",
      "Epoch 177/10000, Batch 80/188, Loss: 0.4262\n",
      "Epoch 177/10000, Batch 90/188, Loss: 0.3567\n",
      "Epoch 177/10000, Batch 100/188, Loss: 0.4431\n",
      "Epoch 177/10000, Batch 110/188, Loss: 0.4301\n",
      "Epoch 177/10000, Batch 120/188, Loss: 0.4143\n",
      "Epoch 177/10000, Batch 130/188, Loss: 0.5084\n",
      "Epoch 177/10000, Batch 140/188, Loss: 0.3813\n",
      "Epoch 177/10000, Batch 150/188, Loss: 0.5363\n",
      "Epoch 177/10000, Batch 160/188, Loss: 0.2392\n",
      "Epoch 177/10000, Batch 170/188, Loss: 0.4932\n",
      "Epoch 177/10000, Batch 180/188, Loss: 0.5198\n",
      "Epoch 177 Finished - Train Loss: 0.4111, Train IoU: 0.5906, Train F1-Score: 0.7373\n",
      "Validation Loss: 0.5664, Validation IoU: 0.4347, Validation F1-Score: 0.5758\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 21/30\n",
      "Epoch 178/10000 starting...\n",
      "Epoch 178/10000, Batch 10/188, Loss: 0.4207\n",
      "Epoch 178/10000, Batch 20/188, Loss: 0.2403\n",
      "Epoch 178/10000, Batch 30/188, Loss: 0.5128\n",
      "Epoch 178/10000, Batch 40/188, Loss: 0.4322\n",
      "Epoch 178/10000, Batch 50/188, Loss: 0.4970\n",
      "Epoch 178/10000, Batch 60/188, Loss: 0.2980\n",
      "Epoch 178/10000, Batch 70/188, Loss: 0.4510\n",
      "Epoch 178/10000, Batch 80/188, Loss: 0.4477\n",
      "Epoch 178/10000, Batch 90/188, Loss: 0.5232\n",
      "Epoch 178/10000, Batch 100/188, Loss: 0.3106\n",
      "Epoch 178/10000, Batch 110/188, Loss: 0.3531\n",
      "Epoch 178/10000, Batch 120/188, Loss: 0.3291\n",
      "Epoch 178/10000, Batch 130/188, Loss: 0.4678\n",
      "Epoch 178/10000, Batch 140/188, Loss: 0.4817\n",
      "Epoch 178/10000, Batch 150/188, Loss: 0.4199\n",
      "Epoch 178/10000, Batch 160/188, Loss: 0.3451\n",
      "Epoch 178/10000, Batch 170/188, Loss: 0.4873\n",
      "Epoch 178/10000, Batch 180/188, Loss: 0.4820\n",
      "Epoch 178 Finished - Train Loss: 0.4155, Train IoU: 0.5862, Train F1-Score: 0.7322\n",
      "Validation Loss: 0.5647, Validation IoU: 0.4364, Validation F1-Score: 0.5784\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 22/30\n",
      "Epoch 179/10000 starting...\n",
      "Epoch 179/10000, Batch 10/188, Loss: 0.2916\n",
      "Epoch 179/10000, Batch 20/188, Loss: 0.3902\n",
      "Epoch 179/10000, Batch 30/188, Loss: 0.4650\n",
      "Epoch 179/10000, Batch 40/188, Loss: 0.5484\n",
      "Epoch 179/10000, Batch 50/188, Loss: 0.5580\n",
      "Epoch 179/10000, Batch 60/188, Loss: 0.4045\n",
      "Epoch 179/10000, Batch 70/188, Loss: 0.3427\n",
      "Epoch 179/10000, Batch 80/188, Loss: 0.2313\n",
      "Epoch 179/10000, Batch 90/188, Loss: 0.4586\n",
      "Epoch 179/10000, Batch 100/188, Loss: 0.5277\n",
      "Epoch 179/10000, Batch 110/188, Loss: 0.3779\n",
      "Epoch 179/10000, Batch 120/188, Loss: 0.4162\n",
      "Epoch 179/10000, Batch 130/188, Loss: 0.3885\n",
      "Epoch 179/10000, Batch 140/188, Loss: 0.4713\n",
      "Epoch 179/10000, Batch 150/188, Loss: 0.6486\n",
      "Epoch 179/10000, Batch 160/188, Loss: 0.1366\n",
      "Epoch 179/10000, Batch 170/188, Loss: 0.5158\n",
      "Epoch 179/10000, Batch 180/188, Loss: 0.4867\n",
      "Epoch 179 Finished - Train Loss: 0.4100, Train IoU: 0.5917, Train F1-Score: 0.7373\n",
      "Validation Loss: 0.5654, Validation IoU: 0.4357, Validation F1-Score: 0.5773\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 23/30\n",
      "Epoch 180/10000 starting...\n",
      "Epoch 180/10000, Batch 10/188, Loss: 0.4934\n",
      "Epoch 180/10000, Batch 20/188, Loss: 0.4101\n",
      "Epoch 180/10000, Batch 30/188, Loss: 0.2851\n",
      "Epoch 180/10000, Batch 40/188, Loss: 0.3475\n",
      "Epoch 180/10000, Batch 50/188, Loss: 0.5134\n",
      "Epoch 180/10000, Batch 60/188, Loss: 0.5513\n",
      "Epoch 180/10000, Batch 70/188, Loss: 0.4438\n",
      "Epoch 180/10000, Batch 80/188, Loss: 0.4400\n",
      "Epoch 180/10000, Batch 90/188, Loss: 0.2397\n",
      "Epoch 180/10000, Batch 100/188, Loss: 0.5013\n",
      "Epoch 180/10000, Batch 110/188, Loss: 0.3514\n",
      "Epoch 180/10000, Batch 120/188, Loss: 0.3359\n",
      "Epoch 180/10000, Batch 130/188, Loss: 0.5217\n",
      "Epoch 180/10000, Batch 140/188, Loss: 0.3890\n",
      "Epoch 180/10000, Batch 150/188, Loss: 0.4107\n",
      "Epoch 180/10000, Batch 160/188, Loss: 0.5271\n",
      "Epoch 180/10000, Batch 170/188, Loss: 0.4748\n",
      "Epoch 180/10000, Batch 180/188, Loss: 0.5587\n",
      "Epoch 180 Finished - Train Loss: 0.4185, Train IoU: 0.5833, Train F1-Score: 0.7300\n",
      "Validation Loss: 0.5640, Validation IoU: 0.4371, Validation F1-Score: 0.5784\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 24/30\n",
      "Epoch 181/10000 starting...\n",
      "Epoch 181/10000, Batch 10/188, Loss: 0.3749\n",
      "Epoch 181/10000, Batch 20/188, Loss: 0.2264\n",
      "Epoch 181/10000, Batch 30/188, Loss: 0.4534\n",
      "Epoch 181/10000, Batch 40/188, Loss: 0.3352\n",
      "Epoch 181/10000, Batch 50/188, Loss: 0.4920\n",
      "Epoch 181/10000, Batch 60/188, Loss: 0.3370\n",
      "Epoch 181/10000, Batch 70/188, Loss: 0.4394\n",
      "Epoch 181/10000, Batch 80/188, Loss: 0.4528\n",
      "Epoch 181/10000, Batch 90/188, Loss: 0.2869\n",
      "Epoch 181/10000, Batch 100/188, Loss: 0.4282\n",
      "Epoch 181/10000, Batch 110/188, Loss: 0.6053\n",
      "Epoch 181/10000, Batch 120/188, Loss: 0.3293\n",
      "Epoch 181/10000, Batch 130/188, Loss: 0.5309\n",
      "Epoch 181/10000, Batch 140/188, Loss: 0.3815\n",
      "Epoch 181/10000, Batch 150/188, Loss: 0.4048\n",
      "Epoch 181/10000, Batch 160/188, Loss: 0.4225\n",
      "Epoch 181/10000, Batch 170/188, Loss: 0.4243\n",
      "Epoch 181/10000, Batch 180/188, Loss: 0.3016\n",
      "Epoch 181 Finished - Train Loss: 0.4115, Train IoU: 0.5902, Train F1-Score: 0.7366\n",
      "Validation Loss: 0.5640, Validation IoU: 0.4372, Validation F1-Score: 0.5791\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 25/30\n",
      "Epoch 182/10000 starting...\n",
      "Epoch 182/10000, Batch 10/188, Loss: 0.5110\n",
      "Epoch 182/10000, Batch 20/188, Loss: 0.4917\n",
      "Epoch 182/10000, Batch 30/188, Loss: 0.4538\n",
      "Epoch 182/10000, Batch 40/188, Loss: 0.5892\n",
      "Epoch 182/10000, Batch 50/188, Loss: 0.5415\n",
      "Epoch 182/10000, Batch 60/188, Loss: 0.3717\n",
      "Epoch 182/10000, Batch 70/188, Loss: 0.2737\n",
      "Epoch 182/10000, Batch 80/188, Loss: 0.4779\n",
      "Epoch 182/10000, Batch 90/188, Loss: 0.3929\n",
      "Epoch 182/10000, Batch 100/188, Loss: 0.3962\n",
      "Epoch 182/10000, Batch 110/188, Loss: 0.2970\n",
      "Epoch 182/10000, Batch 120/188, Loss: 0.4056\n",
      "Epoch 182/10000, Batch 130/188, Loss: 0.5188\n",
      "Epoch 182/10000, Batch 140/188, Loss: 0.4474\n",
      "Epoch 182/10000, Batch 150/188, Loss: 0.4567\n",
      "Epoch 182/10000, Batch 160/188, Loss: 0.3294\n",
      "Epoch 182/10000, Batch 170/188, Loss: 0.3531\n",
      "Epoch 182/10000, Batch 180/188, Loss: 0.2431\n",
      "Epoch 182 Finished - Train Loss: 0.4179, Train IoU: 0.5838, Train F1-Score: 0.7308\n",
      "Validation Loss: 0.5634, Validation IoU: 0.4378, Validation F1-Score: 0.5795\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 26/30\n",
      "Epoch 183/10000 starting...\n",
      "Epoch 183/10000, Batch 10/188, Loss: 0.3603\n",
      "Epoch 183/10000, Batch 20/188, Loss: 0.3882\n",
      "Epoch 183/10000, Batch 30/188, Loss: 0.4355\n",
      "Epoch 183/10000, Batch 40/188, Loss: 0.6106\n",
      "Epoch 183/10000, Batch 50/188, Loss: 0.4041\n",
      "Epoch 183/10000, Batch 60/188, Loss: 0.4104\n",
      "Epoch 183/10000, Batch 70/188, Loss: 0.3808\n",
      "Epoch 183/10000, Batch 80/188, Loss: 0.3479\n",
      "Epoch 183/10000, Batch 90/188, Loss: 0.4137\n",
      "Epoch 183/10000, Batch 100/188, Loss: 0.3995\n",
      "Epoch 183/10000, Batch 110/188, Loss: 0.3393\n",
      "Epoch 183/10000, Batch 120/188, Loss: 0.5290\n",
      "Epoch 183/10000, Batch 130/188, Loss: 0.4117\n",
      "Epoch 183/10000, Batch 140/188, Loss: 0.3682\n",
      "Epoch 183/10000, Batch 150/188, Loss: 0.3342\n",
      "Epoch 183/10000, Batch 160/188, Loss: 0.4903\n",
      "Epoch 183/10000, Batch 170/188, Loss: 0.4454\n",
      "Epoch 183/10000, Batch 180/188, Loss: 0.4450\n",
      "Epoch 183 Finished - Train Loss: 0.4132, Train IoU: 0.5884, Train F1-Score: 0.7344\n",
      "Validation Loss: 0.5647, Validation IoU: 0.4365, Validation F1-Score: 0.5783\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 27/30\n",
      "Epoch 184/10000 starting...\n",
      "Epoch 184/10000, Batch 10/188, Loss: 0.5121\n",
      "Epoch 184/10000, Batch 20/188, Loss: 0.5704\n",
      "Epoch 184/10000, Batch 30/188, Loss: 0.4016\n",
      "Epoch 184/10000, Batch 40/188, Loss: 0.2145\n",
      "Epoch 184/10000, Batch 50/188, Loss: 0.3087\n",
      "Epoch 184/10000, Batch 60/188, Loss: 0.5259\n",
      "Epoch 184/10000, Batch 70/188, Loss: 0.3112\n",
      "Epoch 184/10000, Batch 80/188, Loss: 0.3833\n",
      "Epoch 184/10000, Batch 90/188, Loss: 0.2847\n",
      "Epoch 184/10000, Batch 100/188, Loss: 0.5014\n",
      "Epoch 184/10000, Batch 110/188, Loss: 0.4033\n",
      "Epoch 184/10000, Batch 120/188, Loss: 0.3815\n",
      "Epoch 184/10000, Batch 130/188, Loss: 0.4317\n",
      "Epoch 184/10000, Batch 140/188, Loss: 0.5342\n",
      "Epoch 184/10000, Batch 150/188, Loss: 0.4253\n",
      "Epoch 184/10000, Batch 160/188, Loss: 0.5312\n",
      "Epoch 184/10000, Batch 170/188, Loss: 0.2417\n",
      "Epoch 184/10000, Batch 180/188, Loss: 0.4189\n",
      "Epoch 184 Finished - Train Loss: 0.4127, Train IoU: 0.5890, Train F1-Score: 0.7335\n",
      "Validation Loss: 0.5651, Validation IoU: 0.4361, Validation F1-Score: 0.5777\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 28/30\n",
      "Epoch 185/10000 starting...\n",
      "Epoch 185/10000, Batch 10/188, Loss: 0.4001\n",
      "Epoch 185/10000, Batch 20/188, Loss: 0.3388\n",
      "Epoch 185/10000, Batch 30/188, Loss: 0.5006\n",
      "Epoch 185/10000, Batch 40/188, Loss: 0.2609\n",
      "Epoch 185/10000, Batch 50/188, Loss: 0.4278\n",
      "Epoch 185/10000, Batch 60/188, Loss: 0.4991\n",
      "Epoch 185/10000, Batch 70/188, Loss: 0.3972\n",
      "Epoch 185/10000, Batch 80/188, Loss: 0.4997\n",
      "Epoch 185/10000, Batch 90/188, Loss: 0.4041\n",
      "Epoch 185/10000, Batch 100/188, Loss: 0.3988\n",
      "Epoch 185/10000, Batch 110/188, Loss: 0.3742\n",
      "Epoch 185/10000, Batch 120/188, Loss: 0.4659\n",
      "Epoch 185/10000, Batch 130/188, Loss: 0.5033\n",
      "Epoch 185/10000, Batch 140/188, Loss: 0.5550\n",
      "Epoch 185/10000, Batch 150/188, Loss: 0.5069\n",
      "Epoch 185/10000, Batch 160/188, Loss: 0.3271\n",
      "Epoch 185/10000, Batch 170/188, Loss: 0.3763\n",
      "Epoch 185/10000, Batch 180/188, Loss: 0.3757\n",
      "Epoch 185 Finished - Train Loss: 0.4122, Train IoU: 0.5896, Train F1-Score: 0.7358\n",
      "Validation Loss: 0.5644, Validation IoU: 0.4367, Validation F1-Score: 0.5783\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 29/30\n",
      "Epoch 186/10000 starting...\n",
      "Epoch 186/10000, Batch 10/188, Loss: 0.6064\n",
      "Epoch 186/10000, Batch 20/188, Loss: 0.3277\n",
      "Epoch 186/10000, Batch 30/188, Loss: 0.5024\n",
      "Epoch 186/10000, Batch 40/188, Loss: 0.4242\n",
      "Epoch 186/10000, Batch 50/188, Loss: 0.2964\n",
      "Epoch 186/10000, Batch 60/188, Loss: 0.5624\n",
      "Epoch 186/10000, Batch 70/188, Loss: 0.5654\n",
      "Epoch 186/10000, Batch 80/188, Loss: 0.3207\n",
      "Epoch 186/10000, Batch 90/188, Loss: 0.4841\n",
      "Epoch 186/10000, Batch 100/188, Loss: 0.5310\n",
      "Epoch 186/10000, Batch 110/188, Loss: 0.4001\n",
      "Epoch 186/10000, Batch 120/188, Loss: 0.2839\n",
      "Epoch 186/10000, Batch 130/188, Loss: 0.2730\n",
      "Epoch 186/10000, Batch 140/188, Loss: 0.4164\n",
      "Epoch 186/10000, Batch 150/188, Loss: 0.3616\n",
      "Epoch 186/10000, Batch 160/188, Loss: 0.3535\n",
      "Epoch 186/10000, Batch 170/188, Loss: 0.4787\n",
      "Epoch 186/10000, Batch 180/188, Loss: 0.5011\n",
      "Epoch 186 Finished - Train Loss: 0.4135, Train IoU: 0.5882, Train F1-Score: 0.7344\n",
      "Validation Loss: 0.5642, Validation IoU: 0.4370, Validation F1-Score: 0.5787\n",
      "Current Learning Rate: 0.00000010\n",
      "Validation loss did not improve. Patience: 30/30\n",
      "Early stopping!\n",
      "\n",
      "Training complete.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# --- Chuẩn bị dữ liệu ---\n",
    "train_dataset = CrackDetectionDataset(train_img_paths, train_mask_paths, augment=True)\n",
    "val_dataset = CrackDetectionDataset(val_img_paths, val_mask_paths, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- Khởi tạo mô hình, optimizer, criterion ---\n",
    "model = SwinUNet(input_channels=3, num_classes=1).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-7)\n",
    "\n",
    "# Khởi tạo criterion CHỈ VỚI IoULoss\n",
    "criterion = IoULoss() # Đã bỏ CombinedLoss\n",
    "\n",
    "print(model)\n",
    "\n",
    "callbacks_config = {\n",
    "    'patience': 30,\n",
    "    'checkpoint_path': 'swin_unet_base_IoULoss.pth' # Đổi tên checkpoint để phân biệt\n",
    "}\n",
    "\n",
    "# --- Logic để tiếp tục huấn luyện từ checkpoint ---\n",
    "start_epoch = 0\n",
    "best_val_loss_so_far = float('inf')\n",
    "checkpoint_path = callbacks_config['checkpoint_path']\n",
    "history = None # Khởi tạo history\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Checkpoint found at {checkpoint_path}. Loading to resume training...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        print(\"Scheduler state loaded.\")\n",
    "    \n",
    "    start_epoch = checkpoint['epoch'] + 1 \n",
    "    best_val_loss_so_far = checkpoint['best_val_loss']\n",
    "    \n",
    "    if 'history' in checkpoint:\n",
    "        history = checkpoint['history']\n",
    "        print(\"Training history loaded from checkpoint.\")\n",
    "    else:\n",
    "        print(\"No training history found in checkpoint. Starting new history.\")\n",
    "\n",
    "    print(f\"Loaded checkpoint from Epoch {start_epoch-1}. Resuming training from Epoch {start_epoch}.\")\n",
    "    print(f\"Previous best validation loss: {best_val_loss_so_far:.4f}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting training from scratch (Epoch 0).\")\n",
    "\n",
    "print(\"\\nStarting Swin-Unet Base model training (IoU Loss only)...\")\n",
    "training_history = train_model(model, train_loader, val_loader, optimizer, criterion, scheduler,\n",
    "                               num_epochs=10000, callbacks_config=callbacks_config,\n",
    "                               start_epoch=start_epoch, best_val_loss_so_far=best_val_loss_so_far,\n",
    "                               history=history)\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "\n",
    "# VẼ BIỂU ĐỒ SAU KHI HUẤN LUYỆN\n",
    "if training_history is not None and len(training_history['train_loss']) > 0:\n",
    "    plot_training_history(training_history)\n",
    "else:\n",
    "    print(\"No training history available to plot.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
