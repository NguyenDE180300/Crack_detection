{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a8f22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Sử dụng thiết bị: cuda\n",
      "SwinUNet(\n",
      "  (swin): SwinModel(\n",
      "    (embeddings): SwinEmbeddings(\n",
      "      (patch_embeddings): SwinPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      )\n",
      "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): SwinEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (key): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (key): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.00909090880304575)\n",
      "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): SwinPatchMerging(\n",
      "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (key): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.0181818176060915)\n",
      "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (key): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.027272727340459824)\n",
      "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): SwinPatchMerging(\n",
      "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.036363635212183)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.045454543083906174)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.054545458406209946)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.06363636255264282)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.0727272778749466)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.08181818574666977)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): SwinPatchMerging(\n",
      "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.09090909361839294)\n",
      "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.10000000149011612)\n",
      "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
      "  )\n",
      "  (bottleneck): ConvBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder4): DecoderBlock(\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(1152, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder3): DecoderBlock(\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder2): DecoderBlock(\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(288, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder1): DecoderBlock(\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_conv): Conv2d(48, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Cài đặt 'torchinfo' (pip install torchinfo) để xem tóm tắt mô hình chi tiết.\n",
      "\n",
      "Bắt đầu huấn luyện mô hình Swin-Unet...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import SwinConfig, SwinModel\n",
    "\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 16\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Sử dụng thiết bị: {DEVICE}\")\n",
    "\n",
    "# Thay đổi đường dẫn thư mục tùy theo máy của bạn\n",
    "train_img_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\train\\image'\n",
    "train_mask_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\train\\label'\n",
    "val_img_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\val\\image'\n",
    "val_mask_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\val\\label'\n",
    "\n",
    "# --- Thu thập đường dẫn tệp ảnh và mask ---\n",
    "train_img_paths = sorted([os.path.join(train_img_dir, f) for f in os.listdir(train_img_dir)])\n",
    "train_mask_paths = sorted([os.path.join(train_mask_dir, f) for f in os.listdir(train_mask_dir)])\n",
    "val_img_paths = sorted([os.path.join(val_img_dir, f) for f in os.listdir(val_img_dir)])\n",
    "val_mask_paths = sorted([os.path.join(val_mask_dir, f) for f in os.listdir(val_mask_dir)])\n",
    "\n",
    "# --- Lớp Dataset tùy chỉnh ---\n",
    "class CrackDetectionDataset(Dataset):\n",
    "    def __init__(self, image_filenames, mask_filenames, augment=False):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.mask_filenames = mask_filenames\n",
    "        self.augment = augment\n",
    "        \n",
    "        if len(self.image_filenames) != len(self.mask_filenames):\n",
    "            raise ValueError(\"Số lượng tệp ảnh và tệp mask không khớp.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def pad_and_crop_image_and_mask(self, img, mask, target_size=IMG_SIZE, window_size=4):\n",
    "        h, w = img.shape[:2]\n",
    "        new_h = max(target_size, ((h + window_size - 1) // window_size) * window_size)\n",
    "        new_w = max(target_size, ((w + window_size - 1) // window_size) * window_size)\n",
    "\n",
    "        padded_img = np.zeros((new_h, new_w, 3), dtype=img.dtype)\n",
    "        padded_mask = np.zeros((new_h, new_w), dtype=mask.dtype)\n",
    "\n",
    "        padded_img[:h, :w, :] = img\n",
    "        padded_mask[:h, :w] = mask\n",
    "\n",
    "        cropped_img = padded_img[:target_size, :target_size, :]\n",
    "        cropped_mask = padded_mask[:target_size, :target_size]\n",
    "\n",
    "        return cropped_img, cropped_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.image_filenames[idx])\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Không thể đọc tệp ảnh: {self.image_filenames[idx]}\")\n",
    "\n",
    "        mask = cv2.imread(self.mask_filenames[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise ValueError(f\"Không thể đọc tệp mask: {self.mask_filenames[idx]}\")\n",
    "\n",
    "        img, mask = self.pad_and_crop_image_and_mask(img, mask, target_size=IMG_SIZE, window_size=4)\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        mask = mask.astype(np.float32) / 255.0\n",
    "        \n",
    "        if self.augment:\n",
    "            if random.random() < 0.5:\n",
    "                img = cv2.flip(img, 1)\n",
    "                mask = cv2.flip(mask, 1)\n",
    "            if random.random() < 0.5:\n",
    "                img = cv2.flip(img, 0)\n",
    "                mask = cv2.flip(mask, 0)\n",
    "        \n",
    "        if mask.ndim == 2:\n",
    "            mask = np.expand_dims(mask, axis=-1)\n",
    "\n",
    "        if mask.shape != (IMG_SIZE, IMG_SIZE, 1):\n",
    "            raise ValueError(f\"Hình dạng mask không mong muốn {mask.shape} tại chỉ số {idx} sau khi xử lý.\")\n",
    "\n",
    "        img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
    "        mask_tensor = torch.from_numpy(mask).permute(2, 0, 1)\n",
    "\n",
    "        return img_tensor, mask_tensor\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        \n",
    "        if skip_channels > 0:\n",
    "            self.conv_block = ConvBlock(out_channels + skip_channels, out_channels)\n",
    "        else:\n",
    "            self.conv_block = ConvBlock(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_features=None):\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        if skip_features is not None and skip_features.size(1) > 0:\n",
    "            if x.shape[2:] != skip_features.shape[2:]:\n",
    "                diffY = skip_features.size()[2] - x.size()[2]\n",
    "                diffX = skip_features.size()[3] - x.size()[3]\n",
    "                x = nn.functional.pad(x, [diffX // 2, diffX - diffX // 2,\n",
    "                                          diffY // 2, diffY - diffY // 2])\n",
    "            x = torch.cat([x, skip_features], dim=1)\n",
    "        \n",
    "        x = self.conv_block(x)\n",
    "        return x\n",
    "\n",
    "class SwinUNet(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.IMG_SIZE = IMG_SIZE\n",
    "\n",
    "        config = SwinConfig(image_size=self.IMG_SIZE, num_channels=input_channels, \n",
    "                            patch_size=4, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                            window_size=7, mlp_ratio=4., qkv_bias=True, hidden_dropout_prob=0.0, \n",
    "                            attention_probs_dropout_prob=0.0, drop_path_rate=0.1, \n",
    "                            hidden_act=\"gelu\", use_absolute_embeddings=False, \n",
    "                            patch_norm=True, initializer_range=0.02, layer_norm_eps=1e-05,\n",
    "                            out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"])\n",
    "        self.swin = SwinModel(config)\n",
    "\n",
    "        self.bottleneck = ConvBlock(config.embed_dim * 8, config.embed_dim * 8)\n",
    "\n",
    "        self.decoder4 = DecoderBlock(in_channels=config.embed_dim * 8, skip_channels=config.embed_dim * 4, out_channels=config.embed_dim * 4)\n",
    "        self.decoder3 = DecoderBlock(in_channels=config.embed_dim * 4, skip_channels=config.embed_dim * 2, out_channels=config.embed_dim * 2)\n",
    "        self.decoder2 = DecoderBlock(in_channels=config.embed_dim * 2, skip_channels=config.embed_dim * 1, out_channels=config.embed_dim * 1)\n",
    "        \n",
    "        self.decoder1 = DecoderBlock(in_channels=config.embed_dim * 1, skip_channels=0, out_channels=config.embed_dim // 2)\n",
    "\n",
    "        self.final_upsample = DecoderBlock(in_channels=config.embed_dim // 2, skip_channels=0, out_channels=config.embed_dim // 4)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(config.embed_dim // 4, num_classes, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.swin(pixel_values=x, output_hidden_states=True)\n",
    "        # print(\"\\nHidden States từ SwinModel:\") # Có thể bỏ comment nếu cần debug\n",
    "        # for i, hs in enumerate(outputs.hidden_states):\n",
    "        #     print(f\"  Hidden state {i} shape: {hs.shape}\")\n",
    "\n",
    "        encoder_features = []\n",
    "\n",
    "        # outputs.hidden_states[0] là Patch Embedding (kích thước 64x64, 96 kênh)\n",
    "        hs0 = outputs.hidden_states[0]\n",
    "        batch_size, num_patches, embed_dim = hs0.shape\n",
    "        side = int(np.sqrt(num_patches))\n",
    "        encoder_features.append(hs0.permute(0, 2, 1).reshape(batch_size, embed_dim, side, side))\n",
    "\n",
    "        # outputs.hidden_states[1] là Stage 1 (kích thước 32x32, 192 kênh)\n",
    "        hs1 = outputs.hidden_states[1]\n",
    "        batch_size, num_patches, embed_dim = hs1.shape\n",
    "        side = int(np.sqrt(num_patches))\n",
    "        encoder_features.append(hs1.permute(0, 2, 1).reshape(batch_size, embed_dim, side, side))\n",
    "\n",
    "        # outputs.hidden_states[2] là Stage 2 (kích thước 16x16, 384 kênh)\n",
    "        hs2 = outputs.hidden_states[2]\n",
    "        batch_size, num_patches, embed_dim = hs2.shape\n",
    "        side = int(np.sqrt(num_patches))\n",
    "        encoder_features.append(hs2.permute(0, 2, 1).reshape(batch_size, embed_dim, side, side))\n",
    "\n",
    "        # outputs.hidden_states[4] là đầu ra của Stage 4, dùng cho bottleneck\n",
    "        x_bottleneck = outputs.hidden_states[4]\n",
    "        batch_size, num_patches, embed_dim = x_bottleneck.shape\n",
    "        side = int(np.sqrt(num_patches))\n",
    "        x_bottleneck = x_bottleneck.permute(0, 2, 1).reshape(batch_size, embed_dim, side, side)\n",
    "        \n",
    "        x = self.bottleneck(x_bottleneck)\n",
    "\n",
    "        x = self.decoder4(x, encoder_features[2])\n",
    "        x = self.decoder3(x, encoder_features[1])\n",
    "        x = self.decoder2(x, encoder_features[0])\n",
    "        x = self.decoder1(x)\n",
    "        x = self.final_upsample(x)\n",
    "\n",
    "        outputs = self.final_conv(x)\n",
    "        outputs = self.sigmoid(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# --- Các hàm tính toán chỉ số mới (IoU và F1-Score) ---\n",
    "def calculate_metrics(predicted_masks, true_masks, smooth=1e-6):\n",
    "    # predicted_masks và true_masks phải là tensor nhị phân (0 hoặc 1)\n",
    "    \n",
    "    intersection = (predicted_masks * true_masks).sum()\n",
    "    union = (predicted_masks + true_masks).sum() - intersection\n",
    "    \n",
    "    # IoU\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    # F1-Score (Dice Coefficient)\n",
    "    # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # Precision = TP / (TP + FP)\n",
    "    # Recall = TP / (TP + FN)\n",
    "    \n",
    "    # TP (True Positives): pixel dự đoán là 1, thực tế là 1\n",
    "    # FP (False Positives): pixel dự đoán là 1, thực tế là 0\n",
    "    # FN (False Negatives): pixel dự đoán là 0, thực tế là 1\n",
    "    \n",
    "    # TP = intersection\n",
    "    # FP = (predicted_masks == 1).sum() - TP\n",
    "    # FN = (true_masks == 1).sum() - TP\n",
    "\n",
    "    dice = (2. * intersection + smooth) / ((predicted_masks.sum() + true_masks.sum()) + smooth)\n",
    "    f1_score = dice # F1-score và Dice Coefficient là như nhau cho phân loại nhị phân\n",
    "\n",
    "    return iou.item(), f1_score.item()\n",
    "\n",
    "\n",
    "# Thêm tham số start_epoch và best_val_loss_so_far để tiếp tục từ checkpoint\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, callbacks_config, start_epoch=0, best_val_loss_so_far=float('inf')):\n",
    "    best_val_loss = best_val_loss_so_far # Khởi tạo best_val_loss từ giá trị đã tải\n",
    "    patience_counter = 0\n",
    "    model_checkpoint_path = callbacks_config.get('checkpoint_path', 'swin_unet_best_pytorch.pth')\n",
    "\n",
    "    # Vòng lặp epoch bắt đầu từ start_epoch\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_iou = 0.0\n",
    "        running_f1 = 0.0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Bắt đầu...\")\n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Tính toán IoU và F1-Score\n",
    "            predicted_masks = (outputs > 0.5).float()\n",
    "            \n",
    "            batch_iou, batch_f1 = calculate_metrics(predicted_masks, masks)\n",
    "            running_iou += batch_iou * images.size(0)\n",
    "            running_f1 += batch_f1 * images.size(0)\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_iou = running_iou / len(train_loader.dataset)\n",
    "        epoch_f1 = running_f1 / len(train_loader.dataset)\n",
    "        # Thay đổi dòng in để hiển thị IoU và F1\n",
    "        print(f\"Epoch {epoch+1} Kết thúc - Mất mát Huấn luyện: {epoch_loss:.4f}, IoU Huấn luyện: {epoch_iou:.4f}, F1-Score Huấn luyện: {epoch_f1:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_iou = 0.0\n",
    "        val_f1 = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(DEVICE)\n",
    "                masks = masks.to(DEVICE)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                # Tính toán IoU và F1-Score cho tập xác thực\n",
    "                predicted_masks = (outputs > 0.5).float()\n",
    "                \n",
    "                batch_iou, batch_f1 = calculate_metrics(predicted_masks, masks)\n",
    "                val_iou += batch_iou * images.size(0)\n",
    "                val_f1 += batch_f1 * images.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_iou /= len(val_loader.dataset)\n",
    "        val_f1 /= len(val_loader.dataset)\n",
    "        # Thay đổi dòng in để hiển thị IoU và F1\n",
    "        print(f\"Mất mát Xác thực: {val_loss:.4f}, IoU Xác thực: {val_iou:.4f}, F1-Score Xác thực: {val_f1:.4f}\")\n",
    "\n",
    "        # --- Callbacks: Early Stopping và Model Checkpoint ---\n",
    "        # Vẫn sử dụng val_loss để quyết định lưu mô hình tốt nhất\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            print(f\"Mất mát xác thực tốt nhất được cập nhật: {best_val_loss:.4f}. Lưu mô hình và trạng thái...\")\n",
    "            # Lưu checkpoint đầy đủ\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, model_checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Mất mát xác thực không cải thiện. Sự kiên nhẫn: {patience_counter}/{callbacks_config['patience']}\")\n",
    "            if patience_counter >= callbacks_config['patience']:\n",
    "                print(\"Dừng sớm!\")\n",
    "                break\n",
    "\n",
    "# --- Khởi tạo và tải dữ liệu ---\n",
    "train_dataset = CrackDetectionDataset(train_img_paths, train_mask_paths, augment=True)\n",
    "val_dataset = CrackDetectionDataset(val_img_paths, val_mask_paths, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- Khởi tạo mô hình, optimizer, criterion ---\n",
    "model = SwinUNet(input_channels=3, num_classes=1).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "callbacks_config = {\n",
    "    'patience': 10,\n",
    "    'checkpoint_path': 'swin_unet_best_pytorch.pth'\n",
    "}\n",
    "\n",
    "# --- Logic để tiếp tục huấn luyện từ checkpoint ---\n",
    "start_epoch = 0\n",
    "best_val_loss_so_far = float('inf')\n",
    "checkpoint_path = callbacks_config['checkpoint_path']\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Phát hiện checkpoint tại {checkpoint_path}. Đang tải để tiếp tục huấn luyện...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1 # Bắt đầu từ epoch tiếp theo sau khi checkpoint được lưu\n",
    "    best_val_loss_so_far = checkpoint['best_val_loss']\n",
    "    \n",
    "    print(f\"Đã tải checkpoint từ Epoch {start_epoch-1}. Tiếp tục huấn luyện từ Epoch {start_epoch}.\")\n",
    "    print(f\"Mất mát xác thực tốt nhất trước đó: {best_val_loss_so_far:.4f}\")\n",
    "else:\n",
    "    print(\"Không tìm thấy checkpoint. Bắt đầu huấn luyện từ đầu (Epoch 0).\")\n",
    "\n",
    "print(\"\\nBắt đầu huấn luyện mô hình Swin-Unet...\")\n",
    "# Gọi hàm huấn luyện với các tham số mới\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, \n",
    "            num_epochs=100, callbacks_config=callbacks_config,\n",
    "            start_epoch=start_epoch, best_val_loss_so_far=best_val_loss_so_far)\n",
    "\n",
    "print(\"\\nQuá trình huấn luyện đã hoàn tất.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87d178c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Sử dụng thiết bị: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_17800\\3361398528.py:54: UserWarning: Argument(s) 'value, mask_value' are not valid for transform PadIfNeeded\n",
      "  A.PadIfNeeded(min_height=int(IMG_SIZE * 1.25), min_width=int(IMG_SIZE * 1.25), border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import SwinConfig, SwinModel\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# --- Cấu hình ---\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 8\n",
    "SEED = 42\n",
    "\n",
    "# --- Đặt Seed để tái lập kết quả ---\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Sử dụng thiết bị: {DEVICE}\")\n",
    "\n",
    "# Thay đổi đường dẫn thư mục tùy theo máy của bạn\n",
    "train_img_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\train\\image'\n",
    "train_mask_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\train\\label'\n",
    "val_img_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\val\\image'\n",
    "val_mask_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\val\\label'\n",
    "\n",
    "# --- Thu thập đường dẫn tệp ảnh và mask ---\n",
    "train_img_paths = sorted([os.path.join(train_img_dir, f) for f in os.listdir(train_img_dir)])\n",
    "train_mask_paths = sorted([os.path.join(train_mask_dir, f) for f in os.listdir(train_mask_dir)])\n",
    "val_img_paths = sorted([os.path.join(val_img_dir, f) for f in os.listdir(val_img_dir)])\n",
    "val_mask_paths = sorted([os.path.join(val_mask_dir, f) for f in os.listdir(val_mask_dir)])\n",
    "\n",
    "\n",
    "## Biến Đổi Albumentations\n",
    "\n",
    "# Pipeline tăng cường dữ liệu cho huấn luyện\n",
    "# Chúng ta sẽ thay đổi kích thước ảnh đến một kích thước lớn hơn một chút, sau đó cắt ngẫu nhiên,\n",
    "# và cuối cùng thay đổi kích thước về IMG_SIZE.\n",
    "# Điều này mô phỏng việc \"cắt các phần ngẫu nhiên của ảnh\" và sau đó thay đổi kích thước chúng về kích thước mục tiêu.\n",
    "train_transform = A.Compose([\n",
    "    # Thay đổi kích thước ảnh đến một kích thước lớn hơn một chút trước khi cắt\n",
    "    # để đảm bảo có đủ không gian cho việc cắt và sau đó thay đổi kích thước về IMG_SIZE.\n",
    "    # Chúng ta chọn 1.25 * IMG_SIZE làm kích thước trung gian lớn hơn.\n",
    "    A.LongestMaxSize(max_size=int(IMG_SIZE * 1.25), interpolation=cv2.INTER_AREA),\n",
    "    A.PadIfNeeded(min_height=int(IMG_SIZE * 1.25), min_width=int(IMG_SIZE * 1.25), border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),\n",
    "    A.RandomCrop(height=IMG_SIZE, width=IMG_SIZE, p=1.0), # Luôn thực hiện cắt ngẫu nhiên về IMG_SIZE\n",
    "    A.HorizontalFlip(p=0.5), # Lật ngang ngẫu nhiên\n",
    "    A.VerticalFlip(p=0.5),   # Lật dọc ngẫu nhiên\n",
    "    # Chuẩn hóa ảnh. Mean và Std của ImageNet rất phổ biến khi dùng với các mô hình pre-trained.\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(), # Chuyển ảnh và mask sang PyTorch tensors\n",
    "])\n",
    "\n",
    "# Pipeline biến đổi cho xác thực (chỉ resize và chuẩn hóa)\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=IMG_SIZE, width=IMG_SIZE, interpolation=cv2.INTER_AREA),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63b6374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrackDetectionDataset(Dataset):\n",
    "    def __init__(self, image_filenames, mask_filenames, transform=None):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.mask_filenames = mask_filenames\n",
    "        self.transform = transform\n",
    "\n",
    "        if len(self.image_filenames) != len(self.mask_filenames):\n",
    "            raise ValueError(\"Số lượng tệp ảnh và tệp mask không khớp.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.image_filenames[idx])\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Không thể đọc tệp ảnh: {self.image_filenames[idx]}\")\n",
    "\n",
    "        # Đọc mask dưới dạng ảnh xám\n",
    "        mask = cv2.imread(self.mask_filenames[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise ValueError(f\"Không thể đọc tệp mask: {self.mask_filenames[idx]}\")\n",
    "\n",
    "        # Chuyển đổi ảnh sang RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Albumentations cần mask có giá trị 0 hoặc 1 (hoặc các lớp khác)\n",
    "        # và không cần ở dạng float hoặc có thêm chiều kênh ngay tại đây\n",
    "        mask = (mask > 127).astype(np.float32) # Chuyển mask về 0.0 hoặc 1.0\n",
    "\n",
    "        if self.transform:\n",
    "            # Albumentations mong đợi ảnh (H, W, C) và mask (H, W)\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img_tensor = augmented['image']\n",
    "            mask_tensor = augmented['mask']\n",
    "        else:\n",
    "            # Đây là fallback nếu không có transform, nhưng nên dùng val_transform\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "            mask = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "            img_tensor = torch.from_numpy(img.astype(np.float32) / 255.0).permute(2, 0, 1)\n",
    "            mask_tensor = torch.from_numpy(mask.astype(np.float32)).unsqueeze(0) # Thêm chiều kênh cho mask\n",
    "\n",
    "        # Đảm bảo mask có chiều kênh (1, H, W) và loại float\n",
    "        if mask_tensor.ndim == 2:\n",
    "            mask_tensor = mask_tensor.unsqueeze(0) # Add channel dim for (H,W) -> (1,H,W)\n",
    "        \n",
    "        return img_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a966ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # Sử dụng Interpolate để upsample thay vì ConvTranspose2d,\n",
    "        # giúp tránh các vấn đề 'checkerboard artifacts' và linh hoạt hơn với các kích thước không chia hết.\n",
    "        # ConvTranspose2d vẫn ổn, nhưng Interpolate thường cho kết quả mịn hơn trong segmentation.\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1) # Conv1x1 để điều chỉnh kênh sau upsample\n",
    "        )\n",
    "        \n",
    "        if skip_channels > 0:\n",
    "            self.conv_block = ConvBlock(out_channels + skip_channels, out_channels)\n",
    "        else:\n",
    "            self.conv_block = ConvBlock(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_features=None):\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        if skip_features is not None:\n",
    "            # Đảm bảo kích thước không gian khớp trước khi concatenate\n",
    "            if x.shape[2:] != skip_features.shape[2:]:\n",
    "                x = nn.functional.interpolate(x, size=skip_features.shape[2:], mode='bilinear', align_corners=True)\n",
    "            x = torch.cat([x, skip_features], dim=1)\n",
    "        \n",
    "        x = self.conv_block(x)\n",
    "        return x\n",
    "\n",
    "class SwinUNet(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.IMG_SIZE = IMG_SIZE\n",
    "\n",
    "        # Đảm bảo out_features bao gồm \"patch_embeddings\" để có thể dùng làm skip connection đầu tiên\n",
    "        config = SwinConfig(image_size=self.IMG_SIZE, num_channels=input_channels, \n",
    "                            patch_size=4, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                            window_size=7, mlp_ratio=4., qkv_bias=True, hidden_dropout_prob=0.0, \n",
    "                            attention_probs_dropout_prob=0.0, drop_path_rate=0.1, \n",
    "                            hidden_act=\"gelu\", use_absolute_embeddings=False, \n",
    "                            patch_norm=True, initializer_range=0.02, layer_norm_eps=1e-05,\n",
    "                            out_features=[\"patch_embeddings\", \"stage1\", \"stage2\", \"stage3\", \"stage4\"])\n",
    "        self.swin = SwinModel(config)\n",
    "\n",
    "        # Kích thước kênh đầu ra của các stage Swin (ví dụ cho embed_dim=96):\n",
    "        # patch_embeddings: 96 (từ 256/4 = 64x64)\n",
    "        # stage1: 192 (96*2) (từ 256/8 = 32x32)\n",
    "        # stage2: 384 (96*4) (từ 256/16 = 16x16)\n",
    "        # stage3: 768 (96*8) (từ 256/32 = 8x8)\n",
    "        # stage4: 1536 (96*16) (từ 256/64 = 4x4)\n",
    "\n",
    "        # Bottleneck: Lấy từ stage4 (kích thước 4x4)\n",
    "        self.bottleneck = ConvBlock(config.embed_dim * 16, config.embed_dim * 8) # Từ 1536 kênh về 768 kênh\n",
    "\n",
    "        # Các Decoder Block\n",
    "        # Nối output của bottleneck (768 kênh) với stage3 (768 kênh)\n",
    "        self.decoder4 = DecoderBlock(in_channels=config.embed_dim * 8, skip_channels=config.embed_dim * 8, out_channels=config.embed_dim * 4) # Output 384 kênh\n",
    "        # Nối output của decoder4 (384 kênh) với stage2 (384 kênh)\n",
    "        self.decoder3 = DecoderBlock(in_channels=config.embed_dim * 4, skip_channels=config.embed_dim * 4, out_channels=config.embed_dim * 2) # Output 192 kênh\n",
    "        # Nối output của decoder3 (192 kênh) với stage1 (192 kênh)\n",
    "        self.decoder2 = DecoderBlock(in_channels=config.embed_dim * 2, skip_channels=config.embed_dim * 2, out_channels=config.embed_dim * 1) # Output 96 kênh\n",
    "        # Nối output của decoder2 (96 kênh) với patch_embeddings (96 kênh)\n",
    "        self.decoder1 = DecoderBlock(in_channels=config.embed_dim * 1, skip_channels=config.embed_dim, out_channels=config.embed_dim // 2) # Output 48 kênh\n",
    "\n",
    "        # Final Upsample để đạt kích thước ban đầu (nếu cần)\n",
    "        # Từ 48 kênh lên 24 kênh, kích thước 128x128 -> 256x256\n",
    "        self.final_upsample = DecoderBlock(in_channels=config.embed_dim // 2, skip_channels=0, out_channels=config.embed_dim // 4) \n",
    "\n",
    "        self.final_conv = nn.Conv2d(config.embed_dim // 4, num_classes, kernel_size=1)\n",
    "        # self.sigmoid = nn.Sigmoid() # LOẠI BỎ SIGMOID Ở ĐÂY, BCEWithLogitsLoss sẽ xử lý!\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.swin(pixel_values=x, output_hidden_states=True)\n",
    "        \n",
    "        # Lấy các hidden_states tương ứng và chuyển đổi sang định dạng (B, C, H, W)\n",
    "        # SwinModel trả về hidden_states theo thứ tự: patch_embeddings, stage1, stage2, stage3, stage4\n",
    "        \n",
    "        # Hàm trợ giúp để chuyển đổi từ (batch_size, num_patches, embed_dim) sang (batch_size, embed_dim, H, W)\n",
    "        def reshape_to_cnn_format(hs_tensor, embed_dim_val):\n",
    "            batch_size, num_patches, _ = hs_tensor.shape\n",
    "            side = int(np.sqrt(num_patches))\n",
    "            return hs_tensor.permute(0, 2, 1).reshape(batch_size, embed_dim_val, side, side)\n",
    "\n",
    "        # Lấy các feature maps từ encoder\n",
    "        f_patch_embed = reshape_to_cnn_format(outputs.hidden_states[0], self.swin.config.embed_dim) # 96 kênh, 64x64\n",
    "        f_stage1 = reshape_to_cnn_format(outputs.hidden_states[1], self.swin.config.embed_dim * 2)  # 192 kênh, 32x32\n",
    "        f_stage2 = reshape_to_cnn_format(outputs.hidden_states[2], self.swin.config.embed_dim * 4)  # 384 kênh, 16x16\n",
    "        f_stage3 = reshape_to_cnn_format(outputs.hidden_states[3], self.swin.config.embed_dim * 8)  # 768 kênh, 8x8\n",
    "        f_stage4 = reshape_to_cnn_format(outputs.hidden_states[4], self.swin.config.embed_dim * 16) # 1536 kênh, 4x4 (đây là đầu vào cho bottleneck)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(f_stage4) # (B, 1536, 4, 4) -> (B, 768, 4, 4)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder4(x, f_stage3) # (B, 768, 4, 4) + (B, 768, 8, 8) -> (B, 384, 8, 8)\n",
    "        x = self.decoder3(x, f_stage2) # (B, 384, 8, 8) + (B, 384, 16, 16) -> (B, 192, 16, 16)\n",
    "        x = self.decoder2(x, f_stage1) # (B, 192, 16, 16) + (B, 192, 32, 32) -> (B, 96, 32, 32)\n",
    "        x = self.decoder1(x, f_patch_embed) # (B, 96, 32, 32) + (B, 96, 64, 64) -> (B, 48, 64, 64)\n",
    "        \n",
    "        x = self.final_upsample(x) # (B, 48, 64, 64) -> (B, 24, 128, 128)\n",
    "\n",
    "        # Chuyển đổi về kích thước IMG_SIZE (256x256) nếu cần thiết ở bước cuối cùng\n",
    "        if x.shape[2:] != (self.IMG_SIZE, self.IMG_SIZE):\n",
    "            x = nn.functional.interpolate(x, size=(self.IMG_SIZE, self.IMG_SIZE), mode='bilinear', align_corners=True)\n",
    "\n",
    "        outputs = self.final_conv(x) # (B, 24, 256, 256) -> (B, 1, 256, 256)\n",
    "        # outputs = self.sigmoid(outputs) # LOẠI BỎ Ở ĐÂY!\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1caf5fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "out_features must be a subset of stage_names: ['stem', 'stage1', 'stage2', 'stage3', 'stage4'] got ['patch_embeddings', 'stage1', 'stage2', 'stage3', 'stage4']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 104\u001b[0m\n\u001b[0;32m    101\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# --- Khởi tạo mô hình, optimizer, criterion ---\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSwinUNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m    105\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m    106\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss() \u001b[38;5;66;03m# Đây là hàm loss đúng khi đầu ra là logits\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 50\u001b[0m, in \u001b[0;36mSwinUNet.__init__\u001b[1;34m(self, input_channels, num_classes)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mIMG_SIZE \u001b[38;5;241m=\u001b[39m IMG_SIZE\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Đảm bảo out_features bao gồm \"patch_embeddings\" để có thể dùng làm skip connection đầu tiên\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mSwinConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dropout_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mattention_probs_dropout_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_path_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mhidden_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_absolute_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mpatch_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitializer_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_norm_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mout_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatch_embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstage1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstage2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstage3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstage4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswin \u001b[38;5;241m=\u001b[39m SwinModel(config)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Kích thước kênh đầu ra của các stage Swin (ví dụ cho embed_dim=96):\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# patch_embeddings: 96 (từ 256/4 = 64x64)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# stage1: 192 (96*2) (từ 256/8 = 32x32)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Bottleneck: Lấy từ stage4 (kích thước 4x4)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\swin\\configuration_swin.py:158\u001b[0m, in \u001b[0;36mSwinConfig.__init__\u001b[1;34m(self, image_size, patch_size, num_channels, embed_dim, depths, num_heads, window_size, mlp_ratio, qkv_bias, hidden_dropout_prob, attention_probs_dropout_prob, drop_path_rate, hidden_act, use_absolute_embeddings, initializer_range, layer_norm_eps, encoder_stride, out_features, out_indices, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(embed_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(depths) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(depths) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_out_features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_out_indices \u001b[38;5;241m=\u001b[39m \u001b[43mget_aligned_output_features_output_indices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage_names\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\utils\\backbone_utils.py:131\u001b[0m, in \u001b[0;36mget_aligned_output_features_output_indices\u001b[1;34m(out_features, out_indices, stage_names)\u001b[0m\n\u001b[0;32m    129\u001b[0m out_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out_indices) \u001b[38;5;28;01mif\u001b[39;00m out_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# First verify that the out_features and out_indices are valid\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m \u001b[43mverify_out_features_out_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstage_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m output_features, output_indices \u001b[38;5;241m=\u001b[39m _align_output_features_output_indices(\n\u001b[0;32m    133\u001b[0m     out_features\u001b[38;5;241m=\u001b[39mout_features, out_indices\u001b[38;5;241m=\u001b[39mout_indices, stage_names\u001b[38;5;241m=\u001b[39mstage_names\n\u001b[0;32m    134\u001b[0m )\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Verify that the aligned out_features and out_indices are valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\utils\\backbone_utils.py:45\u001b[0m, in \u001b[0;36mverify_out_features_out_indices\u001b[1;34m(out_features, out_indices, stage_names)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_features must be a list got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(out_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(feat \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stage_names \u001b[38;5;28;01mfor\u001b[39;00m feat \u001b[38;5;129;01min\u001b[39;00m out_features):\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_features must be a subset of stage_names: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out_features) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(out_features)):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_features must not contain any duplicates, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: out_features must be a subset of stage_names: ['stem', 'stage1', 'stage2', 'stage3', 'stage4'] got ['patch_embeddings', 'stage1', 'stage2', 'stage3', 'stage4']"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(predicted_masks, true_masks, smooth=1e-6):\n",
    "    intersection = (predicted_masks * true_masks).sum()\n",
    "    union = (predicted_masks + true_masks).sum() - intersection\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    dice = (2. * intersection + smooth) / ((predicted_masks.sum() + true_masks.sum()) + smooth)\n",
    "    f1_score = dice \n",
    "\n",
    "    return iou.item(), f1_score.item()\n",
    "\n",
    "# Thêm tham số start_epoch và best_val_loss_so_far để tiếp tục từ checkpoint\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, callbacks_config, start_epoch=0, best_val_loss_so_far=float('inf')):\n",
    "    best_val_loss = best_val_loss_so_far \n",
    "    patience_counter = 0\n",
    "    model_checkpoint_path = callbacks_config.get('checkpoint_path', 'swin_unet_best_pytorch.pth')\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_iou = 0.0\n",
    "        running_f1 = 0.0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Bắt đầu...\")\n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks) # BCEWithLogitsLoss nhận logits và targets\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Để tính toán metrics, chúng ta cần chuyển logits sang xác suất rồi sang mask nhị phân\n",
    "            predicted_masks = (torch.sigmoid(outputs) > 0.5).float() \n",
    "            \n",
    "            batch_iou, batch_f1 = calculate_metrics(predicted_masks, masks)\n",
    "            running_iou += batch_iou * images.size(0)\n",
    "            running_f1 += batch_f1 * images.size(0)\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_iou = running_iou / len(train_loader.dataset)\n",
    "        epoch_f1 = running_f1 / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1} Kết thúc - Mất mát Huấn luyện: {epoch_loss:.4f}, IoU Huấn luyện: {epoch_iou:.4f}, F1-Score Huấn luyện: {epoch_f1:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_iou = 0.0\n",
    "        val_f1 = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(DEVICE)\n",
    "                masks = masks.to(DEVICE)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                predicted_masks = (torch.sigmoid(outputs) > 0.5).float() \n",
    "                \n",
    "                batch_iou, batch_f1 = calculate_metrics(predicted_masks, masks)\n",
    "                val_iou += batch_iou * images.size(0)\n",
    "                val_f1 += batch_f1 * images.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_iou /= len(val_loader.dataset)\n",
    "        val_f1 /= len(val_loader.dataset)\n",
    "        print(f\"Mất mát Xác thực: {val_loss:.4f}, IoU Xác thực: {val_iou:.4f}, F1-Score Xác thực: {val_f1:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            print(f\"Mất mát xác thực tốt nhất được cập nhật: {best_val_loss:.4f}. Lưu mô hình và trạng thái...\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, model_checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Mất mát xác thực không cải thiện. Sự kiên nhẫn: {patience_counter}/{callbacks_config['patience']}\")\n",
    "            if patience_counter >= callbacks_config['patience']:\n",
    "                print(\"Dừng sớm!\")\n",
    "                break\n",
    "\n",
    "# --- Khởi tạo Dataset và DataLoader ---\n",
    "train_dataset = CrackDetectionDataset(train_img_paths, train_mask_paths, transform=train_transform)\n",
    "val_dataset = CrackDetectionDataset(val_img_paths, val_mask_paths, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0) # num_workers > 0 nếu bạn muốn tải dữ liệu song song\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- Khởi tạo mô hình, optimizer, criterion ---\n",
    "model = SwinUNet(input_channels=3, num_classes=1).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss() # Đây là hàm loss đúng khi đầu ra là logits\n",
    "\n",
    "# In cấu trúc mô hình để kiểm tra\n",
    "print(model)\n",
    "\n",
    "callbacks_config = {\n",
    "    'patience': 30, # Số epoch chờ trước khi dừng sớm nếu val_loss không cải thiện\n",
    "    'checkpoint_path': 'swin_unet_best_pytorch.pth'\n",
    "}\n",
    "\n",
    "# --- Logic để tiếp tục huấn luyện từ checkpoint ---\n",
    "start_epoch = 0\n",
    "best_val_loss_so_far = float('inf')\n",
    "checkpoint_path = callbacks_config['checkpoint_path']\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Phát hiện checkpoint tại {checkpoint_path}. Đang tải để tiếp tục huấn luyện...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1 \n",
    "    best_val_loss_so_far = checkpoint['best_val_loss']\n",
    "    \n",
    "    print(f\"Đã tải checkpoint từ Epoch {start_epoch-1}. Tiếp tục huấn luyện từ Epoch {start_epoch}.\")\n",
    "    print(f\"Mất mát xác thực tốt nhất trước đó: {best_val_loss_so_far:.4f}\")\n",
    "else:\n",
    "    print(\"Không tìm thấy checkpoint. Bắt đầu huấn luyện từ đầu (Epoch 0).\")\n",
    "\n",
    "print(\"\\nBắt đầu huấn luyện mô hình Swin-Unet...\")\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, \n",
    "            num_epochs=10000, callbacks_config=callbacks_config,\n",
    "            start_epoch=start_epoch, best_val_loss_so_far=best_val_loss_so_far)\n",
    "\n",
    "print(\"\\nQuá trình huấn luyện đã hoàn tất.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087d417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
