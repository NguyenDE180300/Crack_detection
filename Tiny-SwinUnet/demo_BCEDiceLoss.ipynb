{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d73b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Sử dụng thiết bị: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15064\\3353620380.py:447: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinUNet(\n",
      "  (swin): SwinModel(\n",
      "    (embeddings): SwinEmbeddings(\n",
      "      (patch_embeddings): SwinPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      )\n",
      "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): SwinEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (key): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (key): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.00909090880304575)\n",
      "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): SwinPatchMerging(\n",
      "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (key): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.0181818176060915)\n",
      "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (key): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.027272727340459824)\n",
      "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): SwinPatchMerging(\n",
      "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.036363635212183)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.045454543083906174)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.054545458406209946)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.06363636255264282)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.0727272778749466)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.08181818574666977)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): SwinPatchMerging(\n",
      "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.09090909361839294)\n",
      "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.10000000149011612)\n",
      "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
      "  )\n",
      "  (bottleneck): ConvBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder4): DecoderBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(1152, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder3): DecoderBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder2): DecoderBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(288, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder1): DecoderBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(144, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_upsample): DecoderBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (1): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_conv): Conv2d(24, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Phát hiện checkpoint tại swin_unet_best_pytorchDiceloss.pth. Đang tải để tiếp tục huấn luyện...\n",
      "Đã tải checkpoint từ Epoch 96. Tiếp tục huấn luyện từ Epoch 97.\n",
      "Mất mát xác thực tốt nhất trước đó: 0.4729\n",
      "\n",
      "Bắt đầu huấn luyện mô hình Swin-Unet...\n",
      "Epoch 98/10000 Bắt đầu...\n",
      "Epoch 98/10000, Batch 10/188, Loss: 0.5015\n",
      "Epoch 98/10000, Batch 20/188, Loss: 0.5839\n",
      "Epoch 98/10000, Batch 30/188, Loss: 0.4857\n",
      "Epoch 98/10000, Batch 40/188, Loss: 0.4950\n",
      "Epoch 98/10000, Batch 50/188, Loss: 0.8426\n",
      "Epoch 98/10000, Batch 60/188, Loss: 0.3616\n",
      "Epoch 98/10000, Batch 70/188, Loss: 0.6470\n",
      "Epoch 98/10000, Batch 80/188, Loss: 0.5347\n",
      "Epoch 98/10000, Batch 90/188, Loss: 0.5879\n",
      "Epoch 98/10000, Batch 100/188, Loss: 0.4820\n",
      "Epoch 98/10000, Batch 110/188, Loss: 0.5139\n",
      "Epoch 98/10000, Batch 120/188, Loss: 0.8433\n",
      "Epoch 98/10000, Batch 130/188, Loss: 0.4646\n",
      "Epoch 98/10000, Batch 140/188, Loss: 0.8780\n",
      "Epoch 98/10000, Batch 150/188, Loss: 0.4940\n",
      "Epoch 98/10000, Batch 160/188, Loss: 0.3627\n",
      "Epoch 98/10000, Batch 170/188, Loss: 0.3112\n",
      "Epoch 98/10000, Batch 180/188, Loss: 0.7452\n",
      "Epoch 98 Kết thúc - Mất mát Huấn luyện: 0.5375, IoU Huấn luyện: 0.5708, F1-Score Huấn luyện: 0.7190\n",
      "Mất mát Xác thực: 0.6256, IoU Xác thực: 0.3946, F1-Score Xác thực: 0.5305\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 1/30\n",
      "Epoch 99/10000 Bắt đầu...\n",
      "Epoch 99/10000, Batch 10/188, Loss: 1.3496\n",
      "Epoch 99/10000, Batch 20/188, Loss: 0.4983\n",
      "Epoch 99/10000, Batch 30/188, Loss: 0.3347\n",
      "Epoch 99/10000, Batch 40/188, Loss: 0.5581\n",
      "Epoch 99/10000, Batch 50/188, Loss: 0.7766\n",
      "Epoch 99/10000, Batch 60/188, Loss: 0.3856\n",
      "Epoch 99/10000, Batch 70/188, Loss: 0.3487\n",
      "Epoch 99/10000, Batch 80/188, Loss: 0.8350\n",
      "Epoch 99/10000, Batch 90/188, Loss: 0.5311\n",
      "Epoch 99/10000, Batch 100/188, Loss: 0.5577\n",
      "Epoch 99/10000, Batch 110/188, Loss: 0.3127\n",
      "Epoch 99/10000, Batch 120/188, Loss: 0.5089\n",
      "Epoch 99/10000, Batch 130/188, Loss: 1.0924\n",
      "Epoch 99/10000, Batch 140/188, Loss: 0.9491\n",
      "Epoch 99/10000, Batch 150/188, Loss: 0.8803\n",
      "Epoch 99/10000, Batch 160/188, Loss: 0.3209\n",
      "Epoch 99/10000, Batch 170/188, Loss: 0.3240\n",
      "Epoch 99/10000, Batch 180/188, Loss: 1.1233\n",
      "Epoch 99 Kết thúc - Mất mát Huấn luyện: 0.5217, IoU Huấn luyện: 0.5747, F1-Score Huấn luyện: 0.7222\n",
      "Mất mát Xác thực: 0.6207, IoU Xác thực: 0.3941, F1-Score Xác thực: 0.5292\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 2/30\n",
      "Epoch 100/10000 Bắt đầu...\n",
      "Epoch 100/10000, Batch 10/188, Loss: 0.5378\n",
      "Epoch 100/10000, Batch 20/188, Loss: 0.4736\n",
      "Epoch 100/10000, Batch 30/188, Loss: 0.5929\n",
      "Epoch 100/10000, Batch 40/188, Loss: 0.5092\n",
      "Epoch 100/10000, Batch 50/188, Loss: 0.3457\n",
      "Epoch 100/10000, Batch 60/188, Loss: 0.6182\n",
      "Epoch 100/10000, Batch 70/188, Loss: 0.3228\n",
      "Epoch 100/10000, Batch 80/188, Loss: 0.3790\n",
      "Epoch 100/10000, Batch 90/188, Loss: 0.4108\n",
      "Epoch 100/10000, Batch 100/188, Loss: 0.5594\n",
      "Epoch 100/10000, Batch 110/188, Loss: 0.4769\n",
      "Epoch 100/10000, Batch 120/188, Loss: 0.7579\n",
      "Epoch 100/10000, Batch 130/188, Loss: 0.5536\n",
      "Epoch 100/10000, Batch 140/188, Loss: 0.4260\n",
      "Epoch 100/10000, Batch 150/188, Loss: 0.4635\n",
      "Epoch 100/10000, Batch 160/188, Loss: 0.4270\n",
      "Epoch 100/10000, Batch 170/188, Loss: 0.3031\n",
      "Epoch 100/10000, Batch 180/188, Loss: 0.4756\n",
      "Epoch 100 Kết thúc - Mất mát Huấn luyện: 0.5032, IoU Huấn luyện: 0.5703, F1-Score Huấn luyện: 0.7196\n",
      "Mất mát Xác thực: 0.6125, IoU Xác thực: 0.3946, F1-Score Xác thực: 0.5303\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 3/30\n",
      "Epoch 101/10000 Bắt đầu...\n",
      "Epoch 101/10000, Batch 10/188, Loss: 0.6363\n",
      "Epoch 101/10000, Batch 20/188, Loss: 0.5101\n",
      "Epoch 101/10000, Batch 30/188, Loss: 0.9107\n",
      "Epoch 101/10000, Batch 40/188, Loss: 0.3556\n",
      "Epoch 101/10000, Batch 50/188, Loss: 0.3532\n",
      "Epoch 101/10000, Batch 60/188, Loss: 0.4777\n",
      "Epoch 101/10000, Batch 70/188, Loss: 0.4453\n",
      "Epoch 101/10000, Batch 80/188, Loss: 0.5223\n",
      "Epoch 101/10000, Batch 90/188, Loss: 0.2748\n",
      "Epoch 101/10000, Batch 100/188, Loss: 0.4562\n",
      "Epoch 101/10000, Batch 110/188, Loss: 0.8499\n",
      "Epoch 101/10000, Batch 120/188, Loss: 0.4533\n",
      "Epoch 101/10000, Batch 130/188, Loss: 0.5131\n",
      "Epoch 101/10000, Batch 140/188, Loss: 0.4910\n",
      "Epoch 101/10000, Batch 150/188, Loss: 0.3463\n",
      "Epoch 101/10000, Batch 160/188, Loss: 0.5232\n",
      "Epoch 101/10000, Batch 170/188, Loss: 0.3567\n",
      "Epoch 101/10000, Batch 180/188, Loss: 0.4130\n",
      "Epoch 101 Kết thúc - Mất mát Huấn luyện: 0.5049, IoU Huấn luyện: 0.5679, F1-Score Huấn luyện: 0.7170\n",
      "Mất mát Xác thực: 0.6016, IoU Xác thực: 0.3941, F1-Score Xác thực: 0.5312\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 4/30\n",
      "Epoch 102/10000 Bắt đầu...\n",
      "Epoch 102/10000, Batch 10/188, Loss: 0.3256\n",
      "Epoch 102/10000, Batch 20/188, Loss: 0.2699\n",
      "Epoch 102/10000, Batch 30/188, Loss: 0.4365\n",
      "Epoch 102/10000, Batch 40/188, Loss: 0.3038\n",
      "Epoch 102/10000, Batch 50/188, Loss: 0.6269\n",
      "Epoch 102/10000, Batch 60/188, Loss: 0.4505\n",
      "Epoch 102/10000, Batch 70/188, Loss: 0.7073\n",
      "Epoch 102/10000, Batch 80/188, Loss: 0.3278\n",
      "Epoch 102/10000, Batch 90/188, Loss: 0.7637\n",
      "Epoch 102/10000, Batch 100/188, Loss: 0.8635\n",
      "Epoch 102/10000, Batch 110/188, Loss: 0.3447\n",
      "Epoch 102/10000, Batch 120/188, Loss: 0.3401\n",
      "Epoch 102/10000, Batch 130/188, Loss: 0.4440\n",
      "Epoch 102/10000, Batch 140/188, Loss: 0.3372\n",
      "Epoch 102/10000, Batch 150/188, Loss: 0.6102\n",
      "Epoch 102/10000, Batch 160/188, Loss: 0.6264\n",
      "Epoch 102/10000, Batch 170/188, Loss: 0.4002\n",
      "Epoch 102/10000, Batch 180/188, Loss: 0.4197\n",
      "Epoch 102 Kết thúc - Mất mát Huấn luyện: 0.5011, IoU Huấn luyện: 0.5645, F1-Score Huấn luyện: 0.7146\n",
      "Mất mát Xác thực: 0.5934, IoU Xác thực: 0.3920, F1-Score Xác thực: 0.5300\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 5/30\n",
      "Epoch 103/10000 Bắt đầu...\n",
      "Epoch 103/10000, Batch 10/188, Loss: 0.5660\n",
      "Epoch 103/10000, Batch 20/188, Loss: 0.6655\n",
      "Epoch 103/10000, Batch 30/188, Loss: 0.3737\n",
      "Epoch 103/10000, Batch 40/188, Loss: 0.3504\n",
      "Epoch 103/10000, Batch 50/188, Loss: 0.5099\n",
      "Epoch 103/10000, Batch 60/188, Loss: 1.1068\n",
      "Epoch 103/10000, Batch 70/188, Loss: 0.5872\n",
      "Epoch 103/10000, Batch 80/188, Loss: 0.6346\n",
      "Epoch 103/10000, Batch 90/188, Loss: 1.0007\n",
      "Epoch 103/10000, Batch 100/188, Loss: 0.5686\n",
      "Epoch 103/10000, Batch 110/188, Loss: 0.3449\n",
      "Epoch 103/10000, Batch 120/188, Loss: 0.3896\n",
      "Epoch 103/10000, Batch 130/188, Loss: 0.3142\n",
      "Epoch 103/10000, Batch 140/188, Loss: 0.6535\n",
      "Epoch 103/10000, Batch 150/188, Loss: 0.3329\n",
      "Epoch 103/10000, Batch 160/188, Loss: 0.7236\n",
      "Epoch 103/10000, Batch 170/188, Loss: 0.3826\n",
      "Epoch 103/10000, Batch 180/188, Loss: 0.4880\n",
      "Epoch 103 Kết thúc - Mất mát Huấn luyện: 0.4954, IoU Huấn luyện: 0.5611, F1-Score Huấn luyện: 0.7122\n",
      "Mất mát Xác thực: 0.5889, IoU Xác thực: 0.3929, F1-Score Xác thực: 0.5304\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 6/30\n",
      "Epoch 104/10000 Bắt đầu...\n",
      "Epoch 104/10000, Batch 10/188, Loss: 0.3045\n",
      "Epoch 104/10000, Batch 20/188, Loss: 0.4106\n",
      "Epoch 104/10000, Batch 30/188, Loss: 0.5203\n",
      "Epoch 104/10000, Batch 40/188, Loss: 0.7932\n",
      "Epoch 104/10000, Batch 50/188, Loss: 0.8815\n",
      "Epoch 104/10000, Batch 60/188, Loss: 0.2597\n",
      "Epoch 104/10000, Batch 70/188, Loss: 0.5593\n",
      "Epoch 104/10000, Batch 80/188, Loss: 0.4237\n",
      "Epoch 104/10000, Batch 90/188, Loss: 0.5067\n",
      "Epoch 104/10000, Batch 100/188, Loss: 0.4733\n",
      "Epoch 104/10000, Batch 110/188, Loss: 0.5166\n",
      "Epoch 104/10000, Batch 120/188, Loss: 0.5181\n",
      "Epoch 104/10000, Batch 130/188, Loss: 0.3711\n",
      "Epoch 104/10000, Batch 140/188, Loss: 0.4641\n",
      "Epoch 104/10000, Batch 150/188, Loss: 0.2230\n",
      "Epoch 104/10000, Batch 160/188, Loss: 0.3879\n",
      "Epoch 104/10000, Batch 170/188, Loss: 0.5149\n",
      "Epoch 104/10000, Batch 180/188, Loss: 0.4367\n",
      "Epoch 104 Kết thúc - Mất mát Huấn luyện: 0.4898, IoU Huấn luyện: 0.5670, F1-Score Huấn luyện: 0.7169\n",
      "Mất mát Xác thực: 0.5820, IoU Xác thực: 0.3918, F1-Score Xác thực: 0.5299\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 7/30\n",
      "Epoch 105/10000 Bắt đầu...\n",
      "Epoch 105/10000, Batch 10/188, Loss: 0.4900\n",
      "Epoch 105/10000, Batch 20/188, Loss: 0.5493\n",
      "Epoch 105/10000, Batch 30/188, Loss: 0.5180\n",
      "Epoch 105/10000, Batch 40/188, Loss: 0.4449\n",
      "Epoch 105/10000, Batch 50/188, Loss: 0.4303\n",
      "Epoch 105/10000, Batch 60/188, Loss: 0.6163\n",
      "Epoch 105/10000, Batch 70/188, Loss: 0.3445\n",
      "Epoch 105/10000, Batch 80/188, Loss: 0.4132\n",
      "Epoch 105/10000, Batch 90/188, Loss: 0.3283\n",
      "Epoch 105/10000, Batch 100/188, Loss: 0.4252\n",
      "Epoch 105/10000, Batch 110/188, Loss: 0.4931\n",
      "Epoch 105/10000, Batch 120/188, Loss: 0.3746\n",
      "Epoch 105/10000, Batch 130/188, Loss: 0.5252\n",
      "Epoch 105/10000, Batch 140/188, Loss: 0.3659\n",
      "Epoch 105/10000, Batch 150/188, Loss: 0.5280\n",
      "Epoch 105/10000, Batch 160/188, Loss: 0.3643\n",
      "Epoch 105/10000, Batch 170/188, Loss: 0.3950\n",
      "Epoch 105/10000, Batch 180/188, Loss: 0.4091\n",
      "Epoch 105 Kết thúc - Mất mát Huấn luyện: 0.4798, IoU Huấn luyện: 0.5625, F1-Score Huấn luyện: 0.7137\n",
      "Mất mát Xác thực: 0.5774, IoU Xác thực: 0.3914, F1-Score Xác thực: 0.5295\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 8/30\n",
      "Epoch 106/10000 Bắt đầu...\n",
      "Epoch 106/10000, Batch 10/188, Loss: 0.5087\n",
      "Epoch 106/10000, Batch 20/188, Loss: 0.6933\n",
      "Epoch 106/10000, Batch 30/188, Loss: 0.3113\n",
      "Epoch 106/10000, Batch 40/188, Loss: 0.3165\n",
      "Epoch 106/10000, Batch 50/188, Loss: 0.5356\n",
      "Epoch 106/10000, Batch 60/188, Loss: 0.5653\n",
      "Epoch 106/10000, Batch 70/188, Loss: 0.5991\n",
      "Epoch 106/10000, Batch 80/188, Loss: 0.6095\n",
      "Epoch 106/10000, Batch 90/188, Loss: 0.3832\n",
      "Epoch 106/10000, Batch 100/188, Loss: 0.5718\n",
      "Epoch 106/10000, Batch 110/188, Loss: 0.3759\n",
      "Epoch 106/10000, Batch 120/188, Loss: 0.2289\n",
      "Epoch 106/10000, Batch 130/188, Loss: 0.5851\n",
      "Epoch 106/10000, Batch 140/188, Loss: 0.2974\n",
      "Epoch 106/10000, Batch 150/188, Loss: 0.4388\n",
      "Epoch 106/10000, Batch 160/188, Loss: 0.4589\n",
      "Epoch 106/10000, Batch 170/188, Loss: 0.6382\n",
      "Epoch 106/10000, Batch 180/188, Loss: 0.3994\n",
      "Epoch 106 Kết thúc - Mất mát Huấn luyện: 0.4664, IoU Huấn luyện: 0.5685, F1-Score Huấn luyện: 0.7183\n",
      "Mất mát Xác thực: 0.5771, IoU Xác thực: 0.3902, F1-Score Xác thực: 0.5283\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 9/30\n",
      "Epoch 107/10000 Bắt đầu...\n",
      "Epoch 107/10000, Batch 10/188, Loss: 0.2472\n",
      "Epoch 107/10000, Batch 20/188, Loss: 0.5821\n",
      "Epoch 107/10000, Batch 30/188, Loss: 0.5118\n",
      "Epoch 107/10000, Batch 40/188, Loss: 0.5044\n",
      "Epoch 107/10000, Batch 50/188, Loss: 0.2424\n",
      "Epoch 107/10000, Batch 60/188, Loss: 0.5264\n",
      "Epoch 107/10000, Batch 70/188, Loss: 0.2916\n",
      "Epoch 107/10000, Batch 80/188, Loss: 0.3932\n",
      "Epoch 107/10000, Batch 90/188, Loss: 0.5524\n",
      "Epoch 107/10000, Batch 100/188, Loss: 0.4717\n",
      "Epoch 107/10000, Batch 110/188, Loss: 0.3217\n",
      "Epoch 107/10000, Batch 120/188, Loss: 0.6629\n",
      "Epoch 107/10000, Batch 130/188, Loss: 0.3602\n",
      "Epoch 107/10000, Batch 140/188, Loss: 0.4516\n",
      "Epoch 107/10000, Batch 150/188, Loss: 0.5259\n",
      "Epoch 107/10000, Batch 160/188, Loss: 0.4866\n",
      "Epoch 107/10000, Batch 170/188, Loss: 0.4306\n",
      "Epoch 107/10000, Batch 180/188, Loss: 0.3076\n",
      "Epoch 107 Kết thúc - Mất mát Huấn luyện: 0.4748, IoU Huấn luyện: 0.5595, F1-Score Huấn luyện: 0.7105\n",
      "Mất mát Xác thực: 0.5756, IoU Xác thực: 0.3893, F1-Score Xác thực: 0.5281\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 10/30\n",
      "Epoch 108/10000 Bắt đầu...\n",
      "Epoch 108/10000, Batch 10/188, Loss: 0.8722\n",
      "Epoch 108/10000, Batch 20/188, Loss: 0.2935\n",
      "Epoch 108/10000, Batch 30/188, Loss: 0.3569\n",
      "Epoch 108/10000, Batch 40/188, Loss: 0.4654\n",
      "Epoch 108/10000, Batch 50/188, Loss: 0.4398\n",
      "Epoch 108/10000, Batch 60/188, Loss: 0.2577\n",
      "Epoch 108/10000, Batch 70/188, Loss: 0.7433\n",
      "Epoch 108/10000, Batch 80/188, Loss: 0.6419\n",
      "Epoch 108/10000, Batch 90/188, Loss: 0.4699\n",
      "Epoch 108/10000, Batch 100/188, Loss: 0.4371\n",
      "Epoch 108/10000, Batch 110/188, Loss: 0.3416\n",
      "Epoch 108/10000, Batch 120/188, Loss: 0.4823\n",
      "Epoch 108/10000, Batch 130/188, Loss: 0.7467\n",
      "Epoch 108/10000, Batch 140/188, Loss: 0.4291\n",
      "Epoch 108/10000, Batch 150/188, Loss: 0.9289\n",
      "Epoch 108/10000, Batch 160/188, Loss: 0.3150\n",
      "Epoch 108/10000, Batch 170/188, Loss: 0.5234\n",
      "Epoch 108/10000, Batch 180/188, Loss: 0.3862\n",
      "Epoch 108 Kết thúc - Mất mát Huấn luyện: 0.4718, IoU Huấn luyện: 0.5547, F1-Score Huấn luyện: 0.7067\n",
      "Mất mát Xác thực: 0.5722, IoU Xác thực: 0.3880, F1-Score Xác thực: 0.5269\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 11/30\n",
      "Epoch 109/10000 Bắt đầu...\n",
      "Epoch 109/10000, Batch 10/188, Loss: 0.3451\n",
      "Epoch 109/10000, Batch 20/188, Loss: 0.4821\n",
      "Epoch 109/10000, Batch 30/188, Loss: 0.3873\n",
      "Epoch 109/10000, Batch 40/188, Loss: 0.6917\n",
      "Epoch 109/10000, Batch 50/188, Loss: 0.3572\n",
      "Epoch 109/10000, Batch 60/188, Loss: 0.5132\n",
      "Epoch 109/10000, Batch 70/188, Loss: 0.3597\n",
      "Epoch 109/10000, Batch 80/188, Loss: 0.6152\n",
      "Epoch 109/10000, Batch 90/188, Loss: 0.4821\n",
      "Epoch 109/10000, Batch 100/188, Loss: 0.8122\n",
      "Epoch 109/10000, Batch 110/188, Loss: 0.5460\n",
      "Epoch 109/10000, Batch 120/188, Loss: 0.5761\n",
      "Epoch 109/10000, Batch 130/188, Loss: 0.5089\n",
      "Epoch 109/10000, Batch 140/188, Loss: 0.5738\n",
      "Epoch 109/10000, Batch 150/188, Loss: 0.4218\n",
      "Epoch 109/10000, Batch 160/188, Loss: 0.3972\n",
      "Epoch 109/10000, Batch 170/188, Loss: 0.7667\n",
      "Epoch 109/10000, Batch 180/188, Loss: 0.4336\n",
      "Epoch 109 Kết thúc - Mất mát Huấn luyện: 0.4691, IoU Huấn luyện: 0.5624, F1-Score Huấn luyện: 0.7137\n",
      "Mất mát Xác thực: 0.5717, IoU Xác thực: 0.3896, F1-Score Xác thực: 0.5281\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 12/30\n",
      "Epoch 110/10000 Bắt đầu...\n",
      "Epoch 110/10000, Batch 10/188, Loss: 0.3370\n",
      "Epoch 110/10000, Batch 20/188, Loss: 0.3568\n",
      "Epoch 110/10000, Batch 30/188, Loss: 0.4628\n",
      "Epoch 110/10000, Batch 40/188, Loss: 0.4955\n",
      "Epoch 110/10000, Batch 50/188, Loss: 0.3345\n",
      "Epoch 110/10000, Batch 60/188, Loss: 0.8232\n",
      "Epoch 110/10000, Batch 70/188, Loss: 0.3532\n",
      "Epoch 110/10000, Batch 80/188, Loss: 0.3166\n",
      "Epoch 110/10000, Batch 90/188, Loss: 0.2476\n",
      "Epoch 110/10000, Batch 100/188, Loss: 0.2422\n",
      "Epoch 110/10000, Batch 110/188, Loss: 0.4578\n",
      "Epoch 110/10000, Batch 120/188, Loss: 0.4322\n",
      "Epoch 110/10000, Batch 130/188, Loss: 0.2595\n",
      "Epoch 110/10000, Batch 140/188, Loss: 0.3307\n",
      "Epoch 110/10000, Batch 150/188, Loss: 0.3884\n",
      "Epoch 110/10000, Batch 160/188, Loss: 0.5143\n",
      "Epoch 110/10000, Batch 170/188, Loss: 0.2551\n",
      "Epoch 110/10000, Batch 180/188, Loss: 0.9355\n",
      "Epoch 110 Kết thúc - Mất mát Huấn luyện: 0.4595, IoU Huấn luyện: 0.5532, F1-Score Huấn luyện: 0.7063\n",
      "Mất mát Xác thực: 0.5655, IoU Xác thực: 0.3879, F1-Score Xác thực: 0.5269\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 13/30\n",
      "Epoch 111/10000 Bắt đầu...\n",
      "Epoch 111/10000, Batch 10/188, Loss: 0.5513\n",
      "Epoch 111/10000, Batch 20/188, Loss: 0.2927\n",
      "Epoch 111/10000, Batch 30/188, Loss: 0.4698\n",
      "Epoch 111/10000, Batch 40/188, Loss: 0.4398\n",
      "Epoch 111/10000, Batch 50/188, Loss: 0.4087\n",
      "Epoch 111/10000, Batch 60/188, Loss: 0.4274\n",
      "Epoch 111/10000, Batch 70/188, Loss: 0.6786\n",
      "Epoch 111/10000, Batch 80/188, Loss: 0.3245\n",
      "Epoch 111/10000, Batch 90/188, Loss: 0.2811\n",
      "Epoch 111/10000, Batch 100/188, Loss: 0.3051\n",
      "Epoch 111/10000, Batch 110/188, Loss: 0.2857\n",
      "Epoch 111/10000, Batch 120/188, Loss: 0.4058\n",
      "Epoch 111/10000, Batch 130/188, Loss: 0.4841\n",
      "Epoch 111/10000, Batch 140/188, Loss: 0.2535\n",
      "Epoch 111/10000, Batch 150/188, Loss: 0.5489\n",
      "Epoch 111/10000, Batch 160/188, Loss: 0.4281\n",
      "Epoch 111/10000, Batch 170/188, Loss: 0.6364\n",
      "Epoch 111/10000, Batch 180/188, Loss: 0.6100\n",
      "Epoch 111 Kết thúc - Mất mát Huấn luyện: 0.4633, IoU Huấn luyện: 0.5479, F1-Score Huấn luyện: 0.6997\n",
      "Mất mát Xác thực: 0.5601, IoU Xác thực: 0.3877, F1-Score Xác thực: 0.5268\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 14/30\n",
      "Epoch 112/10000 Bắt đầu...\n",
      "Epoch 112/10000, Batch 10/188, Loss: 0.2891\n",
      "Epoch 112/10000, Batch 20/188, Loss: 0.2550\n",
      "Epoch 112/10000, Batch 30/188, Loss: 0.2658\n",
      "Epoch 112/10000, Batch 40/188, Loss: 0.3129\n",
      "Epoch 112/10000, Batch 50/188, Loss: 0.1976\n",
      "Epoch 112/10000, Batch 60/188, Loss: 0.4097\n",
      "Epoch 112/10000, Batch 70/188, Loss: 0.3724\n",
      "Epoch 112/10000, Batch 80/188, Loss: 0.4430\n",
      "Epoch 112/10000, Batch 90/188, Loss: 0.4499\n",
      "Epoch 112/10000, Batch 100/188, Loss: 0.5755\n",
      "Epoch 112/10000, Batch 110/188, Loss: 0.5000\n",
      "Epoch 112/10000, Batch 120/188, Loss: 0.2561\n",
      "Epoch 112/10000, Batch 130/188, Loss: 0.5585\n",
      "Epoch 112/10000, Batch 140/188, Loss: 0.4270\n",
      "Epoch 112/10000, Batch 150/188, Loss: 1.0117\n",
      "Epoch 112/10000, Batch 160/188, Loss: 0.6134\n",
      "Epoch 112/10000, Batch 170/188, Loss: 0.8699\n",
      "Epoch 112/10000, Batch 180/188, Loss: 0.3981\n",
      "Epoch 112 Kết thúc - Mất mát Huấn luyện: 0.4650, IoU Huấn luyện: 0.5517, F1-Score Huấn luyện: 0.7040\n",
      "Mất mát Xác thực: 0.5587, IoU Xác thực: 0.3879, F1-Score Xác thực: 0.5266\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 15/30\n",
      "Epoch 113/10000 Bắt đầu...\n",
      "Epoch 113/10000, Batch 10/188, Loss: 0.2906\n",
      "Epoch 113/10000, Batch 20/188, Loss: 0.3805\n",
      "Epoch 113/10000, Batch 30/188, Loss: 0.4243\n",
      "Epoch 113/10000, Batch 40/188, Loss: 0.3295\n",
      "Epoch 113/10000, Batch 50/188, Loss: 0.5090\n",
      "Epoch 113/10000, Batch 60/188, Loss: 0.3914\n",
      "Epoch 113/10000, Batch 70/188, Loss: 0.2401\n",
      "Epoch 113/10000, Batch 80/188, Loss: 0.4925\n",
      "Epoch 113/10000, Batch 90/188, Loss: 0.4170\n",
      "Epoch 113/10000, Batch 100/188, Loss: 0.5139\n",
      "Epoch 113/10000, Batch 110/188, Loss: 0.8440\n",
      "Epoch 113/10000, Batch 120/188, Loss: 0.6889\n",
      "Epoch 113/10000, Batch 130/188, Loss: 0.6431\n",
      "Epoch 113/10000, Batch 140/188, Loss: 0.4909\n",
      "Epoch 113/10000, Batch 150/188, Loss: 0.2097\n",
      "Epoch 113/10000, Batch 160/188, Loss: 0.5336\n",
      "Epoch 113/10000, Batch 170/188, Loss: 0.3512\n",
      "Epoch 113/10000, Batch 180/188, Loss: 0.5352\n",
      "Epoch 113 Kết thúc - Mất mát Huấn luyện: 0.4503, IoU Huấn luyện: 0.5540, F1-Score Huấn luyện: 0.7061\n",
      "Mất mát Xác thực: 0.5568, IoU Xác thực: 0.3859, F1-Score Xác thực: 0.5252\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 16/30\n",
      "Epoch 114/10000 Bắt đầu...\n",
      "Epoch 114/10000, Batch 10/188, Loss: 0.4148\n",
      "Epoch 114/10000, Batch 20/188, Loss: 0.3598\n",
      "Epoch 114/10000, Batch 30/188, Loss: 0.3452\n",
      "Epoch 114/10000, Batch 40/188, Loss: 0.3124\n",
      "Epoch 114/10000, Batch 50/188, Loss: 0.9190\n",
      "Epoch 114/10000, Batch 60/188, Loss: 0.3132\n",
      "Epoch 114/10000, Batch 70/188, Loss: 0.7197\n",
      "Epoch 114/10000, Batch 80/188, Loss: 0.2852\n",
      "Epoch 114/10000, Batch 90/188, Loss: 0.3235\n",
      "Epoch 114/10000, Batch 100/188, Loss: 0.5124\n",
      "Epoch 114/10000, Batch 110/188, Loss: 0.4517\n",
      "Epoch 114/10000, Batch 120/188, Loss: 0.5756\n",
      "Epoch 114/10000, Batch 130/188, Loss: 0.3212\n",
      "Epoch 114/10000, Batch 140/188, Loss: 0.5354\n",
      "Epoch 114/10000, Batch 150/188, Loss: 0.4032\n",
      "Epoch 114/10000, Batch 160/188, Loss: 0.5805\n",
      "Epoch 114/10000, Batch 170/188, Loss: 0.6607\n",
      "Epoch 114/10000, Batch 180/188, Loss: 0.3195\n",
      "Epoch 114 Kết thúc - Mất mát Huấn luyện: 0.4614, IoU Huấn luyện: 0.5488, F1-Score Huấn luyện: 0.7009\n",
      "Mất mát Xác thực: 0.5554, IoU Xác thực: 0.3877, F1-Score Xác thực: 0.5266\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 17/30\n",
      "Epoch 115/10000 Bắt đầu...\n",
      "Epoch 115/10000, Batch 10/188, Loss: 0.3733\n",
      "Epoch 115/10000, Batch 20/188, Loss: 0.8019\n",
      "Epoch 115/10000, Batch 30/188, Loss: 0.3790\n",
      "Epoch 115/10000, Batch 40/188, Loss: 0.4035\n",
      "Epoch 115/10000, Batch 50/188, Loss: 0.2749\n",
      "Epoch 115/10000, Batch 60/188, Loss: 0.4214\n",
      "Epoch 115/10000, Batch 70/188, Loss: 0.4036\n",
      "Epoch 115/10000, Batch 80/188, Loss: 0.6567\n",
      "Epoch 115/10000, Batch 90/188, Loss: 0.3363\n",
      "Epoch 115/10000, Batch 100/188, Loss: 0.3323\n",
      "Epoch 115/10000, Batch 110/188, Loss: 0.4556\n",
      "Epoch 115/10000, Batch 120/188, Loss: 0.7763\n",
      "Epoch 115/10000, Batch 130/188, Loss: 0.4222\n",
      "Epoch 115/10000, Batch 140/188, Loss: 0.7412\n",
      "Epoch 115/10000, Batch 150/188, Loss: 0.4840\n",
      "Epoch 115/10000, Batch 160/188, Loss: 0.5147\n",
      "Epoch 115/10000, Batch 170/188, Loss: 0.4077\n",
      "Epoch 115/10000, Batch 180/188, Loss: 0.4451\n",
      "Epoch 115 Kết thúc - Mất mát Huấn luyện: 0.4558, IoU Huấn luyện: 0.5479, F1-Score Huấn luyện: 0.7021\n",
      "Mất mát Xác thực: 0.5552, IoU Xác thực: 0.3880, F1-Score Xác thực: 0.5268\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 18/30\n",
      "Epoch 116/10000 Bắt đầu...\n",
      "Epoch 116/10000, Batch 10/188, Loss: 0.4980\n",
      "Epoch 116/10000, Batch 20/188, Loss: 0.3542\n",
      "Epoch 116/10000, Batch 30/188, Loss: 0.5072\n",
      "Epoch 116/10000, Batch 40/188, Loss: 0.6739\n",
      "Epoch 116/10000, Batch 50/188, Loss: 0.5080\n",
      "Epoch 116/10000, Batch 60/188, Loss: 0.5901\n",
      "Epoch 116/10000, Batch 70/188, Loss: 0.5338\n",
      "Epoch 116/10000, Batch 80/188, Loss: 0.3684\n",
      "Epoch 116/10000, Batch 90/188, Loss: 0.5071\n",
      "Epoch 116/10000, Batch 100/188, Loss: 0.6910\n",
      "Epoch 116/10000, Batch 110/188, Loss: 0.3993\n",
      "Epoch 116/10000, Batch 120/188, Loss: 0.5622\n",
      "Epoch 116/10000, Batch 130/188, Loss: 0.4551\n",
      "Epoch 116/10000, Batch 140/188, Loss: 0.3314\n",
      "Epoch 116/10000, Batch 150/188, Loss: 0.3824\n",
      "Epoch 116/10000, Batch 160/188, Loss: 0.2268\n",
      "Epoch 116/10000, Batch 170/188, Loss: 0.4498\n",
      "Epoch 116/10000, Batch 180/188, Loss: 0.4374\n",
      "Epoch 116 Kết thúc - Mất mát Huấn luyện: 0.4489, IoU Huấn luyện: 0.5458, F1-Score Huấn luyện: 0.6979\n",
      "Mất mát Xác thực: 0.5551, IoU Xác thực: 0.3839, F1-Score Xác thực: 0.5242\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 19/30\n",
      "Epoch 117/10000 Bắt đầu...\n",
      "Epoch 117/10000, Batch 10/188, Loss: 0.7100\n",
      "Epoch 117/10000, Batch 20/188, Loss: 0.5344\n",
      "Epoch 117/10000, Batch 30/188, Loss: 0.3846\n",
      "Epoch 117/10000, Batch 40/188, Loss: 0.5079\n",
      "Epoch 117/10000, Batch 50/188, Loss: 0.3142\n",
      "Epoch 117/10000, Batch 60/188, Loss: 0.3743\n",
      "Epoch 117/10000, Batch 70/188, Loss: 0.3343\n",
      "Epoch 117/10000, Batch 80/188, Loss: 0.6014\n",
      "Epoch 117/10000, Batch 90/188, Loss: 0.5051\n",
      "Epoch 117/10000, Batch 100/188, Loss: 0.4299\n",
      "Epoch 117/10000, Batch 110/188, Loss: 0.5872\n",
      "Epoch 117/10000, Batch 120/188, Loss: 0.6675\n",
      "Epoch 117/10000, Batch 130/188, Loss: 0.3668\n",
      "Epoch 117/10000, Batch 140/188, Loss: 0.4562\n",
      "Epoch 117/10000, Batch 150/188, Loss: 0.6372\n",
      "Epoch 117/10000, Batch 160/188, Loss: 0.4541\n",
      "Epoch 117/10000, Batch 170/188, Loss: 0.5064\n",
      "Epoch 117/10000, Batch 180/188, Loss: 0.5902\n",
      "Epoch 117 Kết thúc - Mất mát Huấn luyện: 0.4553, IoU Huấn luyện: 0.5381, F1-Score Huấn luyện: 0.6924\n",
      "Mất mát Xác thực: 0.5463, IoU Xác thực: 0.3857, F1-Score Xác thực: 0.5254\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 20/30\n",
      "Epoch 118/10000 Bắt đầu...\n",
      "Epoch 118/10000, Batch 10/188, Loss: 0.5880\n",
      "Epoch 118/10000, Batch 20/188, Loss: 0.2744\n",
      "Epoch 118/10000, Batch 30/188, Loss: 0.4081\n",
      "Epoch 118/10000, Batch 40/188, Loss: 0.1500\n",
      "Epoch 118/10000, Batch 50/188, Loss: 0.4476\n",
      "Epoch 118/10000, Batch 60/188, Loss: 0.3897\n",
      "Epoch 118/10000, Batch 70/188, Loss: 0.8798\n",
      "Epoch 118/10000, Batch 80/188, Loss: 0.3684\n",
      "Epoch 118/10000, Batch 90/188, Loss: 0.7134\n",
      "Epoch 118/10000, Batch 100/188, Loss: 0.5585\n",
      "Epoch 118/10000, Batch 110/188, Loss: 0.8055\n",
      "Epoch 118/10000, Batch 120/188, Loss: 0.4787\n",
      "Epoch 118/10000, Batch 130/188, Loss: 0.3255\n",
      "Epoch 118/10000, Batch 140/188, Loss: 0.5905\n",
      "Epoch 118/10000, Batch 150/188, Loss: 0.7113\n",
      "Epoch 118/10000, Batch 160/188, Loss: 0.6723\n",
      "Epoch 118/10000, Batch 170/188, Loss: 0.4467\n",
      "Epoch 118/10000, Batch 180/188, Loss: 0.6211\n",
      "Epoch 118 Kết thúc - Mất mát Huấn luyện: 0.4474, IoU Huấn luyện: 0.5490, F1-Score Huấn luyện: 0.7016\n",
      "Mất mát Xác thực: 0.5457, IoU Xác thực: 0.3862, F1-Score Xác thực: 0.5251\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 21/30\n",
      "Epoch 119/10000 Bắt đầu...\n",
      "Epoch 119/10000, Batch 10/188, Loss: 0.2986\n",
      "Epoch 119/10000, Batch 20/188, Loss: 0.4738\n",
      "Epoch 119/10000, Batch 30/188, Loss: 0.5388\n",
      "Epoch 119/10000, Batch 40/188, Loss: 0.8859\n",
      "Epoch 119/10000, Batch 50/188, Loss: 0.3803\n",
      "Epoch 119/10000, Batch 60/188, Loss: 0.8362\n",
      "Epoch 119/10000, Batch 70/188, Loss: 0.3114\n",
      "Epoch 119/10000, Batch 80/188, Loss: 0.5452\n",
      "Epoch 119/10000, Batch 90/188, Loss: 0.3917\n",
      "Epoch 119/10000, Batch 100/188, Loss: 0.2740\n",
      "Epoch 119/10000, Batch 110/188, Loss: 0.4674\n",
      "Epoch 119/10000, Batch 120/188, Loss: 0.4569\n",
      "Epoch 119/10000, Batch 130/188, Loss: 0.6930\n",
      "Epoch 119/10000, Batch 140/188, Loss: 0.3581\n",
      "Epoch 119/10000, Batch 150/188, Loss: 0.2867\n",
      "Epoch 119/10000, Batch 160/188, Loss: 0.6880\n",
      "Epoch 119/10000, Batch 170/188, Loss: 0.5454\n",
      "Epoch 119/10000, Batch 180/188, Loss: 0.4613\n",
      "Epoch 119 Kết thúc - Mất mát Huấn luyện: 0.4419, IoU Huấn luyện: 0.5478, F1-Score Huấn luyện: 0.7002\n",
      "Mất mát Xác thực: 0.5483, IoU Xác thực: 0.3869, F1-Score Xác thực: 0.5265\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 22/30\n",
      "Epoch 120/10000 Bắt đầu...\n",
      "Epoch 120/10000, Batch 10/188, Loss: 0.4879\n",
      "Epoch 120/10000, Batch 20/188, Loss: 0.3621\n",
      "Epoch 120/10000, Batch 30/188, Loss: 0.5091\n",
      "Epoch 120/10000, Batch 40/188, Loss: 0.3775\n",
      "Epoch 120/10000, Batch 50/188, Loss: 0.3392\n",
      "Epoch 120/10000, Batch 60/188, Loss: 0.3446\n",
      "Epoch 120/10000, Batch 70/188, Loss: 0.2340\n",
      "Epoch 120/10000, Batch 80/188, Loss: 0.3775\n",
      "Epoch 120/10000, Batch 90/188, Loss: 0.3437\n",
      "Epoch 120/10000, Batch 100/188, Loss: 0.2508\n",
      "Epoch 120/10000, Batch 110/188, Loss: 0.3829\n",
      "Epoch 120/10000, Batch 120/188, Loss: 0.5004\n",
      "Epoch 120/10000, Batch 130/188, Loss: 0.3212\n",
      "Epoch 120/10000, Batch 140/188, Loss: 0.5793\n",
      "Epoch 120/10000, Batch 150/188, Loss: 0.4156\n",
      "Epoch 120/10000, Batch 160/188, Loss: 0.4434\n",
      "Epoch 120/10000, Batch 170/188, Loss: 0.6404\n",
      "Epoch 120/10000, Batch 180/188, Loss: 0.2981\n",
      "Epoch 120 Kết thúc - Mất mát Huấn luyện: 0.4406, IoU Huấn luyện: 0.5462, F1-Score Huấn luyện: 0.7000\n",
      "Mất mát Xác thực: 0.5392, IoU Xác thực: 0.3857, F1-Score Xác thực: 0.5257\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 23/30\n",
      "Epoch 121/10000 Bắt đầu...\n",
      "Epoch 121/10000, Batch 10/188, Loss: 0.4341\n",
      "Epoch 121/10000, Batch 20/188, Loss: 0.3027\n",
      "Epoch 121/10000, Batch 30/188, Loss: 0.4242\n",
      "Epoch 121/10000, Batch 40/188, Loss: 0.5542\n",
      "Epoch 121/10000, Batch 50/188, Loss: 0.4489\n",
      "Epoch 121/10000, Batch 60/188, Loss: 0.3658\n",
      "Epoch 121/10000, Batch 70/188, Loss: 0.2517\n",
      "Epoch 121/10000, Batch 80/188, Loss: 0.7036\n",
      "Epoch 121/10000, Batch 90/188, Loss: 0.3283\n",
      "Epoch 121/10000, Batch 100/188, Loss: 0.4063\n",
      "Epoch 121/10000, Batch 110/188, Loss: 0.3558\n",
      "Epoch 121/10000, Batch 120/188, Loss: 0.3323\n",
      "Epoch 121/10000, Batch 130/188, Loss: 0.7019\n",
      "Epoch 121/10000, Batch 140/188, Loss: 0.3362\n",
      "Epoch 121/10000, Batch 150/188, Loss: 0.4514\n",
      "Epoch 121/10000, Batch 160/188, Loss: 0.4100\n",
      "Epoch 121/10000, Batch 170/188, Loss: 0.4082\n",
      "Epoch 121/10000, Batch 180/188, Loss: 0.7655\n",
      "Epoch 121 Kết thúc - Mất mát Huấn luyện: 0.4404, IoU Huấn luyện: 0.5440, F1-Score Huấn luyện: 0.6983\n",
      "Mất mát Xác thực: 0.5390, IoU Xác thực: 0.3833, F1-Score Xác thực: 0.5234\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 24/30\n",
      "Epoch 122/10000 Bắt đầu...\n",
      "Epoch 122/10000, Batch 10/188, Loss: 0.3182\n",
      "Epoch 122/10000, Batch 20/188, Loss: 0.9309\n",
      "Epoch 122/10000, Batch 30/188, Loss: 0.2098\n",
      "Epoch 122/10000, Batch 40/188, Loss: 0.7264\n",
      "Epoch 122/10000, Batch 50/188, Loss: 0.3157\n",
      "Epoch 122/10000, Batch 60/188, Loss: 0.4837\n",
      "Epoch 122/10000, Batch 70/188, Loss: 0.3753\n",
      "Epoch 122/10000, Batch 80/188, Loss: 0.5004\n",
      "Epoch 122/10000, Batch 90/188, Loss: 0.3619\n",
      "Epoch 122/10000, Batch 100/188, Loss: 0.3239\n",
      "Epoch 122/10000, Batch 110/188, Loss: 0.5471\n",
      "Epoch 122/10000, Batch 120/188, Loss: 0.7149\n",
      "Epoch 122/10000, Batch 130/188, Loss: 0.4645\n",
      "Epoch 122/10000, Batch 140/188, Loss: 0.4624\n",
      "Epoch 122/10000, Batch 150/188, Loss: 0.5205\n",
      "Epoch 122/10000, Batch 160/188, Loss: 0.2902\n",
      "Epoch 122/10000, Batch 170/188, Loss: 0.6780\n",
      "Epoch 122/10000, Batch 180/188, Loss: 0.4349\n",
      "Epoch 122 Kết thúc - Mất mát Huấn luyện: 0.4431, IoU Huấn luyện: 0.5432, F1-Score Huấn luyện: 0.6972\n",
      "Mất mát Xác thực: 0.5390, IoU Xác thực: 0.3856, F1-Score Xác thực: 0.5254\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 25/30\n",
      "Epoch 123/10000 Bắt đầu...\n",
      "Epoch 123/10000, Batch 10/188, Loss: 0.2724\n",
      "Epoch 123/10000, Batch 20/188, Loss: 0.4338\n",
      "Epoch 123/10000, Batch 30/188, Loss: 0.3771\n",
      "Epoch 123/10000, Batch 40/188, Loss: 0.3499\n",
      "Epoch 123/10000, Batch 50/188, Loss: 0.4344\n",
      "Epoch 123/10000, Batch 60/188, Loss: 0.3169\n",
      "Epoch 123/10000, Batch 70/188, Loss: 0.6010\n",
      "Epoch 123/10000, Batch 80/188, Loss: 0.7153\n",
      "Epoch 123/10000, Batch 90/188, Loss: 0.4138\n",
      "Epoch 123/10000, Batch 100/188, Loss: 0.3359\n",
      "Epoch 123/10000, Batch 110/188, Loss: 0.5161\n",
      "Epoch 123/10000, Batch 120/188, Loss: 0.2590\n",
      "Epoch 123/10000, Batch 130/188, Loss: 0.4443\n",
      "Epoch 123/10000, Batch 140/188, Loss: 0.5040\n",
      "Epoch 123/10000, Batch 150/188, Loss: 0.5222\n",
      "Epoch 123/10000, Batch 160/188, Loss: 0.6152\n",
      "Epoch 123/10000, Batch 170/188, Loss: 0.3441\n",
      "Epoch 123/10000, Batch 180/188, Loss: 0.3071\n",
      "Epoch 123 Kết thúc - Mất mát Huấn luyện: 0.4392, IoU Huấn luyện: 0.5405, F1-Score Huấn luyện: 0.6939\n",
      "Mất mát Xác thực: 0.5410, IoU Xác thực: 0.3834, F1-Score Xác thực: 0.5236\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 26/30\n",
      "Epoch 124/10000 Bắt đầu...\n",
      "Epoch 124/10000, Batch 10/188, Loss: 0.3877\n",
      "Epoch 124/10000, Batch 20/188, Loss: 0.3925\n",
      "Epoch 124/10000, Batch 30/188, Loss: 0.5668\n",
      "Epoch 124/10000, Batch 40/188, Loss: 0.5525\n",
      "Epoch 124/10000, Batch 50/188, Loss: 0.4575\n",
      "Epoch 124/10000, Batch 60/188, Loss: 0.2902\n",
      "Epoch 124/10000, Batch 70/188, Loss: 0.4202\n",
      "Epoch 124/10000, Batch 80/188, Loss: 0.3420\n",
      "Epoch 124/10000, Batch 90/188, Loss: 0.3685\n",
      "Epoch 124/10000, Batch 100/188, Loss: 0.4349\n",
      "Epoch 124/10000, Batch 110/188, Loss: 0.4897\n",
      "Epoch 124/10000, Batch 120/188, Loss: 0.3582\n",
      "Epoch 124/10000, Batch 130/188, Loss: 0.3698\n",
      "Epoch 124/10000, Batch 140/188, Loss: 0.4927\n",
      "Epoch 124/10000, Batch 150/188, Loss: 0.3683\n",
      "Epoch 124/10000, Batch 160/188, Loss: 0.6326\n",
      "Epoch 124/10000, Batch 170/188, Loss: 0.4158\n",
      "Epoch 124/10000, Batch 180/188, Loss: 0.7977\n",
      "Epoch 124 Kết thúc - Mất mát Huấn luyện: 0.4378, IoU Huấn luyện: 0.5371, F1-Score Huấn luyện: 0.6915\n",
      "Mất mát Xác thực: 0.5337, IoU Xác thực: 0.3813, F1-Score Xác thực: 0.5221\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 27/30\n",
      "Epoch 125/10000 Bắt đầu...\n",
      "Epoch 125/10000, Batch 10/188, Loss: 0.3033\n",
      "Epoch 125/10000, Batch 20/188, Loss: 0.6113\n",
      "Epoch 125/10000, Batch 30/188, Loss: 0.3658\n",
      "Epoch 125/10000, Batch 40/188, Loss: 0.2388\n",
      "Epoch 125/10000, Batch 50/188, Loss: 0.3892\n",
      "Epoch 125/10000, Batch 60/188, Loss: 0.2145\n",
      "Epoch 125/10000, Batch 70/188, Loss: 0.4735\n",
      "Epoch 125/10000, Batch 80/188, Loss: 0.3012\n",
      "Epoch 125/10000, Batch 90/188, Loss: 0.3377\n",
      "Epoch 125/10000, Batch 100/188, Loss: 0.2806\n",
      "Epoch 125/10000, Batch 110/188, Loss: 0.3948\n",
      "Epoch 125/10000, Batch 120/188, Loss: 0.2089\n",
      "Epoch 125/10000, Batch 130/188, Loss: 0.1939\n",
      "Epoch 125/10000, Batch 140/188, Loss: 0.3552\n",
      "Epoch 125/10000, Batch 150/188, Loss: 0.3227\n",
      "Epoch 125/10000, Batch 160/188, Loss: 0.6519\n",
      "Epoch 125/10000, Batch 170/188, Loss: 0.2040\n",
      "Epoch 125/10000, Batch 180/188, Loss: 0.3840\n",
      "Epoch 125 Kết thúc - Mất mát Huấn luyện: 0.4371, IoU Huấn luyện: 0.5413, F1-Score Huấn luyện: 0.6954\n",
      "Mất mát Xác thực: 0.5318, IoU Xác thực: 0.3855, F1-Score Xác thực: 0.5258\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 28/30\n",
      "Epoch 126/10000 Bắt đầu...\n",
      "Epoch 126/10000, Batch 10/188, Loss: 0.4786\n",
      "Epoch 126/10000, Batch 20/188, Loss: 0.2885\n",
      "Epoch 126/10000, Batch 30/188, Loss: 0.2870\n",
      "Epoch 126/10000, Batch 40/188, Loss: 0.2670\n",
      "Epoch 126/10000, Batch 50/188, Loss: 0.4463\n",
      "Epoch 126/10000, Batch 60/188, Loss: 0.5658\n",
      "Epoch 126/10000, Batch 70/188, Loss: 0.4069\n",
      "Epoch 126/10000, Batch 80/188, Loss: 0.6160\n",
      "Epoch 126/10000, Batch 90/188, Loss: 0.5971\n",
      "Epoch 126/10000, Batch 100/188, Loss: 0.4742\n",
      "Epoch 126/10000, Batch 110/188, Loss: 0.7757\n",
      "Epoch 126/10000, Batch 120/188, Loss: 0.3578\n",
      "Epoch 126/10000, Batch 130/188, Loss: 0.6513\n",
      "Epoch 126/10000, Batch 140/188, Loss: 0.3276\n",
      "Epoch 126/10000, Batch 150/188, Loss: 0.3843\n",
      "Epoch 126/10000, Batch 160/188, Loss: 0.7182\n",
      "Epoch 126/10000, Batch 170/188, Loss: 0.2997\n",
      "Epoch 126/10000, Batch 180/188, Loss: 0.4106\n",
      "Epoch 126 Kết thúc - Mất mát Huấn luyện: 0.4354, IoU Huấn luyện: 0.5385, F1-Score Huấn luyện: 0.6929\n",
      "Mất mát Xác thực: 0.5297, IoU Xác thực: 0.3845, F1-Score Xác thực: 0.5246\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 29/30\n",
      "Epoch 127/10000 Bắt đầu...\n",
      "Epoch 127/10000, Batch 10/188, Loss: 0.4385\n",
      "Epoch 127/10000, Batch 20/188, Loss: 0.3365\n",
      "Epoch 127/10000, Batch 30/188, Loss: 0.2270\n",
      "Epoch 127/10000, Batch 40/188, Loss: 0.2551\n",
      "Epoch 127/10000, Batch 50/188, Loss: 0.2986\n",
      "Epoch 127/10000, Batch 60/188, Loss: 0.3823\n",
      "Epoch 127/10000, Batch 70/188, Loss: 0.5581\n",
      "Epoch 127/10000, Batch 80/188, Loss: 0.3801\n",
      "Epoch 127/10000, Batch 90/188, Loss: 0.4110\n",
      "Epoch 127/10000, Batch 100/188, Loss: 0.4350\n",
      "Epoch 127/10000, Batch 110/188, Loss: 0.4867\n",
      "Epoch 127/10000, Batch 120/188, Loss: 0.7058\n",
      "Epoch 127/10000, Batch 130/188, Loss: 0.3052\n",
      "Epoch 127/10000, Batch 140/188, Loss: 0.4005\n",
      "Epoch 127/10000, Batch 150/188, Loss: 0.5124\n",
      "Epoch 127/10000, Batch 160/188, Loss: 0.3664\n",
      "Epoch 127/10000, Batch 170/188, Loss: 0.4181\n",
      "Epoch 127/10000, Batch 180/188, Loss: 0.2704\n",
      "Epoch 127 Kết thúc - Mất mát Huấn luyện: 0.4343, IoU Huấn luyện: 0.5339, F1-Score Huấn luyện: 0.6889\n",
      "Mất mát Xác thực: 0.5286, IoU Xác thực: 0.3828, F1-Score Xác thực: 0.5230\n",
      "Mất mát xác thực không cải thiện. Sự kiên nhẫn: 30/30\n",
      "Dừng sớm!\n",
      "\n",
      "Quá trình huấn luyện đã hoàn tất.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # Import F for functional operations\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import SwinConfig, SwinModel\n",
    "import torch.optim.lr_scheduler as lr_scheduler # Import scheduler\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# --- Cài đặt tham số cố định ---\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 8\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Sử dụng thiết bị: {DEVICE}\")\n",
    "\n",
    "# Thay đổi đường dẫn thư mục tùy theo máy của bạn\n",
    "train_img_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\train\\image'\n",
    "train_mask_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\train\\label'\n",
    "val_img_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\val\\image'\n",
    "val_mask_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\val\\label'\n",
    "\n",
    "# --- Thu thập đường dẫn tệp ảnh và mask ---\n",
    "train_img_paths = sorted([os.path.join(train_img_dir, f) for f in os.listdir(train_img_dir)])\n",
    "train_mask_paths = sorted([os.path.join(train_mask_dir, f) for f in os.listdir(train_mask_dir)])\n",
    "val_img_paths = sorted([os.path.join(val_img_dir, f) for f in os.listdir(val_img_dir)])\n",
    "val_mask_paths = sorted([os.path.join(val_mask_dir, f) for f in os.listdir(val_mask_dir)])\n",
    "\n",
    "class CrackDetectionDataset(Dataset):\n",
    "    def __init__(self, image_filenames, mask_filenames, augment=False):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.mask_filenames = mask_filenames\n",
    "        self.augment = augment\n",
    "\n",
    "        if len(self.image_filenames) != len(self.mask_filenames):\n",
    "            raise ValueError(\"Số lượng tệp ảnh và tệp mask không khớp.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    # Hàm thực hiện resize ảnh và mask về kích thước mong muốn (ban đầu)\n",
    "    def resize_image_and_mask(self, img, mask, target_size=IMG_SIZE):\n",
    "        img = cv2.resize(img, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
    "        mask = cv2.resize(mask, (target_size, target_size), interpolation=cv2.INTER_NEAREST)\n",
    "        return img, mask\n",
    "\n",
    "    # Hàm thực hiện cắt ngẫu nhiên cho ảnh và mask\n",
    "    def random_crop_image_and_mask(self, img, mask, crop_size=IMG_SIZE, min_scale=0.7):\n",
    "        h, w = img.shape[:2]\n",
    "        # Tính toán kích thước crop ngẫu nhiên, không nhỏ hơn min_scale của IMG_SIZE\n",
    "        # Đảm bảo kích thước cắt không lớn hơn kích thước hiện tại của ảnh\n",
    "        current_min_dim = min(h, w)\n",
    "        crop_h = crop_w = int(random.uniform(min_scale, 1.0) * crop_size)\n",
    "        \n",
    "        # Đảm bảo crop_h, crop_w không lớn hơn kích thước hiện tại của ảnh\n",
    "        crop_h = min(crop_h, h)\n",
    "        crop_w = min(crop_w, w)\n",
    "\n",
    "        if h == crop_h and w == crop_w: # Không cần cắt nếu kích thước đã khớp\n",
    "            return img, mask\n",
    "\n",
    "        # Chọn điểm bắt đầu ngẫu nhiên\n",
    "        start_x = random.randint(0, w - crop_w)\n",
    "        start_y = random.randint(0, h - crop_h)\n",
    "\n",
    "        cropped_img = img[start_y:start_y + crop_h, start_x:start_x + crop_w]\n",
    "        cropped_mask = mask[start_y:start_y + crop_h, start_x:start_x + crop_w]\n",
    "        \n",
    "        # Resize lại về kích thước IMG_SIZE sau khi cắt\n",
    "        cropped_img = cv2.resize(cropped_img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "        cropped_mask = cv2.resize(cropped_mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        return cropped_img, cropped_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.image_filenames[idx])\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Không thể đọc tệp ảnh: {self.image_filenames[idx]}\")\n",
    "\n",
    "        mask = cv2.imread(self.mask_filenames[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise ValueError(f\"Không thể đọc tệp mask: {self.mask_filenames[idx]}\")\n",
    "\n",
    "        # Chuyển đổi ảnh sang RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Áp dụng Augmentation\n",
    "        if self.augment:\n",
    "            # Random Crop đầu tiên để có các phần khác nhau của ảnh\n",
    "            # Đảm bảo ảnh đủ lớn để crop, nếu không, resize trước\n",
    "            if img.shape[0] < IMG_SIZE or img.shape[1] < IMG_SIZE:\n",
    "                img, mask = self.resize_image_and_mask(img, mask, target_size=IMG_SIZE)\n",
    "            \n",
    "            # Chỉ thực hiện crop nếu ảnh đủ lớn sau resize hoặc nếu ban đầu đã lớn\n",
    "            # Với `min_scale=0.7`, chúng ta sẽ cắt một vùng có kích thước từ 0.7*IMG_SIZE đến IMG_SIZE\n",
    "            img, mask = self.random_crop_image_and_mask(img, mask, crop_size=IMG_SIZE, min_scale=0.7)\n",
    "\n",
    "            # Lật Ngang\n",
    "            if random.random() < 0.5:\n",
    "                img = cv2.flip(img, 1)\n",
    "                mask = cv2.flip(mask, 1)\n",
    "            # Lật Dọc\n",
    "            if random.random() < 0.5:\n",
    "                img = cv2.flip(img, 0)\n",
    "                mask = cv2.flip(mask, 0)\n",
    "        else:\n",
    "            # Nếu không augment, chỉ resize về đúng kích thước IMG_SIZE\n",
    "            img, mask = self.resize_image_and_mask(img, mask, target_size=IMG_SIZE)\n",
    "\n",
    "        # Chuẩn hóa ảnh và mask về [0, 1]\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        mask = mask.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Đảm bảo mask có chiều kênh (1, H, W)\n",
    "        if mask.ndim == 2: # Nếu mask vẫn là (H, W)\n",
    "            mask = np.expand_dims(mask, axis=0) # Thêm chiều kênh ở vị trí 0 -> (1, H, W)\n",
    "        elif mask.ndim == 3 and mask.shape[0] != 1: # Nếu là (C, H, W) nhưng C không phải 1\n",
    "            if mask.shape[0] == 3: # Nếu là 3 kênh (ví dụ, mask gốc là RGB)\n",
    "                mask = mask[0:1, :, :] # Lấy kênh đầu tiên\n",
    "            else:\n",
    "                raise ValueError(f\"Mask có hình dạng không mong muốn {mask.shape} tại chỉ số {idx} sau khi biến đổi. Expected channel dim 1.\")\n",
    "        \n",
    "        # Chuyển đổi từ NumPy array sang PyTorch tensor\n",
    "        img_tensor = torch.from_numpy(img).permute(2, 0, 1) # Ảnh: (H, W, C) -> (C, H, W)\n",
    "        mask_tensor = torch.from_numpy(mask) # Mask đã ở (1, H, W)\n",
    "\n",
    "        return img_tensor, mask_tensor\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # Ưu tiên sử dụng nn.Upsample để tránh checkerboard artifacts\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), # Lên gấp đôi kích thước không gian\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.conv_block = ConvBlock(out_channels + skip_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_features=None):\n",
    "        x = self.upsample(x) \n",
    "        \n",
    "        if skip_features is not None:\n",
    "            # Bước 2: Đảm bảo kích thước không gian của skip_features khớp với x.\n",
    "            # Đây là điểm sửa lỗi: nội suy skip_features để nó khớp với kích thước đã được upsample của x.\n",
    "            if x.shape[2:] != skip_features.shape[2:]:\n",
    "                skip_features = F.interpolate(skip_features, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "            \n",
    "            # Bước 3: Nối (concatenate) x và skip_features theo chiều kênh\n",
    "            x = torch.cat([x, skip_features], dim=1)\n",
    "        \n",
    "        # Bước 4: Chạy qua khối convolution để xử lý các đặc trưng đã nối\n",
    "        x = self.conv_block(x)\n",
    "        return x\n",
    "    \n",
    "class SwinUNet(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.IMG_SIZE = IMG_SIZE\n",
    "\n",
    "        config = SwinConfig(image_size=self.IMG_SIZE, num_channels=input_channels,\n",
    "                            patch_size=4, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                            window_size=7, mlp_ratio=4., qkv_bias=True, hidden_dropout_prob=0.0,\n",
    "                            attention_probs_dropout_prob=0.0, drop_path_rate=0.1,\n",
    "                            hidden_act=\"gelu\", use_absolute_embeddings=False,\n",
    "                            patch_norm=True, initializer_range=0.02, layer_norm_eps=1e-05,\n",
    "                            out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"])\n",
    "        self.swin = SwinModel(config)\n",
    "\n",
    "        # Bottleneck: input từ hidden_state[3] (768 kênh), vì hidden_state[4] là sau LayerNorm và có thể không cần thiết\n",
    "        self.bottleneck = ConvBlock(config.embed_dim * 8, config.embed_dim * 8) # (768, 768)\n",
    "\n",
    "        # Decoder 4: in_channels từ bottleneck (768), skip từ hidden_state[3] (768 kênh), ra 384 kênh\n",
    "        self.decoder4 = DecoderBlock(in_channels=config.embed_dim * 8, skip_channels=config.embed_dim * 8, out_channels=config.embed_dim * 4) # 768, 768, 384\n",
    "        \n",
    "        # Decoder 3: in_channels từ decoder4 (384), skip từ hidden_state[2] (384 kênh), ra 192 kênh\n",
    "        self.decoder3 = DecoderBlock(in_channels=config.embed_dim * 4, skip_channels=config.embed_dim * 4, out_channels=config.embed_dim * 2) # 384, 384, 192\n",
    "        \n",
    "        # Decoder 2: in_channels từ decoder3 (192), upsample to 96, skip từ hidden_state[1] (192 kênh), ra 96 kênh\n",
    "        self.decoder2 = DecoderBlock(in_channels=config.embed_dim * 2, skip_channels=config.embed_dim * 2, out_channels=config.embed_dim * 1) # 192, 192, 96\n",
    "        \n",
    "        # Decoder 1: in_channels từ decoder2 (96), upsample to 48, skip từ hidden_state[0] (96 kênh), ra 48 kênh\n",
    "        self.decoder1 = DecoderBlock(in_channels=config.embed_dim * 1, skip_channels=config.embed_dim * 1, out_channels=config.embed_dim // 2) # 96, 96, 48\n",
    "        \n",
    "        self.final_upsample = DecoderBlock(in_channels=config.embed_dim // 2, skip_channels=0, out_channels=config.embed_dim // 4) # 48, 0, 24\n",
    "        self.final_conv = nn.Conv2d(config.embed_dim // 4, num_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.swin(pixel_values=x, output_hidden_states=True)\n",
    "        encoder_features = []\n",
    "\n",
    "        hs_skip_res_H4 = outputs.hidden_states[0] \n",
    "        b, n, c = hs_skip_res_H4.shape\n",
    "        s = int(np.sqrt(n))\n",
    "        encoder_features.append(hs_skip_res_H4.permute(0, 2, 1).reshape(b, c, s, s)) # index 0\n",
    "\n",
    "        # Skip connection cho decoder2 (H/8): hidden_state[1]\n",
    "        hs_skip_res_H8 = outputs.hidden_states[1] \n",
    "        b, n, c = hs_skip_res_H8.shape\n",
    "        s = int(np.sqrt(n))\n",
    "        encoder_features.append(hs_skip_res_H8.permute(0, 2, 1).reshape(b, c, s, s)) # index 1\n",
    "\n",
    "        # Skip connection cho decoder3 (H/16): hidden_state[2]\n",
    "        hs_skip_res_H16 = outputs.hidden_states[2] \n",
    "        b, n, c = hs_skip_res_H16.shape\n",
    "        s = int(np.sqrt(n))\n",
    "        encoder_features.append(hs_skip_res_H16.permute(0, 2, 1).reshape(b, c, s, s)) # index 2\n",
    "\n",
    "        # Skip connection cho decoder4 (H/32): hidden_state[3]\n",
    "        hs_skip_res_H32 = outputs.hidden_states[3] \n",
    "        b, n, c = hs_skip_res_H32.shape\n",
    "        s = int(np.sqrt(n))\n",
    "        encoder_features.append(hs_skip_res_H32.permute(0, 2, 1).reshape(b, c, s, s)) # index 3\n",
    "\n",
    "        x_bottleneck = outputs.hidden_states[3] \n",
    "        b, n, c = x_bottleneck.shape\n",
    "        s = int(np.sqrt(n))\n",
    "        x_bottleneck = x_bottleneck.permute(0, 2, 1).reshape(b, c, s, s)\n",
    "        \n",
    "        x = self.bottleneck(x_bottleneck)\n",
    "\n",
    "        # decoder4: x_in từ bottleneck (768), upsample to 384, skip từ encoder_features[3] (768 kênh)\n",
    "        x = self.decoder4(x, encoder_features[3])\n",
    "        # x_out của decoder4 là 384 kênh, 16x16\n",
    "\n",
    "        # decoder3: x_in từ decoder4 (384), upsample to 192, skip từ encoder_features[2] (384 kênh)\n",
    "        x = self.decoder3(x, encoder_features[2])\n",
    "        # x_out của decoder3 là 192 kênh, 32x32\n",
    "\n",
    "        # decoder2: x_in từ decoder3 (192), upsample to 96, skip từ encoder_features[1] (192 kênh)\n",
    "        x = self.decoder2(x, encoder_features[1])\n",
    "        # x_out của decoder2 là 96 kênh, 64x64\n",
    "        \n",
    "        # decoder1: x_in từ decoder2 (96), upsample to 48, skip từ encoder_features[0] (96 kênh)\n",
    "        x = self.decoder1(x, encoder_features[0])\n",
    "        \n",
    "        x = self.final_upsample(x)\n",
    "        outputs = self.final_conv(x)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum() \n",
    "        dice = (2.*intersection + self.smooth)/(inputs.sum() + targets.sum() + self.smooth) \n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5, pos_weight=None, smooth=1e-6):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        self.dice = DiceLoss(smooth=smooth)\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = self.bce(inputs, targets)\n",
    "\n",
    "        sigmoid_inputs = torch.sigmoid(inputs)\n",
    "        dice_loss = self.dice(sigmoid_inputs, targets)\n",
    "        \n",
    "        return self.bce_weight * bce_loss + self.dice_weight * dice_loss\n",
    "\n",
    "def calculate_metrics(predicted_masks, true_masks, smooth=1e-6):\n",
    "    \n",
    "    intersection = (predicted_masks * true_masks).sum()\n",
    "    union = (predicted_masks + true_masks).sum() - intersection\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    dice = (2. * intersection + smooth) / ((predicted_masks.sum() + true_masks.sum()) + smooth)\n",
    "    f1_score = dice \n",
    "\n",
    "    return iou.item(), f1_score.item()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, callbacks_config, start_epoch=0, best_val_loss_so_far=float('inf')):\n",
    "    best_val_loss = best_val_loss_so_far \n",
    "    patience_counter = 0\n",
    "    model_checkpoint_path = callbacks_config.get('checkpoint_path', 'swin_unet_best_pytorch.pth')\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_iou = 0.0\n",
    "        running_f1 = 0.0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Bắt đầu...\")\n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # swin_outputs_for_debug = model.swin(pixel_values=images, output_hidden_states=True) # Dòng này không cần thiết trong quá trình huấn luyện thông thường\n",
    "\n",
    "            loss = criterion(outputs, masks) # criterion sẽ xử lý sigmoid bên trong cho Dice loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Để tính metrics, cần áp dụng sigmoid cho outputs và sau đó ngưỡng\n",
    "            predicted_masks = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            batch_iou, batch_f1 = calculate_metrics(predicted_masks, masks)\n",
    "            running_iou += batch_iou * images.size(0)\n",
    "            running_f1 += batch_f1 * images.size(0)\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_iou = running_iou / len(train_loader.dataset)\n",
    "        epoch_f1 = running_f1 / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1} Kết thúc - Mất mát Huấn luyện: {epoch_loss:.4f}, IoU Huấn luyện: {epoch_iou:.4f}, F1-Score Huấn luyện: {epoch_f1:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_iou = 0.0\n",
    "        val_f1 = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(DEVICE)\n",
    "                masks = masks.to(DEVICE)\n",
    "\n",
    "                outputs = model(images) # outputs là logits\n",
    "                loss = criterion(outputs, masks) # criterion sẽ xử lý sigmoid bên trong\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                # Để tính metrics, cần áp dụng sigmoid cho outputs và sau đó ngưỡng\n",
    "                predicted_masks = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                \n",
    "                batch_iou, batch_f1 = calculate_metrics(predicted_masks, masks)\n",
    "                val_iou += batch_iou * images.size(0)\n",
    "                val_f1 += batch_f1 * images.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_iou /= len(val_loader.dataset)\n",
    "        val_f1 /= len(val_loader.dataset)\n",
    "        print(f\"Mất mát Xác thực: {val_loss:.4f}, IoU Xác thực: {val_iou:.4f}, F1-Score Xác thực: {val_f1:.4f}\")\n",
    "\n",
    "        # CẬP NHẬT SCHEDULER DỰA TRÊN VAL_LOSS\n",
    "        scheduler.step(val_loss) # Quan trọng: truyền val_loss vào scheduler\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            print(f\"Mất mát xác thực tốt nhất được cập nhật: {best_val_loss:.4f}. Lưu mô hình và trạng thái...\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'scheduler_state_dict': scheduler.state_dict() # Lưu trạng thái scheduler\n",
    "            }, model_checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Mất mát xác thực không cải thiện. Sự kiên nhẫn: {patience_counter}/{callbacks_config['patience']}\")\n",
    "            if patience_counter >= callbacks_config['patience']:\n",
    "                print(\"Dừng sớm!\")\n",
    "                break\n",
    "\n",
    "# --- Chuẩn bị dữ liệu ---\n",
    "train_dataset = CrackDetectionDataset(train_img_paths, train_mask_paths, augment=True)\n",
    "val_dataset = CrackDetectionDataset(val_img_paths, val_mask_paths, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- Khởi tạo mô hình, optimizer, criterion ---\n",
    "model = SwinUNet(input_channels=3, num_classes=1).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "\n",
    "# Khởi tạo Scheduler (sau optimizer)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',         # Giảm LR khi val_loss ngừng giảm\n",
    "    factor=0.5,         # Hệ số giảm LR (ví dụ: LR mới = LR cũ * 0.5)\n",
    "    patience=10,        # Số epoch chờ đợi trước khi giảm LR\n",
    "    threshold=0.0001,   # Ngưỡng cải thiện tối thiểu\n",
    "    threshold_mode='rel',\n",
    "    cooldown=0,\n",
    "    min_lr=1e-7,        # Tốc độ học tối thiểu\n",
    "    verbose=True        # In thông báo khi LR thay đổi\n",
    ")\n",
    "\n",
    "my_pos_weight = 11\n",
    "pos_weight_tensor = torch.tensor(my_pos_weight, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "criterion = CombinedLoss(bce_weight=0.5, dice_weight=0.5, pos_weight=pos_weight_tensor)\n",
    "\n",
    "print(model)\n",
    "\n",
    "callbacks_config = {\n",
    "    'patience': 30,\n",
    "    'checkpoint_path': 'swin_unet_best_pytorchDiceloss.pth'\n",
    "}\n",
    "\n",
    "start_epoch = 0\n",
    "best_val_loss_so_far = float('inf')\n",
    "checkpoint_path = callbacks_config['checkpoint_path']\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Phát hiện checkpoint tại {checkpoint_path}. Đang tải để tiếp tục huấn luyện...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    # Tải trạng thái scheduler\n",
    "    if 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    start_epoch = checkpoint['epoch'] + 1 \n",
    "    best_val_loss_so_far = checkpoint['best_val_loss']\n",
    "    \n",
    "    print(f\"Đã tải checkpoint từ Epoch {start_epoch-1}. Tiếp tục huấn luyện từ Epoch {start_epoch}.\")\n",
    "    print(f\"Mất mát xác thực tốt nhất trước đó: {best_val_loss_so_far:.4f}\")\n",
    "else:\n",
    "    print(\"Không tìm thấy checkpoint. Bắt đầu huấn luyện từ đầu (Epoch 0).\")\n",
    "\n",
    "print(\"\\nBắt đầu huấn luyện mô hình Swin-Unet...\")\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, scheduler,\n",
    "            num_epochs=10000, callbacks_config=callbacks_config,\n",
    "            start_epoch=start_epoch, best_val_loss_so_far=best_val_loss_so_far)\n",
    "\n",
    "print(\"\\nQuá trình huấn luyện đã hoàn tất.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
