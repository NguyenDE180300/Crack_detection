{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5890f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import SwinModel\n",
    "from transformers.models.swin.modeling_swin import SwinLayer\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.swin.modeling_swin import SwinPatchEmbeddings, SwinPatchMerging, SwinLayer\n",
    "from transformers import SwinConfig\n",
    "from typing import Tuple, Optional\n",
    "# --- Cài đặt tham số cố định ---\n",
    "IMG_SIZE = 320 \n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "SEED = 42\n",
    "BATCH_SIZE = 2\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Đường dẫn dữ liệu (Thay đổi nếu cần) ---\n",
    "train_img_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\train\\image'\n",
    "train_mask_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\train\\label'\n",
    "val_img_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\val\\image'\n",
    "val_mask_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\val\\label'\n",
    "\n",
    "# --- Thu thập đường dẫn tệp ảnh và mask ---\n",
    "train_img_paths = sorted([os.path.join(train_img_dir, f) for f in os.listdir(train_img_dir)])\n",
    "train_mask_paths = sorted([os.path.join(train_mask_dir, f) for f in os.listdir(train_mask_dir)])\n",
    "val_img_paths = sorted([os.path.join(val_img_dir, f) for f in os.listdir(val_img_dir)])\n",
    "val_mask_paths = sorted([os.path.join(val_mask_dir, f) for f in os.listdir(val_mask_dir)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e076cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrackDetectionDataset(Dataset):\n",
    "    def __init__(self, image_filenames, mask_filenames, transform=None):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.mask_filenames = mask_filenames\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.image_filenames[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = cv2.imread(self.mask_filenames[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = np.where(mask > 0, 255.0, 0.0).astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        mask = mask / 255.0\n",
    "        mask = mask.unsqueeze(0)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d425ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExpanding(nn.Module):\n",
    "    def __init__(self, input_resolution: Tuple[int, int], dim: int, return_vector: bool = True):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.expand = nn.Linear(dim, 2 * dim, bias=False)\n",
    "        self.norm = nn.LayerNorm(dim // 2)\n",
    "        self.return_vector = return_vector\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        H, W = self.input_resolution\n",
    "        x = x.view(-1, H, W, self.dim)\n",
    "        x = self.expand(x)\n",
    "        B, H, W, C = x.shape\n",
    "        x = x.view(B, H, W, 2, 2, C // 4)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H * 2, W * 2, C // 4)\n",
    "        x = self.norm(x)\n",
    "        if self.return_vector:\n",
    "            x = x.view(B, -1, C // 4)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FinalPatchExpanding(nn.Module):\n",
    "    def __init__(self, dim: int, scale_factor: int = 4):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.output_dim = dim // scale_factor\n",
    "\n",
    "        expand_dim = self.output_dim * (scale_factor**2)\n",
    "        self.expand = nn.Linear(dim, expand_dim, bias=False)\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, L, _ = x.shape\n",
    "        H = W = int(np.sqrt(L))\n",
    "        x_expanded = self.expand(x)\n",
    "\n",
    "        x_reshaped = x_expanded.view(B, H, W, self.scale_factor, self.scale_factor, self.output_dim)\n",
    "\n",
    "        x_permuted = x_reshaped.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H * self.scale_factor, W * self.scale_factor, self.output_dim)\n",
    "\n",
    "        x_normed = self.norm(x_permuted)\n",
    "\n",
    "        output = x_normed.view(B, -1, self.output_dim)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class SkipAttention(nn.Module):\n",
    "    def __init__(self, query_dim: int, context_dim: Optional[int] = None, num_heads: int = 8, dim_head: int = 64):\n",
    "        super().__init__()\n",
    "        if context_dim is None:\n",
    "            context_dim = query_dim\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(embed_dim=query_dim, num_heads=num_heads, \n",
    "                                          kdim=context_dim, vdim=context_dim, \n",
    "                                          batch_first=True)\n",
    "        self.norm = nn.LayerNorm(query_dim)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, context: torch.Tensor) -> torch.Tensor:\n",
    "        attn_output, _ = self.attn(query=query, key=context, value=context)\n",
    "        return self.norm(query + attn_output)\n",
    "\n",
    "\n",
    "class ResidualSwinBlock(nn.Module):\n",
    "    def __init__(self, config: SwinConfig, dim: int, input_resolution: Tuple[int, int], num_layers: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            SwinLayer(\n",
    "                config=config, \n",
    "                dim=dim, \n",
    "                input_resolution=input_resolution, \n",
    "                num_heads=num_heads, \n",
    "                shift_size=0 if (i % 2 == 0) else config.window_size // 2\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, input_resolution: Tuple[int, int]) -> torch.Tensor:\n",
    "        residual = x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, input_resolution)[0]\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class iSwinUnet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1, img_size: int = 320, pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        config = SwinConfig(image_size=img_size, patch_size=4, window_size=7)\n",
    "        self.config = config\n",
    "        self.encoder_channels = [config.embed_dim * (2**i) for i in range(len(config.depths))]\n",
    "\n",
    "        self.patch_embed = SwinPatchEmbeddings(config)\n",
    "        self.pos_drop = nn.Dropout(p=config.hidden_dropout_prob)\n",
    "\n",
    "        self.encoder_stage1 = ResidualSwinBlock(config, self.encoder_channels[0], (img_size//4, img_size//4), config.depths[0], config.num_heads[0])\n",
    "        self.merge1 = SwinPatchMerging(input_resolution=(img_size//4, img_size//4), dim=self.encoder_channels[0])\n",
    "        \n",
    "        self.encoder_stage2 = ResidualSwinBlock(config, self.encoder_channels[1], (img_size//8, img_size//8), config.depths[1], config.num_heads[1])\n",
    "        self.merge2 = SwinPatchMerging(input_resolution=(img_size//8, img_size//8), dim=self.encoder_channels[1])\n",
    "\n",
    "        self.encoder_stage3 = ResidualSwinBlock(config, self.encoder_channels[2], (img_size//16, img_size//16), config.depths[2], config.num_heads[2])\n",
    "        self.merge3 = SwinPatchMerging(input_resolution=(img_size//16, img_size//16), dim=self.encoder_channels[2])\n",
    "        \n",
    "        self.bottleneck = ResidualSwinBlock(config, self.encoder_channels[3], (img_size//32, img_size//32), config.depths[3], config.num_heads[3])\n",
    "\n",
    "        self.decoder_expand3 = PatchExpanding(input_resolution=(img_size//32, img_size//32), dim=self.encoder_channels[3])\n",
    "        self.skip_attn3 = SkipAttention(query_dim=self.encoder_channels[2], context_dim=self.encoder_channels[2], num_heads=config.num_heads[2])\n",
    "        self.decoder_stage3 = ResidualSwinBlock(config, self.encoder_channels[2], (img_size//16, img_size//16), config.depths[2], config.num_heads[2])\n",
    "\n",
    "        self.decoder_expand2 = PatchExpanding(input_resolution=(img_size//16, img_size//16), dim=self.encoder_channels[2])\n",
    "        self.skip_attn2 = SkipAttention(query_dim=self.encoder_channels[1], context_dim=self.encoder_channels[1], num_heads=config.num_heads[1])\n",
    "        self.decoder_stage2 = ResidualSwinBlock(config, self.encoder_channels[1], (img_size//8, img_size//8), config.depths[1], config.num_heads[1])\n",
    "\n",
    "        self.decoder_expand1 = PatchExpanding(input_resolution=(img_size//8, img_size//8), dim=self.encoder_channels[1])\n",
    "        self.skip_attn1 = SkipAttention(query_dim=self.encoder_channels[0], context_dim=self.encoder_channels[0], num_heads=config.num_heads[0])\n",
    "        self.decoder_stage1 = ResidualSwinBlock(config, self.encoder_channels[0], (img_size//4, img_size//4), config.depths[0], config.num_heads[0])\n",
    "\n",
    "        self.final_expand = FinalPatchExpanding(dim=self.encoder_channels[0], scale_factor=4)\n",
    "        self.final_conv = nn.Conv2d(self.final_expand.output_dim, num_classes, kernel_size=1)\n",
    "\n",
    "        if pretrained:\n",
    "            self.load_pretrained_weights()\n",
    "\n",
    "    def load_pretrained_weights(self):\n",
    "            print(\"Loading pretrained weights for Swin-Tiny encoder...\")\n",
    "            try:\n",
    "                swin_original = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "                original_state_dict = swin_original.state_dict()\n",
    "\n",
    "                model_state_dict = self.state_dict()\n",
    "\n",
    "                prefix_mapping = {\n",
    "                    'embeddings.patch_embeddings': 'patch_embed',\n",
    "                    'encoder.layers.0.downsample': 'merge1',\n",
    "                    'encoder.layers.1.downsample': 'merge2',\n",
    "                    'encoder.layers.2.downsample': 'merge3',\n",
    "                }\n",
    "\n",
    "                stage_mapping = {\n",
    "                    'encoder.layers.0': 'encoder_stage1',\n",
    "                    'encoder.layers.1': 'encoder_stage2',\n",
    "                    'encoder.layers.2': 'encoder_stage3',\n",
    "                    'encoder.layers.3': 'bottleneck',\n",
    "                }\n",
    "\n",
    "                loaded_count = 0\n",
    "                \n",
    "                orig_pos_embed_key = 'embeddings.absolute_position_embedding'\n",
    "                if orig_pos_embed_key in original_state_dict:\n",
    "                    orig_pos_embed = original_state_dict[orig_pos_embed_key]\n",
    "                    target_pos_embed = model_state_dict[orig_pos_embed_key]\n",
    "                    if orig_pos_embed.shape != target_pos_embed.shape:\n",
    "                        print(f\"Interpolating absolute position embedding due to size mismatch...\")\n",
    "                        orig_pos_embed_spatial = orig_pos_embed[0, 1:, :].permute(1, 0)\n",
    "\n",
    "                        H_orig = W_orig = int((orig_pos_embed.shape[1] - 1) ** 0.5)\n",
    "                        orig_pos_embed_spatial = orig_pos_embed_spatial.view(1, -1, H_orig, W_orig)\n",
    "                        H_target = W_target = int((target_pos_embed.shape[1] - 1) ** 0.5)\n",
    "                        interp_pos_embed = nn.functional.interpolate(\n",
    "                            orig_pos_embed_spatial, size=(H_target, W_target), mode='bicubic', align_corners=False\n",
    "                        )\n",
    "                        interp_pos_embed = interp_pos_embed.view(1, -1, H_target * W_target).permute(0, 2, 1)\n",
    "                        # Thêm lại token [CLS]\n",
    "                        final_pos_embed = torch.cat((orig_pos_embed[0, 0:1, :], interp_pos_embed[0]), dim=0).unsqueeze(0)\n",
    "                        model_state_dict[orig_pos_embed_key] = final_pos_embed\n",
    "                        loaded_count += 1\n",
    "                    del original_state_dict[orig_pos_embed_key]\n",
    "\n",
    "                for key_orig, value in original_state_dict.items():\n",
    "                    new_key = None\n",
    "\n",
    "                    if 'pooler' in key_orig:\n",
    "                        continue\n",
    "\n",
    "                    for old_prefix, new_prefix in prefix_mapping.items():\n",
    "                        if key_orig.startswith(old_prefix):\n",
    "                            new_key = key_orig.replace(old_prefix, new_prefix)\n",
    "                            break\n",
    "\n",
    "                    if new_key is None:\n",
    "                        for old_stage_prefix, new_stage_prefix in stage_mapping.items():\n",
    "                            if key_orig.startswith(old_stage_prefix):\n",
    "                                remainder = key_orig[len(old_stage_prefix):]\n",
    "                                if remainder.startswith('.blocks'):\n",
    "                                    remainder = remainder.replace('.blocks', '', 1)\n",
    "                                    new_key = f\"{new_stage_prefix}.layers{remainder}\"\n",
    "                                break\n",
    "\n",
    "                    if new_key is None:\n",
    "                        if key_orig.startswith('embeddings.norm'):\n",
    "                            new_key = key_orig.replace('embeddings.norm', 'patch_embed.norm')\n",
    "                        elif key_orig.startswith('encoder.norm'):\n",
    "                            continue\n",
    "\n",
    "                    if new_key and new_key in model_state_dict and model_state_dict[new_key].shape == value.shape:\n",
    "                        model_state_dict[new_key] = value\n",
    "                        loaded_count += 1\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                self.load_state_dict(model_state_dict, strict=False)\n",
    "\n",
    "                expected_keys = len([k for k in swin_original.state_dict().keys() if 'pooler' not in k and 'encoder.norm' not in k])\n",
    "                print(f\"✅ Successfully loaded {loaded_count}/{expected_keys} tensors from pretrained Swin-Tiny.\")\n",
    "                print(\"   (Any unloaded tensors are expected to be from the decoder and final layers).\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ ERROR: Could not load pretrained weights. Error: {e}. Training from scratch.\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        s1_skip = self.patch_embed(x)[0]\n",
    "        s1_skip = self.pos_drop(s1_skip)\n",
    "        \n",
    "        s1_out = self.encoder_stage1(s1_skip, (self.img_size//4, self.img_size//4))\n",
    "        s2_in = self.merge1(s1_out, (self.img_size//4, self.img_size//4))\n",
    "        \n",
    "        s2_out = self.encoder_stage2(s2_in, (self.img_size//8, self.img_size//8))\n",
    "        s3_in = self.merge2(s2_out, (self.img_size//8, self.img_size//8))\n",
    "\n",
    "        s3_out = self.encoder_stage3(s3_in, (self.img_size//16, self.img_size//16))\n",
    "        b_in = self.merge3(s3_out, (self.img_size//16, self.img_size//16))\n",
    "        \n",
    "        b_out = self.bottleneck(b_in, (self.img_size//32, self.img_size//32))\n",
    "\n",
    "        d3_in = self.decoder_expand3(b_out)\n",
    "        d3_in = self.skip_attn3(query=d3_in, context=s3_out)\n",
    "        d3_out = self.decoder_stage3(d3_in, (self.img_size//16, self.img_size//16))\n",
    "\n",
    "        d2_in = self.decoder_expand2(d3_out)\n",
    "        d2_in = self.skip_attn2(query=d2_in, context=s2_out)\n",
    "        d2_out = self.decoder_stage2(d2_in, (self.img_size//8, self.img_size//8))\n",
    "\n",
    "        d1_in = self.decoder_expand1(d2_out)\n",
    "        d1_in = self.skip_attn1(query=d1_in, context=s1_out)\n",
    "        d1_out = self.decoder_stage1(d1_in, (self.img_size//4, self.img_size//4))\n",
    "        \n",
    "        final_features = self.final_expand(d1_out)\n",
    "        \n",
    "        B, L, C = final_features.shape\n",
    "        H = W = int(np.sqrt(L))\n",
    "        final_features = final_features.permute(0, 2, 1).contiguous().view(B, C, H, W)\n",
    "        \n",
    "        output = self.final_conv(final_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab28d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(IoULoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs là logits từ model, cần đưa qua sigmoid để có xác suất (0, 1)\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "        \n",
    "        # Làm phẳng inputs và targets để tính toán dễ dàng\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Tính toán intersection (phần giao) và union (phần hợp)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        total = (inputs + targets).sum()\n",
    "        union = total - intersection\n",
    "        \n",
    "        # Tính IoU, thêm smooth để tránh chia cho 0\n",
    "        iou = (intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # Loss là phần bù của IoU\n",
    "        return 1 - iou\n",
    "\n",
    "\n",
    "def calculate_metrics(predicted_masks, true_masks, smooth=1e-6):\n",
    "    predicted_masks = predicted_masks.view(-1)\n",
    "    true_masks = true_masks.view(-1)\n",
    "    \n",
    "    intersection = (predicted_masks * true_masks).sum()\n",
    "    union = (predicted_masks + true_masks).sum() - intersection\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    dice = (2. * intersection + smooth) / (predicted_masks.sum() + true_masks.sum() + smooth)\n",
    "    \n",
    "    return iou.item(), dice.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e3822f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, callbacks_config, start_epoch=0, best_val_iou=0.0, history=None):\n",
    "    best_iou = best_val_iou\n",
    "    patience_counter = 0\n",
    "    checkpoint_path = callbacks_config.get('checkpoint_path', 'model_best.pth')\n",
    "    \n",
    "    # Sửa đổi: Chỉ lưu loss và iou\n",
    "    if history is None:\n",
    "        history = {'train_loss': [], 'val_loss': [], 'train_iou': [], 'val_iou': [], 'lr': []}\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        # Sửa đổi: Bỏ train_dice\n",
    "        train_loss, train_iou = 0.0, 0.0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for images, masks in loop:\n",
    "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            predicted_masks = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            # Sửa đổi: Chỉ lấy iou, bỏ qua dice\n",
    "            batch_iou, _ = calculate_metrics(predicted_masks, masks)\n",
    "            train_iou += batch_iou\n",
    "            \n",
    "            # Sửa đổi: Bỏ dice khỏi thanh tiến trình\n",
    "            loop.set_postfix(loss=loss.item(), iou=batch_iou)\n",
    "\n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "        history['train_iou'].append(train_iou / len(train_loader))\n",
    "        # Sửa đổi: Bỏ history cho train_dice\n",
    "\n",
    "        model.eval()\n",
    "        # Sửa đổi: Bỏ val_dice\n",
    "        val_loss, val_iou = 0.0, 0.0\n",
    "        \n",
    "        loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for images, masks in loop:\n",
    "                images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                predicted_masks = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                \n",
    "                # Sửa đổi: Chỉ lấy iou, bỏ qua dice\n",
    "                batch_iou, _ = calculate_metrics(predicted_masks, masks)\n",
    "                val_iou += batch_iou\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_iou = val_iou / len(val_loader)\n",
    "        \n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_iou'].append(avg_val_iou)\n",
    "        # Sửa đổi: Bỏ history cho val_dice\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Summary:\")\n",
    "        # Sửa đổi: Bỏ Dice khỏi bản tóm tắt\n",
    "        print(f\"  Train -> Loss: {history['train_loss'][-1]:.4f}, IoU: {history['train_iou'][-1]:.4f}\")\n",
    "        print(f\"  Val   -> Loss: {avg_val_loss:.4f}, IoU: {avg_val_iou:.4f}\")\n",
    "        print(f\"  LR: {current_lr:.8f}\")\n",
    "\n",
    "        if avg_val_iou > best_iou:\n",
    "            best_iou = avg_val_iou\n",
    "            patience_counter = 0\n",
    "            print(f\"🚀 New best validation IoU: {best_iou:.4f}. Saving model...\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_iou': best_iou,\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'history': history\n",
    "            }, checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation IoU did not improve. Patience: {patience_counter}/{callbacks_config['patience']}\")\n",
    "            if patience_counter >= callbacks_config['patience']:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "    return history\n",
    "\n",
    "# --- Hàm vẽ biểu đồ (Đã chỉnh sửa chỉ dùng IoU) ---\n",
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    \n",
    "    # Sửa đổi: Tạo một hàng 3 cột cho 3 biểu đồ\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(22, 6))\n",
    "\n",
    "    # Biểu đồ 1: Loss\n",
    "    axs[0].plot(epochs, history['train_loss'], 'o-', label='Train Loss')\n",
    "    axs[0].plot(epochs, history['val_loss'], 'o-', label='Validation Loss')\n",
    "    axs[0].set_title('Training and Validation Loss')\n",
    "    axs[0].set(xlabel='Epoch', ylabel='Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Biểu đồ 2: IoU\n",
    "    axs[1].plot(epochs, history['train_iou'], 'o-', label='Train IoU')\n",
    "    axs[1].plot(epochs, history['val_iou'], 'o-', label='Validation IoU')\n",
    "    axs[1].set_title('Training and Validation IoU')\n",
    "    axs[1].set(xlabel='Epoch', ylabel='IoU')\n",
    "    axs[1].legend()\n",
    "\n",
    "    axs[2].plot(epochs, history['lr'], 'o-', label='Learning Rate', color='purple')\n",
    "    axs[2].set_title('Learning Rate per Epoch')\n",
    "    axs[2].set(xlabel='Epoch', ylabel='Learning Rate')\n",
    "    axs[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb0bedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "\n",
    "def evaluate_model(model, data_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs)\n",
    "\n",
    "            preds = (preds > threshold).float()\n",
    "\n",
    "            all_preds.append(preds.view(-1).cpu().numpy())\n",
    "            all_targets.append(masks.view(-1).cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "\n",
    "    precision = precision_score(all_targets, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_targets, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
    "    iou = jaccard_score(all_targets, all_preds, zero_division=0)\n",
    "\n",
    "    print(f\"\\n--- Evaluation Metrics ---\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(f\"IoU:       {iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d982b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing iSwinUnet model (paper version)...\n",
      "Loading pretrained weights for Swin-Tiny encoder...\n",
      "✅ Successfully loaded 227/231 tensors from pretrained Swin-Tiny.\n",
      "   (Any unloaded tensors are expected to be from the decoder and final layers).\n",
      "==================================================\n",
      "!!! USING BCEWithLogitsLoss with pos_weight=30 !!!\n",
      "Model has 41,657,773 trainable parameters.\n",
      "==================================================\n",
      "No checkpoint found. Starting from scratch.\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 [Train]: 100%|██████████| 750/750 [02:40<00:00,  4.67it/s, iou=0.385, loss=0.433]   \n",
      "Epoch 1/200 [Val]: 100%|██████████| 200/200 [00:15<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary:\n",
      "  Train -> Loss: 0.8160, IoU: 0.1256\n",
      "  Val   -> Loss: 0.5661, IoU: 0.1524\n",
      "  LR: 0.00010000\n",
      "🚀 New best validation IoU: 0.1524. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 [Train]:   0%|          | 2/750 [00:00<02:39,  4.68it/s, iou=0.0473, loss=0.331]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# --- Bắt đầu Huấn luyện ---\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Starting Training ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_val_iou\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_val_iou\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training complete. ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# --- Vẽ biểu đồ và Đánh giá ---\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, callbacks_config, start_epoch, best_val_iou, history)\u001b[0m\n\u001b[0;32m     13\u001b[0m train_loss, train_iou \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     15\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Train]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[0;32m     17\u001b[0m     images, masks \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(DEVICE), masks\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m, in \u001b[0;36mCrackDetectionDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     11\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_filenames[idx])\n\u001b[0;32m     12\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 14\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_filenames\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMREAD_GRAYSCALE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(mask \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # --- Chuẩn bị dữ liệu ---\n",
    "    train_img_paths = sorted([os.path.join(train_img_dir, f) for f in os.listdir(train_img_dir)])\n",
    "    train_mask_paths = sorted([os.path.join(train_mask_dir, f) for f in os.listdir(train_mask_dir)])\n",
    "    val_img_paths = sorted([os.path.join(val_img_dir, f) for f in os.listdir(val_img_dir)])\n",
    "    val_mask_paths = sorted([os.path.join(val_mask_dir, f) for f in os.listdir(val_mask_dir)])\n",
    "\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.RandomRotate90(p=0.5),\n",
    "        A.ColorJitter(p=0.2),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = CrackDetectionDataset(train_img_paths, train_mask_paths, transform=train_transform)\n",
    "    val_dataset = CrackDetectionDataset(val_img_paths, val_mask_paths, transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # --- Khởi tạo Model, Optimizer, Loss ---\n",
    "    print(\"Initializing iSwinUnet model (paper version)...\")\n",
    "    model = iSwinUnet(num_classes=1, img_size=IMG_SIZE, pretrained=True).to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-7)\n",
    "    \n",
    "    pos_weight_value = 30\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight_value], device=DEVICE))\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(f\"!!! USING BCEWithLogitsLoss with pos_weight={pos_weight_value} !!!\")\n",
    "    print(f\"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # --- Cấu hình và tải Checkpoint (nếu có) ---\n",
    "    CHECKPOINT_PATH = 'iswin_unet_paper(test).pth'\n",
    "    callbacks_config = {'patience': 30, 'checkpoint_path': CHECKPOINT_PATH}\n",
    "\n",
    "    start_epoch, best_val_iou, history = 0, 0.0, None\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        print(f\"Checkpoint found. Loading...\")\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(DEVICE)\n",
    "        best_val_iou = checkpoint.get('best_val_iou', 0.0)\n",
    "        print(f\"Resuming from Epoch {start_epoch}. Best Val IoU so far: {best_val_iou:.4f}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "\n",
    "    # --- Bắt đầu Huấn luyện ---\n",
    "    print(f\"\\n--- Starting Training ---\")\n",
    "    training_history = train_model(\n",
    "        model, train_loader, val_loader, optimizer, criterion, scheduler,\n",
    "        num_epochs=NUM_EPOCHS, callbacks_config=callbacks_config,\n",
    "        start_epoch=start_epoch, best_val_iou=best_val_iou, history=history\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- Training complete. ---\")\n",
    "\n",
    "    # --- Vẽ biểu đồ và Đánh giá ---\n",
    "    if training_history and len(training_history['train_loss']) > 0:\n",
    "        print(\"Plotting training history...\")\n",
    "        plot_training_history(training_history)\n",
    "        \n",
    "        # Đánh giá trên tập validation với mô hình tốt nhất đã lưu\n",
    "        print(\"\\n--- Evaluating the best model on the validation set ---\")\n",
    "        model.load_state_dict(torch.load(CHECKPOINT_PATH)['model_state_dict'])\n",
    "        evaluate_model(model, val_loader, threshold=0.5)\n",
    "    else:\n",
    "        print(\"No training history available to plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a165c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.65, 1.01, 0.01)\n",
    "all_preds_raw = []\n",
    "all_targets = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(val_loader, desc=\"Collecting predictions\"):\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "\n",
    "        outputs = model(images)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "\n",
    "        all_preds_raw.append(probs.view(-1).cpu().numpy())\n",
    "        all_targets.append(masks.view(-1).cpu().numpy())\n",
    "\n",
    "all_preds_raw = np.concatenate(all_preds_raw)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "metrics = {'threshold': [], 'precision': [], 'recall': [], 'f1': [], 'iou': []}\n",
    "\n",
    "for t in thresholds:\n",
    "    preds_bin = (all_preds_raw > t).astype(np.uint8)\n",
    "\n",
    "    precision = precision_score(all_targets, preds_bin, zero_division=0)\n",
    "    recall = recall_score(all_targets, preds_bin, zero_division=0)\n",
    "    f1 = f1_score(all_targets, preds_bin, zero_division=0)\n",
    "    iou = jaccard_score(all_targets, preds_bin, zero_division=0)\n",
    "\n",
    "    metrics['threshold'].append(t)\n",
    "    metrics['precision'].append(precision)\n",
    "    metrics['recall'].append(recall)\n",
    "    metrics['f1'].append(f1)\n",
    "    metrics['iou'].append(iou)\n",
    "\n",
    "best_f1_idx = int(np.argmax(metrics['f1']))\n",
    "best_iou_idx = int(np.argmax(metrics['iou']))\n",
    "\n",
    "print(\"\\n=== Threshold Tốt Nhất ===\")\n",
    "print(f\"[F1]  Best at threshold = {metrics['threshold'][best_f1_idx]:.2f} | F1 = {metrics['f1'][best_f1_idx]:.4f}\")\n",
    "print(f\"[IoU] Best at threshold = {metrics['threshold'][best_iou_idx]:.2f} | IoU = {metrics['iou'][best_iou_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [round(t, 2) for t in metrics['threshold']]\n",
    "x_idx = np.arange(len(x))\n",
    "width = 0.25\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x_idx - width, metrics['precision'], width=width, label='Precision')\n",
    "plt.bar(x_idx, metrics['f1'], width=width, label='F1-score')\n",
    "plt.bar(x_idx + width, metrics['iou'], width=width, label='IoU')\n",
    "\n",
    "plt.xticks(x_idx, x)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Segmentation Metrics at Different Thresholds')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
