{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5890f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import SwinModel\n",
    "from transformers.models.swin.modeling_swin import SwinLayer\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.swin.modeling_swin import SwinPatchEmbeddings, SwinPatchMerging, SwinLayer\n",
    "from transformers import SwinConfig\n",
    "from typing import Tuple, Optional\n",
    "# --- CÃ i Ä‘áº·t tham sá»‘ cá»‘ Ä‘á»‹nh ---\n",
    "IMG_SIZE = 320 \n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "SEED = 42\n",
    "BATCH_SIZE = 2\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- ÄÆ°á»ng dáº«n dá»¯ liá»‡u (Thay Ä‘á»•i náº¿u cáº§n) ---\n",
    "train_img_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\train\\image'\n",
    "train_mask_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\train\\label'\n",
    "val_img_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\val\\image'\n",
    "val_mask_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\val\\label'\n",
    "\n",
    "# --- Thu tháº­p Ä‘Æ°á»ng dáº«n tá»‡p áº£nh vÃ  mask ---\n",
    "train_img_paths = sorted([os.path.join(train_img_dir, f) for f in os.listdir(train_img_dir)])\n",
    "train_mask_paths = sorted([os.path.join(train_mask_dir, f) for f in os.listdir(train_mask_dir)])\n",
    "val_img_paths = sorted([os.path.join(val_img_dir, f) for f in os.listdir(val_img_dir)])\n",
    "val_mask_paths = sorted([os.path.join(val_mask_dir, f) for f in os.listdir(val_mask_dir)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e076cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrackDetectionDataset(Dataset):\n",
    "    def __init__(self, image_filenames, mask_filenames, transform=None):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.mask_filenames = mask_filenames\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.image_filenames[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = cv2.imread(self.mask_filenames[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = np.where(mask > 0, 255.0, 0.0).astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        mask = mask / 255.0\n",
    "        mask = mask.unsqueeze(0)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d425ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExpanding(nn.Module):\n",
    "    def __init__(self, input_resolution: Tuple[int, int], dim: int, return_vector: bool = True):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.expand = nn.Linear(dim, 2 * dim, bias=False)\n",
    "        self.norm = nn.LayerNorm(dim // 2)\n",
    "        self.return_vector = return_vector\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        H, W = self.input_resolution\n",
    "        x = x.view(-1, H, W, self.dim)\n",
    "        x = self.expand(x)\n",
    "        B, H, W, C = x.shape\n",
    "        x = x.view(B, H, W, 2, 2, C // 4)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H * 2, W * 2, C // 4)\n",
    "        x = self.norm(x)\n",
    "        if self.return_vector:\n",
    "            x = x.view(B, -1, C // 4)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FinalPatchExpanding(nn.Module):\n",
    "    def __init__(self, dim: int, scale_factor: int = 4):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.output_dim = dim // scale_factor\n",
    "\n",
    "        expand_dim = self.output_dim * (scale_factor**2)\n",
    "        self.expand = nn.Linear(dim, expand_dim, bias=False)\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, L, _ = x.shape\n",
    "        H = W = int(np.sqrt(L))\n",
    "        x_expanded = self.expand(x)\n",
    "\n",
    "        x_reshaped = x_expanded.view(B, H, W, self.scale_factor, self.scale_factor, self.output_dim)\n",
    "\n",
    "        x_permuted = x_reshaped.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H * self.scale_factor, W * self.scale_factor, self.output_dim)\n",
    "\n",
    "        x_normed = self.norm(x_permuted)\n",
    "\n",
    "        output = x_normed.view(B, -1, self.output_dim)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class SkipAttention(nn.Module):\n",
    "    def __init__(self, query_dim: int, context_dim: Optional[int] = None, num_heads: int = 8, dim_head: int = 64):\n",
    "        super().__init__()\n",
    "        if context_dim is None:\n",
    "            context_dim = query_dim\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(embed_dim=query_dim, num_heads=num_heads, \n",
    "                                          kdim=context_dim, vdim=context_dim, \n",
    "                                          batch_first=True)\n",
    "        self.norm = nn.LayerNorm(query_dim)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, context: torch.Tensor) -> torch.Tensor:\n",
    "        attn_output, _ = self.attn(query=query, key=context, value=context)\n",
    "        return self.norm(query + attn_output)\n",
    "\n",
    "\n",
    "class ResidualSwinBlock(nn.Module):\n",
    "    def __init__(self, config: SwinConfig, dim: int, input_resolution: Tuple[int, int], num_layers: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            SwinLayer(\n",
    "                config=config, \n",
    "                dim=dim, \n",
    "                input_resolution=input_resolution, \n",
    "                num_heads=num_heads, \n",
    "                shift_size=0 if (i % 2 == 0) else config.window_size // 2\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, input_resolution: Tuple[int, int]) -> torch.Tensor:\n",
    "        residual = x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, input_resolution)[0]\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class iSwinUnet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1, img_size: int = 320, pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        config = SwinConfig(image_size=img_size, patch_size=4, window_size=7)\n",
    "        self.config = config\n",
    "        self.encoder_channels = [config.embed_dim * (2**i) for i in range(len(config.depths))]\n",
    "\n",
    "        self.patch_embed = SwinPatchEmbeddings(config)\n",
    "        self.pos_drop = nn.Dropout(p=config.hidden_dropout_prob)\n",
    "\n",
    "        self.encoder_stage1 = ResidualSwinBlock(config, self.encoder_channels[0], (img_size//4, img_size//4), config.depths[0], config.num_heads[0])\n",
    "        self.merge1 = SwinPatchMerging(input_resolution=(img_size//4, img_size//4), dim=self.encoder_channels[0])\n",
    "        \n",
    "        self.encoder_stage2 = ResidualSwinBlock(config, self.encoder_channels[1], (img_size//8, img_size//8), config.depths[1], config.num_heads[1])\n",
    "        self.merge2 = SwinPatchMerging(input_resolution=(img_size//8, img_size//8), dim=self.encoder_channels[1])\n",
    "\n",
    "        self.encoder_stage3 = ResidualSwinBlock(config, self.encoder_channels[2], (img_size//16, img_size//16), config.depths[2], config.num_heads[2])\n",
    "        self.merge3 = SwinPatchMerging(input_resolution=(img_size//16, img_size//16), dim=self.encoder_channels[2])\n",
    "        \n",
    "        self.bottleneck = ResidualSwinBlock(config, self.encoder_channels[3], (img_size//32, img_size//32), config.depths[3], config.num_heads[3])\n",
    "\n",
    "        self.decoder_expand3 = PatchExpanding(input_resolution=(img_size//32, img_size//32), dim=self.encoder_channels[3])\n",
    "        self.skip_attn3 = SkipAttention(query_dim=self.encoder_channels[2], context_dim=self.encoder_channels[2], num_heads=config.num_heads[2])\n",
    "        self.decoder_stage3 = ResidualSwinBlock(config, self.encoder_channels[2], (img_size//16, img_size//16), config.depths[2], config.num_heads[2])\n",
    "\n",
    "        self.decoder_expand2 = PatchExpanding(input_resolution=(img_size//16, img_size//16), dim=self.encoder_channels[2])\n",
    "        self.skip_attn2 = SkipAttention(query_dim=self.encoder_channels[1], context_dim=self.encoder_channels[1], num_heads=config.num_heads[1])\n",
    "        self.decoder_stage2 = ResidualSwinBlock(config, self.encoder_channels[1], (img_size//8, img_size//8), config.depths[1], config.num_heads[1])\n",
    "\n",
    "        self.decoder_expand1 = PatchExpanding(input_resolution=(img_size//8, img_size//8), dim=self.encoder_channels[1])\n",
    "        self.skip_attn1 = SkipAttention(query_dim=self.encoder_channels[0], context_dim=self.encoder_channels[0], num_heads=config.num_heads[0])\n",
    "        self.decoder_stage1 = ResidualSwinBlock(config, self.encoder_channels[0], (img_size//4, img_size//4), config.depths[0], config.num_heads[0])\n",
    "\n",
    "        self.final_expand = FinalPatchExpanding(dim=self.encoder_channels[0], scale_factor=4)\n",
    "        self.final_conv = nn.Conv2d(self.final_expand.output_dim, num_classes, kernel_size=1)\n",
    "\n",
    "        if pretrained:\n",
    "            self.load_pretrained_weights()\n",
    "\n",
    "    def load_pretrained_weights(self):\n",
    "            print(\"Loading pretrained weights for Swin-Tiny encoder...\")\n",
    "            try:\n",
    "                swin_original = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "                original_state_dict = swin_original.state_dict()\n",
    "\n",
    "                model_state_dict = self.state_dict()\n",
    "\n",
    "                prefix_mapping = {\n",
    "                    'embeddings.patch_embeddings': 'patch_embed',\n",
    "                    'encoder.layers.0.downsample': 'merge1',\n",
    "                    'encoder.layers.1.downsample': 'merge2',\n",
    "                    'encoder.layers.2.downsample': 'merge3',\n",
    "                }\n",
    "\n",
    "                stage_mapping = {\n",
    "                    'encoder.layers.0': 'encoder_stage1',\n",
    "                    'encoder.layers.1': 'encoder_stage2',\n",
    "                    'encoder.layers.2': 'encoder_stage3',\n",
    "                    'encoder.layers.3': 'bottleneck',\n",
    "                }\n",
    "\n",
    "                loaded_count = 0\n",
    "                \n",
    "                orig_pos_embed_key = 'embeddings.absolute_position_embedding'\n",
    "                if orig_pos_embed_key in original_state_dict:\n",
    "                    orig_pos_embed = original_state_dict[orig_pos_embed_key]\n",
    "                    target_pos_embed = model_state_dict[orig_pos_embed_key]\n",
    "                    if orig_pos_embed.shape != target_pos_embed.shape:\n",
    "                        print(f\"Interpolating absolute position embedding due to size mismatch...\")\n",
    "                        orig_pos_embed_spatial = orig_pos_embed[0, 1:, :].permute(1, 0)\n",
    "\n",
    "                        H_orig = W_orig = int((orig_pos_embed.shape[1] - 1) ** 0.5)\n",
    "                        orig_pos_embed_spatial = orig_pos_embed_spatial.view(1, -1, H_orig, W_orig)\n",
    "                        H_target = W_target = int((target_pos_embed.shape[1] - 1) ** 0.5)\n",
    "                        interp_pos_embed = nn.functional.interpolate(\n",
    "                            orig_pos_embed_spatial, size=(H_target, W_target), mode='bicubic', align_corners=False\n",
    "                        )\n",
    "                        interp_pos_embed = interp_pos_embed.view(1, -1, H_target * W_target).permute(0, 2, 1)\n",
    "                        # ThÃªm láº¡i token [CLS]\n",
    "                        final_pos_embed = torch.cat((orig_pos_embed[0, 0:1, :], interp_pos_embed[0]), dim=0).unsqueeze(0)\n",
    "                        model_state_dict[orig_pos_embed_key] = final_pos_embed\n",
    "                        loaded_count += 1\n",
    "                    del original_state_dict[orig_pos_embed_key]\n",
    "\n",
    "                for key_orig, value in original_state_dict.items():\n",
    "                    new_key = None\n",
    "\n",
    "                    if 'pooler' in key_orig:\n",
    "                        continue\n",
    "\n",
    "                    for old_prefix, new_prefix in prefix_mapping.items():\n",
    "                        if key_orig.startswith(old_prefix):\n",
    "                            new_key = key_orig.replace(old_prefix, new_prefix)\n",
    "                            break\n",
    "\n",
    "                    if new_key is None:\n",
    "                        for old_stage_prefix, new_stage_prefix in stage_mapping.items():\n",
    "                            if key_orig.startswith(old_stage_prefix):\n",
    "                                remainder = key_orig[len(old_stage_prefix):]\n",
    "                                if remainder.startswith('.blocks'):\n",
    "                                    remainder = remainder.replace('.blocks', '', 1)\n",
    "                                    new_key = f\"{new_stage_prefix}.layers{remainder}\"\n",
    "                                break\n",
    "\n",
    "                    if new_key is None:\n",
    "                        if key_orig.startswith('embeddings.norm'):\n",
    "                            new_key = key_orig.replace('embeddings.norm', 'patch_embed.norm')\n",
    "                        elif key_orig.startswith('encoder.norm'):\n",
    "                            continue\n",
    "\n",
    "                    if new_key and new_key in model_state_dict and model_state_dict[new_key].shape == value.shape:\n",
    "                        model_state_dict[new_key] = value\n",
    "                        loaded_count += 1\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                self.load_state_dict(model_state_dict, strict=False)\n",
    "\n",
    "                expected_keys = len([k for k in swin_original.state_dict().keys() if 'pooler' not in k and 'encoder.norm' not in k])\n",
    "                print(f\"âœ… Successfully loaded {loaded_count}/{expected_keys} tensors from pretrained Swin-Tiny.\")\n",
    "                print(\"   (Any unloaded tensors are expected to be from the decoder and final layers).\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ERROR: Could not load pretrained weights. Error: {e}. Training from scratch.\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        s1_skip = self.patch_embed(x)[0]\n",
    "        s1_skip = self.pos_drop(s1_skip)\n",
    "        \n",
    "        s1_out = self.encoder_stage1(s1_skip, (self.img_size//4, self.img_size//4))\n",
    "        s2_in = self.merge1(s1_out, (self.img_size//4, self.img_size//4))\n",
    "        \n",
    "        s2_out = self.encoder_stage2(s2_in, (self.img_size//8, self.img_size//8))\n",
    "        s3_in = self.merge2(s2_out, (self.img_size//8, self.img_size//8))\n",
    "\n",
    "        s3_out = self.encoder_stage3(s3_in, (self.img_size//16, self.img_size//16))\n",
    "        b_in = self.merge3(s3_out, (self.img_size//16, self.img_size//16))\n",
    "        \n",
    "        b_out = self.bottleneck(b_in, (self.img_size//32, self.img_size//32))\n",
    "\n",
    "        d3_in = self.decoder_expand3(b_out)\n",
    "        d3_in = self.skip_attn3(query=d3_in, context=s3_out)\n",
    "        d3_out = self.decoder_stage3(d3_in, (self.img_size//16, self.img_size//16))\n",
    "\n",
    "        d2_in = self.decoder_expand2(d3_out)\n",
    "        d2_in = self.skip_attn2(query=d2_in, context=s2_out)\n",
    "        d2_out = self.decoder_stage2(d2_in, (self.img_size//8, self.img_size//8))\n",
    "\n",
    "        d1_in = self.decoder_expand1(d2_out)\n",
    "        d1_in = self.skip_attn1(query=d1_in, context=s1_out)\n",
    "        d1_out = self.decoder_stage1(d1_in, (self.img_size//4, self.img_size//4))\n",
    "        \n",
    "        final_features = self.final_expand(d1_out)\n",
    "        \n",
    "        B, L, C = final_features.shape\n",
    "        H = W = int(np.sqrt(L))\n",
    "        final_features = final_features.permute(0, 2, 1).contiguous().view(B, C, H, W)\n",
    "        \n",
    "        output = self.final_conv(final_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab28d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(IoULoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs lÃ  logits tá»« model, cáº§n Ä‘Æ°a qua sigmoid Ä‘á»ƒ cÃ³ xÃ¡c suáº¥t (0, 1)\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "        \n",
    "        # LÃ m pháº³ng inputs vÃ  targets Ä‘á»ƒ tÃ­nh toÃ¡n dá»… dÃ ng\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # TÃ­nh toÃ¡n intersection (pháº§n giao) vÃ  union (pháº§n há»£p)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        total = (inputs + targets).sum()\n",
    "        union = total - intersection\n",
    "        \n",
    "        # TÃ­nh IoU, thÃªm smooth Ä‘á»ƒ trÃ¡nh chia cho 0\n",
    "        iou = (intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # Loss lÃ  pháº§n bÃ¹ cá»§a IoU\n",
    "        return 1 - iou\n",
    "\n",
    "\n",
    "def calculate_metrics(predicted_masks, true_masks, smooth=1e-6):\n",
    "    predicted_masks = predicted_masks.view(-1)\n",
    "    true_masks = true_masks.view(-1)\n",
    "    \n",
    "    intersection = (predicted_masks * true_masks).sum()\n",
    "    union = (predicted_masks + true_masks).sum() - intersection\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    dice = (2. * intersection + smooth) / (predicted_masks.sum() + true_masks.sum() + smooth)\n",
    "    \n",
    "    return iou.item(), dice.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e3822f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, callbacks_config, start_epoch=0, best_val_iou=0.0, history=None):\n",
    "    best_iou = best_val_iou\n",
    "    patience_counter = 0\n",
    "    checkpoint_path = callbacks_config.get('checkpoint_path', 'model_best.pth')\n",
    "    \n",
    "    # Sá»­a Ä‘á»•i: Chá»‰ lÆ°u loss vÃ  iou\n",
    "    if history is None:\n",
    "        history = {'train_loss': [], 'val_loss': [], 'train_iou': [], 'val_iou': [], 'lr': []}\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        # Sá»­a Ä‘á»•i: Bá» train_dice\n",
    "        train_loss, train_iou = 0.0, 0.0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for images, masks in loop:\n",
    "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            predicted_masks = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            # Sá»­a Ä‘á»•i: Chá»‰ láº¥y iou, bá» qua dice\n",
    "            batch_iou, _ = calculate_metrics(predicted_masks, masks)\n",
    "            train_iou += batch_iou\n",
    "            \n",
    "            # Sá»­a Ä‘á»•i: Bá» dice khá»i thanh tiáº¿n trÃ¬nh\n",
    "            loop.set_postfix(loss=loss.item(), iou=batch_iou)\n",
    "\n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "        history['train_iou'].append(train_iou / len(train_loader))\n",
    "        # Sá»­a Ä‘á»•i: Bá» history cho train_dice\n",
    "\n",
    "        model.eval()\n",
    "        # Sá»­a Ä‘á»•i: Bá» val_dice\n",
    "        val_loss, val_iou = 0.0, 0.0\n",
    "        \n",
    "        loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for images, masks in loop:\n",
    "                images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                predicted_masks = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                \n",
    "                # Sá»­a Ä‘á»•i: Chá»‰ láº¥y iou, bá» qua dice\n",
    "                batch_iou, _ = calculate_metrics(predicted_masks, masks)\n",
    "                val_iou += batch_iou\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_iou = val_iou / len(val_loader)\n",
    "        \n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_iou'].append(avg_val_iou)\n",
    "        # Sá»­a Ä‘á»•i: Bá» history cho val_dice\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Summary:\")\n",
    "        # Sá»­a Ä‘á»•i: Bá» Dice khá»i báº£n tÃ³m táº¯t\n",
    "        print(f\"  Train -> Loss: {history['train_loss'][-1]:.4f}, IoU: {history['train_iou'][-1]:.4f}\")\n",
    "        print(f\"  Val   -> Loss: {avg_val_loss:.4f}, IoU: {avg_val_iou:.4f}\")\n",
    "        print(f\"  LR: {current_lr:.8f}\")\n",
    "\n",
    "        if avg_val_iou > best_iou:\n",
    "            best_iou = avg_val_iou\n",
    "            patience_counter = 0\n",
    "            print(f\"ðŸš€ New best validation IoU: {best_iou:.4f}. Saving model...\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_iou': best_iou,\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'history': history\n",
    "            }, checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation IoU did not improve. Patience: {patience_counter}/{callbacks_config['patience']}\")\n",
    "            if patience_counter >= callbacks_config['patience']:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "    return history\n",
    "\n",
    "# --- HÃ m váº½ biá»ƒu Ä‘á»“ (ÄÃ£ chá»‰nh sá»­a chá»‰ dÃ¹ng IoU) ---\n",
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    \n",
    "    # Sá»­a Ä‘á»•i: Táº¡o má»™t hÃ ng 3 cá»™t cho 3 biá»ƒu Ä‘á»“\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(22, 6))\n",
    "\n",
    "    # Biá»ƒu Ä‘á»“ 1: Loss\n",
    "    axs[0].plot(epochs, history['train_loss'], 'o-', label='Train Loss')\n",
    "    axs[0].plot(epochs, history['val_loss'], 'o-', label='Validation Loss')\n",
    "    axs[0].set_title('Training and Validation Loss')\n",
    "    axs[0].set(xlabel='Epoch', ylabel='Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Biá»ƒu Ä‘á»“ 2: IoU\n",
    "    axs[1].plot(epochs, history['train_iou'], 'o-', label='Train IoU')\n",
    "    axs[1].plot(epochs, history['val_iou'], 'o-', label='Validation IoU')\n",
    "    axs[1].set_title('Training and Validation IoU')\n",
    "    axs[1].set(xlabel='Epoch', ylabel='IoU')\n",
    "    axs[1].legend()\n",
    "\n",
    "    axs[2].plot(epochs, history['lr'], 'o-', label='Learning Rate', color='purple')\n",
    "    axs[2].set_title('Learning Rate per Epoch')\n",
    "    axs[2].set(xlabel='Epoch', ylabel='Learning Rate')\n",
    "    axs[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb0bedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "\n",
    "def evaluate_model(model, data_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs)\n",
    "\n",
    "            preds = (preds > threshold).float()\n",
    "\n",
    "            all_preds.append(preds.view(-1).cpu().numpy())\n",
    "            all_targets.append(masks.view(-1).cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "\n",
    "    precision = precision_score(all_targets, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_targets, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
    "    iou = jaccard_score(all_targets, all_preds, zero_division=0)\n",
    "\n",
    "    print(f\"\\n--- Evaluation Metrics ---\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(f\"IoU:       {iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d982b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing iSwinUnet model (paper version)...\n",
      "Loading pretrained weights for Swin-Tiny encoder...\n",
      "âœ… Successfully loaded 227/231 tensors from pretrained Swin-Tiny.\n",
      "   (Any unloaded tensors are expected to be from the decoder and final layers).\n",
      "==================================================\n",
      "!!! USING BCEWithLogitsLoss with pos_weight=30 !!!\n",
      "Model has 41,657,773 trainable parameters.\n",
      "==================================================\n",
      "No checkpoint found. Starting from scratch.\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 750/750 [02:40<00:00,  4.67it/s, iou=0.385, loss=0.433]   \n",
      "Epoch 1/200 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary:\n",
      "  Train -> Loss: 0.8160, IoU: 0.1256\n",
      "  Val   -> Loss: 0.5661, IoU: 0.1524\n",
      "  LR: 0.00010000\n",
      "ðŸš€ New best validation IoU: 0.1524. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 [Train]:   0%|          | 2/750 [00:00<02:39,  4.68it/s, iou=0.0473, loss=0.331]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# --- Báº¯t Ä‘áº§u Huáº¥n luyá»‡n ---\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Starting Training ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_val_iou\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_val_iou\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training complete. ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# --- Váº½ biá»ƒu Ä‘á»“ vÃ  ÄÃ¡nh giÃ¡ ---\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, callbacks_config, start_epoch, best_val_iou, history)\u001b[0m\n\u001b[0;32m     13\u001b[0m train_loss, train_iou \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     15\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Train]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[0;32m     17\u001b[0m     images, masks \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(DEVICE), masks\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m, in \u001b[0;36mCrackDetectionDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     11\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_filenames[idx])\n\u001b[0;32m     12\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 14\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_filenames\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMREAD_GRAYSCALE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(mask \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # --- Chuáº©n bá»‹ dá»¯ liá»‡u ---\n",
    "    train_img_paths = sorted([os.path.join(train_img_dir, f) for f in os.listdir(train_img_dir)])\n",
    "    train_mask_paths = sorted([os.path.join(train_mask_dir, f) for f in os.listdir(train_mask_dir)])\n",
    "    val_img_paths = sorted([os.path.join(val_img_dir, f) for f in os.listdir(val_img_dir)])\n",
    "    val_mask_paths = sorted([os.path.join(val_mask_dir, f) for f in os.listdir(val_mask_dir)])\n",
    "\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.RandomRotate90(p=0.5),\n",
    "        A.ColorJitter(p=0.2),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = CrackDetectionDataset(train_img_paths, train_mask_paths, transform=train_transform)\n",
    "    val_dataset = CrackDetectionDataset(val_img_paths, val_mask_paths, transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # --- Khá»Ÿi táº¡o Model, Optimizer, Loss ---\n",
    "    print(\"Initializing iSwinUnet model (paper version)...\")\n",
    "    model = iSwinUnet(num_classes=1, img_size=IMG_SIZE, pretrained=True).to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-7)\n",
    "    \n",
    "    pos_weight_value = 30\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight_value], device=DEVICE))\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(f\"!!! USING BCEWithLogitsLoss with pos_weight={pos_weight_value} !!!\")\n",
    "    print(f\"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # --- Cáº¥u hÃ¬nh vÃ  táº£i Checkpoint (náº¿u cÃ³) ---\n",
    "    CHECKPOINT_PATH = 'iswin_unet_paper(test).pth'\n",
    "    callbacks_config = {'patience': 30, 'checkpoint_path': CHECKPOINT_PATH}\n",
    "\n",
    "    start_epoch, best_val_iou, history = 0, 0.0, None\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        print(f\"Checkpoint found. Loading...\")\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(DEVICE)\n",
    "        best_val_iou = checkpoint.get('best_val_iou', 0.0)\n",
    "        print(f\"Resuming from Epoch {start_epoch}. Best Val IoU so far: {best_val_iou:.4f}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "\n",
    "    # --- Báº¯t Ä‘áº§u Huáº¥n luyá»‡n ---\n",
    "    print(f\"\\n--- Starting Training ---\")\n",
    "    training_history = train_model(\n",
    "        model, train_loader, val_loader, optimizer, criterion, scheduler,\n",
    "        num_epochs=NUM_EPOCHS, callbacks_config=callbacks_config,\n",
    "        start_epoch=start_epoch, best_val_iou=best_val_iou, history=history\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- Training complete. ---\")\n",
    "\n",
    "    # --- Váº½ biá»ƒu Ä‘á»“ vÃ  ÄÃ¡nh giÃ¡ ---\n",
    "    if training_history and len(training_history['train_loss']) > 0:\n",
    "        print(\"Plotting training history...\")\n",
    "        plot_training_history(training_history)\n",
    "        \n",
    "        # ÄÃ¡nh giÃ¡ trÃªn táº­p validation vá»›i mÃ´ hÃ¬nh tá»‘t nháº¥t Ä‘Ã£ lÆ°u\n",
    "        print(\"\\n--- Evaluating the best model on the validation set ---\")\n",
    "        model.load_state_dict(torch.load(CHECKPOINT_PATH)['model_state_dict'])\n",
    "        evaluate_model(model, val_loader, threshold=0.5)\n",
    "    else:\n",
    "        print(\"No training history available to plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a165c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.65, 1.01, 0.01)\n",
    "all_preds_raw = []\n",
    "all_targets = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(val_loader, desc=\"Collecting predictions\"):\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "\n",
    "        outputs = model(images)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "\n",
    "        all_preds_raw.append(probs.view(-1).cpu().numpy())\n",
    "        all_targets.append(masks.view(-1).cpu().numpy())\n",
    "\n",
    "all_preds_raw = np.concatenate(all_preds_raw)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "metrics = {'threshold': [], 'precision': [], 'recall': [], 'f1': [], 'iou': []}\n",
    "\n",
    "for t in thresholds:\n",
    "    preds_bin = (all_preds_raw > t).astype(np.uint8)\n",
    "\n",
    "    precision = precision_score(all_targets, preds_bin, zero_division=0)\n",
    "    recall = recall_score(all_targets, preds_bin, zero_division=0)\n",
    "    f1 = f1_score(all_targets, preds_bin, zero_division=0)\n",
    "    iou = jaccard_score(all_targets, preds_bin, zero_division=0)\n",
    "\n",
    "    metrics['threshold'].append(t)\n",
    "    metrics['precision'].append(precision)\n",
    "    metrics['recall'].append(recall)\n",
    "    metrics['f1'].append(f1)\n",
    "    metrics['iou'].append(iou)\n",
    "\n",
    "best_f1_idx = int(np.argmax(metrics['f1']))\n",
    "best_iou_idx = int(np.argmax(metrics['iou']))\n",
    "\n",
    "print(\"\\n=== Threshold Tá»‘t Nháº¥t ===\")\n",
    "print(f\"[F1]  Best at threshold = {metrics['threshold'][best_f1_idx]:.2f} | F1 = {metrics['f1'][best_f1_idx]:.4f}\")\n",
    "print(f\"[IoU] Best at threshold = {metrics['threshold'][best_iou_idx]:.2f} | IoU = {metrics['iou'][best_iou_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [round(t, 2) for t in metrics['threshold']]\n",
    "x_idx = np.arange(len(x))\n",
    "width = 0.25\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x_idx - width, metrics['precision'], width=width, label='Precision')\n",
    "plt.bar(x_idx, metrics['f1'], width=width, label='F1-score')\n",
    "plt.bar(x_idx + width, metrics['iou'], width=width, label='IoU')\n",
    "\n",
    "plt.xticks(x_idx, x)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Segmentation Metrics at Different Thresholds')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
