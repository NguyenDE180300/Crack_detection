{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f394802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m \u001b[38;5;66;03m# Import F for functional operations\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SwinConfig, SwinModel\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mA\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToTensorV2\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\utils\\import_utils.py:2045\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2045\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2046\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\utils\\import_utils.py:2073\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2072\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2073\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2074\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2075\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\swin\\modeling_swin.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ACT2FN\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneOutput\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelOutput, auto_docstring, logging, torch_int\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\modeling_utils.py:73\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdpa_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sdpa_attention_forward\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_parallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     65\u001b[0m     ALL_PARALLEL_STYLES,\n\u001b[0;32m     66\u001b[0m     _get_parameter_tp_plan,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m     verify_tp_plan,\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LOSS_MAPPING\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     Conv1D,\n\u001b[0;32m     76\u001b[0m     apply_chunking_to_forward,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m     prune_linear_layer,\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoHfQuantizer, HfQuantizer\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\loss\\loss_utils.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, MSELoss\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_d_fine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DFineForObjectDetectionLoss\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_deformable_detr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\loss\\loss_d_fine.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_vision_available\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     box_iou,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_rt_detr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RTDetrHungarianMatcher, RTDetrLoss\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available():\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\loss\\loss_for_object_detection.py:32\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m linear_sum_assignment\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available():\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m center_to_corners_format\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdice_loss\u001b[39m(inputs, targets, num_boxes):\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    Compute the DICE loss, similar to generalized IOU for masks\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m                 class).\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\image_transforms.py:47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\__init__.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     46\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m feature_column\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interim\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute.combinations namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;66;03m# line: 456\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate \u001b[38;5;66;03m# line: 365\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m in_main_process \u001b[38;5;66;03m# line: 418\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py:33\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_all_reduce_strategy\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow_server_pb2\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_device_ops \u001b[38;5;28;01mas\u001b[39;00m cross_device_ops_lib\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_lib\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute_utils\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callable, List, Optional, Union\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m values \u001b[38;5;28;01mas\u001b[39;00m value_lib\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backprop_util\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\values.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m struct_pb2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m packed_distributed_variable \u001b[38;5;28;01mas\u001b[39;00m packed\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reduce_util\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:206\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ag_ctx \u001b[38;5;28;01mas\u001b[39;00m autograph_ctx\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m autograph\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataset_ops\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:98\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:419\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_dataset\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data_service_pb2\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compression_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pywrap_server_lib\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structure\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompress\u001b[39m(element):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m resource_variable_ops\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ragged_tensor\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m internal\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_tensor.py:3149\u001b[0m\n\u001b[0;32m   3144\u001b[0m RaggedOrDense \u001b[38;5;241m=\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[Ragged, core_types\u001b[38;5;241m.\u001b[39mTensorLike]\n\u001b[0;32m   3146\u001b[0m \u001b[38;5;66;03m# RaggedTensor must import ragged_ops to ensure that all dispatched ragged ops\u001b[39;00m\n\u001b[0;32m   3147\u001b[0m \u001b[38;5;66;03m# are registered. Ragged ops import RaggedTensor, so import at bottom of the\u001b[39;00m\n\u001b[0;32m   3148\u001b[0m \u001b[38;5;66;03m# file to avoid a partially-initialized module error.\u001b[39;00m\n\u001b[1;32m-> 3149\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ragged_ops  \u001b[38;5;66;03m# pylint: disable=unused-import, g-bad-import-order, g-import-not-at-top\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_ops.py:41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ragged_gather_ops\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ragged_getitem\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ragged_image_ops\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ragged_map_ops\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ragged_math_ops\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_image_ops.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cond\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m image_ops\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m map_fn\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m math_ops\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops.py:159\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgen_image_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_ops_impl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# TODO(drpng): remove these once internal use has discontinued.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_ops_impl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _Check3DImage\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_image_ops\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn_impl\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn_ops\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m random_ops\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m candidate_sampling_ops\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cond \u001b[38;5;28;01mas\u001b[39;00m tf_cond\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ctc_ops  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_gradient\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m embedding_ops\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\ctc_ops.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_ops_stack\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_gradient\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional_ops\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_array_ops\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\custom_gradient.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Decorator to overrides the gradient for a function.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backprop\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m record\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_array_ops\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_math_ops\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gradients_impl  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m resource_variable_ops\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel_for\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m control_flow_ops \u001b[38;5;28;01mas\u001b[39;00m pfor_ops\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging_ops  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m manip_grad  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m math_grad  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nccl_ops  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:34\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m special_math_ops\n\u001b[0;32m     33\u001b[0m \u001b[38;5;129;43m@ops\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRegisterGradient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mArgMax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43m_ArgMaxGrad\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mdel\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1682\u001b[0m, in \u001b[0;36mRegisterGradient.__call__\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   1680\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, f):\n\u001b[0;32m   1681\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Registers the function `f` as gradient function for `op_type`.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1682\u001b[0m   \u001b[43mgradient_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1683\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\registry.py:65\u001b[0m, in \u001b[0;36mRegistry.register\u001b[1;34m(self, candidate, name)\u001b[0m\n\u001b[0;32m     62\u001b[0m logging\u001b[38;5;241m.\u001b[39mvlog(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegistering \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, candidate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# stack trace is [this_function, Register(), user_function,...]\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# so the user function is #2.\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m stack_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mlen\u001b[39m(stack) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stack_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\traceback.py:227\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    226\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 227\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\traceback.py:383\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m--> 383\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\traceback.py:306\u001b[0m, in \u001b[0;36mFrameSummary.line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_line \u001b[38;5;241m=\u001b[39m \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlineno\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_line\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\linecache.py:30\u001b[0m, in \u001b[0;36mgetline\u001b[1;34m(filename, lineno, module_globals)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgetline\u001b[39m(filename, lineno, module_globals\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a line for a Python source file from the cache.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    Update the cache if it doesn't contain an entry for this file already.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     lines \u001b[38;5;241m=\u001b[39m \u001b[43mgetlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_globals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m lineno \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(lines):\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m lines[lineno \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\linecache.py:46\u001b[0m, in \u001b[0;36mgetlines\u001b[1;34m(filename, module_globals)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cache[filename][\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mupdatecache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_globals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mMemoryError\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     clearcache()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\linecache.py:136\u001b[0m, in \u001b[0;36mupdatecache\u001b[1;34m(filename, module_globals)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    137\u001b[0m         lines \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mSyntaxError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\tokenize.py:394\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen\u001b[39m(filename):\n\u001b[0;32m    391\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Open a file in read only mode using the encoding detected by\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m    detect_encoding().\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m     buffer \u001b[38;5;241m=\u001b[39m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    396\u001b[0m         encoding, lines \u001b[38;5;241m=\u001b[39m detect_encoding(buffer\u001b[38;5;241m.\u001b[39mreadline)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # Import F for functional operations\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import SwinConfig, SwinModel\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# --- Ci t tham s c nh ---\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 8\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"S dng thit b: {DEVICE}\")\n",
    "\n",
    "# Thay i ng dn th mc ty theo my ca bn\n",
    "train_img_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\train\\image'\n",
    "train_mask_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\train\\label'\n",
    "val_img_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\val\\image'\n",
    "val_mask_dir = r'C:\\Users\\Admin\\Documents\\Python Project\\DPL Crack detection\\UDTIRI-Crack Detection\\val\\label'\n",
    "\n",
    "# --- Thu thp ng dn tp nh v mask ---\n",
    "train_img_paths = sorted([os.path.join(train_img_dir, f) for f in os.listdir(train_img_dir)])\n",
    "train_mask_paths = sorted([os.path.join(train_mask_dir, f) for f in os.listdir(train_mask_dir)])\n",
    "val_img_paths = sorted([os.path.join(val_img_dir, f) for f in os.listdir(val_img_dir)])\n",
    "val_mask_paths = sorted([os.path.join(val_mask_dir, f) for f in os.listdir(val_mask_dir)])\n",
    "\n",
    "class CrackDetectionDataset(Dataset):\n",
    "    def __init__(self, image_filenames, mask_filenames, augment=False):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.mask_filenames = mask_filenames\n",
    "        self.augment = augment\n",
    "\n",
    "        if len(self.image_filenames) != len(self.mask_filenames):\n",
    "            raise ValueError(\"S lng tp nh v tp mask khng khp.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    # Hm thc hin resize nh v mask v kch thc mong mun (ban u)\n",
    "    def resize_image_and_mask(self, img, mask, target_size=IMG_SIZE):\n",
    "        img = cv2.resize(img, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
    "        mask = cv2.resize(mask, (target_size, target_size), interpolation=cv2.INTER_NEAREST)\n",
    "        return img, mask\n",
    "\n",
    "    # Hm thc hin ct ngu nhin cho nh v mask\n",
    "    def random_crop_image_and_mask(self, img, mask, crop_size=IMG_SIZE, min_scale=0.7):\n",
    "        h, w = img.shape[:2]\n",
    "        # Tnh ton kch thc crop ngu nhin, khng nh hn min_scale ca IMG_SIZE\n",
    "        # m bo kch thc ct khng ln hn kch thc hin ti ca nh\n",
    "        current_min_dim = min(h, w)\n",
    "        crop_h = crop_w = int(random.uniform(min_scale, 1.0) * crop_size)\n",
    "        \n",
    "        # m bo crop_h, crop_w khng ln hn kch thc hin ti ca nh\n",
    "        crop_h = min(crop_h, h)\n",
    "        crop_w = min(crop_w, w)\n",
    "\n",
    "        if h == crop_h and w == crop_w: # Khng cn ct nu kch thc  khp\n",
    "            return img, mask\n",
    "\n",
    "        # Chn im bt u ngu nhin\n",
    "        start_x = random.randint(0, w - crop_w)\n",
    "        start_y = random.randint(0, h - crop_h)\n",
    "\n",
    "        cropped_img = img[start_y:start_y + crop_h, start_x:start_x + crop_w]\n",
    "        cropped_mask = mask[start_y:start_y + crop_h, start_x:start_x + crop_w]\n",
    "        \n",
    "        # Resize li v kch thc IMG_SIZE sau khi ct\n",
    "        cropped_img = cv2.resize(cropped_img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "        cropped_mask = cv2.resize(cropped_mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        return cropped_img, cropped_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.image_filenames[idx])\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Khng th c tp nh: {self.image_filenames[idx]}\")\n",
    "\n",
    "        mask = cv2.imread(self.mask_filenames[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise ValueError(f\"Khng th c tp mask: {self.mask_filenames[idx]}\")\n",
    "\n",
    "        # Chuyn i nh sang RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # p dng Augmentation\n",
    "        if self.augment:\n",
    "            # Random Crop u tin  c cc phn khc nhau ca nh\n",
    "            # m bo nh  ln  crop, nu khng, resize trc\n",
    "            if img.shape[0] < IMG_SIZE or img.shape[1] < IMG_SIZE:\n",
    "                img, mask = self.resize_image_and_mask(img, mask, target_size=IMG_SIZE)\n",
    "            \n",
    "            # Ch thc hin crop nu nh  ln sau resize hoc nu ban u  ln\n",
    "            # Vi `min_scale=0.7`, chng ta s ct mt vng c kch thc t 0.7*IMG_SIZE n IMG_SIZE\n",
    "            img, mask = self.random_crop_image_and_mask(img, mask, crop_size=IMG_SIZE, min_scale=0.7)\n",
    "\n",
    "            # Lt Ngang\n",
    "            if random.random() < 0.5:\n",
    "                img = cv2.flip(img, 1)\n",
    "                mask = cv2.flip(mask, 1)\n",
    "            # Lt Dc\n",
    "            if random.random() < 0.5:\n",
    "                img = cv2.flip(img, 0)\n",
    "                mask = cv2.flip(mask, 0)\n",
    "        else:\n",
    "            # Nu khng augment, ch resize v ng kch thc IMG_SIZE\n",
    "            img, mask = self.resize_image_and_mask(img, mask, target_size=IMG_SIZE)\n",
    "\n",
    "        # Chun ha nh v mask v [0, 1]\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        mask = mask.astype(np.float32) / 255.0\n",
    "        \n",
    "        # m bo mask c chiu knh (1, H, W)\n",
    "        if mask.ndim == 2: # Nu mask vn l (H, W)\n",
    "            mask = np.expand_dims(mask, axis=0) # Thm chiu knh  v tr 0 -> (1, H, W)\n",
    "        elif mask.ndim == 3 and mask.shape[0] != 1: # Nu l (C, H, W) nhng C khng phi 1\n",
    "            if mask.shape[0] == 3: # Nu l 3 knh (v d, mask gc l RGB)\n",
    "                mask = mask[0:1, :, :] # Ly knh u tin\n",
    "            else:\n",
    "                raise ValueError(f\"Mask c hnh dng khng mong mun {mask.shape} ti ch s {idx} sau khi bin i. Expected channel dim 1.\")\n",
    "        \n",
    "        # Chuyn i t NumPy array sang PyTorch tensor\n",
    "        img_tensor = torch.from_numpy(img).permute(2, 0, 1) # nh: (H, W, C) -> (C, H, W)\n",
    "        mask_tensor = torch.from_numpy(mask) # Mask   (1, H, W)\n",
    "\n",
    "        return img_tensor, mask_tensor\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # u tin s dng nn.Upsample  trnh checkerboard artifacts\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), # Ln gp i kch thc khng gian\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1) # Gim s knh v out_channels\n",
    "        )\n",
    "        \n",
    "        # Kch thc u vo cho ConvBlock s l out_channels (t upsample) + skip_channels\n",
    "        self.conv_block = ConvBlock(out_channels + skip_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_features=None):\n",
    "        # Bc 1: Upsample u vo x t tng trc \n",
    "        x = self.upsample(x) \n",
    "        # Ti y, x  c kch thc khng gian ln hn (gp i so vi trc upsample)\n",
    "        # v s knh  c iu chnh v out_channels.\n",
    "        \n",
    "        if skip_features is not None:\n",
    "            # Bc 2: m bo kch thc khng gian ca skip_features khp vi x.\n",
    "            # y l im sa li: ni suy skip_features  n khp vi kch thc  c upsample ca x.\n",
    "            if x.shape[2:] != skip_features.shape[2:]:\n",
    "                skip_features = F.interpolate(skip_features, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "            \n",
    "            # Bc 3: Ni (concatenate) x v skip_features theo chiu knh\n",
    "            x = torch.cat([x, skip_features], dim=1)\n",
    "        \n",
    "        # Bc 4: Chy qua khi convolution  x l cc c trng  ni\n",
    "        x = self.conv_block(x)\n",
    "        return x\n",
    "    \n",
    "class SwinUNet(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.IMG_SIZE = IMG_SIZE\n",
    "\n",
    "        config = SwinConfig(image_size=self.IMG_SIZE, num_channels=input_channels,\n",
    "                            patch_size=4, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                            window_size=7, mlp_ratio=4., qkv_bias=True, hidden_dropout_prob=0.0,\n",
    "                            attention_probs_dropout_prob=0.0, drop_path_rate=0.1,\n",
    "                            hidden_act=\"gelu\", use_absolute_embeddings=False,\n",
    "                            patch_norm=True, initializer_range=0.02, layer_norm_eps=1e-05,\n",
    "                            out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"])\n",
    "        self.swin = SwinModel(config)\n",
    "\n",
    "        # Bottleneck: input t hidden_state[3] (768 knh), v hidden_state[4] l sau LayerNorm v c th khng cn thit\n",
    "        self.bottleneck = ConvBlock(config.embed_dim * 8, config.embed_dim * 8) # (768, 768)\n",
    "\n",
    "        # Decoder 4: in_channels t bottleneck (768), skip t hidden_state[3] (768 knh), ra 384 knh\n",
    "        self.decoder4 = DecoderBlock(in_channels=config.embed_dim * 8, skip_channels=config.embed_dim * 8, out_channels=config.embed_dim * 4) # 768, 768, 384\n",
    "        \n",
    "        # Decoder 3: in_channels t decoder4 (384), skip t hidden_state[2] (384 knh), ra 192 knh\n",
    "        self.decoder3 = DecoderBlock(in_channels=config.embed_dim * 4, skip_channels=config.embed_dim * 4, out_channels=config.embed_dim * 2) # 384, 384, 192\n",
    "        \n",
    "        # Decoder 2: in_channels t decoder3 (192), upsample to 96, skip t hidden_state[1] (192 knh)\n",
    "        self.decoder2 = DecoderBlock(in_channels=config.embed_dim * 2, skip_channels=config.embed_dim * 2, out_channels=config.embed_dim * 1) # 192, 192, 96\n",
    "        \n",
    "        # Decoder 1: in_channels t decoder2 (96), upsample to 48, skip t hidden_state[0] (96 knh)\n",
    "        self.decoder1 = DecoderBlock(in_channels=config.embed_dim * 1, skip_channels=config.embed_dim * 1, out_channels=config.embed_dim // 2) # 96, 96, 48\n",
    "        \n",
    "        # Final Upsample: upsample cui cng khng c skip\n",
    "        self.final_upsample = DecoderBlock(in_channels=config.embed_dim // 2, skip_channels=0, out_channels=config.embed_dim // 4) # 48, 0, 24\n",
    "        self.final_conv = nn.Conv2d(config.embed_dim // 4, num_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.swin(pixel_values=x, output_hidden_states=True)\n",
    "        # encoder_features s lu cc skip connections cn thit\n",
    "        encoder_features = []\n",
    "\n",
    "        # Cc hidden_states ca Swin-Tiny/Base:\n",
    "        # hidden_state[0]: (B, 4096, 96) -> reshape (B, 96, 64, 64)   (H/4)\n",
    "        # hidden_state[1]: (B, 1024, 192) -> reshape (B, 192, 32, 32) (H/8)\n",
    "        # hidden_state[2]: (B, 256, 384) -> reshape (B, 384, 16, 16) (H/16)\n",
    "        # hidden_state[3]: (B, 64, 768) -> reshape (B, 768, 8, 8)    (H/32)\n",
    "        # hidden_state[4]: (B, 64, 768) -> after final LayerNorm\n",
    "\n",
    "        # Trch xut v nh hnh li cc c trng t Swin Transformer cho cc skip connection:\n",
    "        # Skip connection cho decoder1 (H/4): hidden_state[0]\n",
    "        hs_skip_res_H4 = outputs.hidden_states[0] \n",
    "        b, n, c = hs_skip_res_H4.shape\n",
    "        s = int(np.sqrt(n))\n",
    "        encoder_features.append(hs_skip_res_H4.permute(0, 2, 1).reshape(b, c, s, s)) # index 0\n",
    "\n",
    "        # Skip connection cho decoder2 (H/8): hidden_state[1]\n",
    "        hs_skip_res_H8 = outputs.hidden_states[1] \n",
    "        b, n, c = hs_skip_res_H8.shape\n",
    "        s = int(np.sqrt(n))\n",
    "        encoder_features.append(hs_skip_res_H8.permute(0, 2, 1).reshape(b, c, s, s)) # index 1\n",
    "\n",
    "        # Skip connection cho decoder3 (H/16): hidden_state[2]\n",
    "        hs_skip_res_H16 = outputs.hidden_states[2] \n",
    "        b, n, c = hs_skip_res_H16.shape\n",
    "        s = int(np.sqrt(n))\n",
    "        encoder_features.append(hs_skip_res_H16.permute(0, 2, 1).reshape(b, c, s, s)) # index 2\n",
    "\n",
    "        # Skip connection cho decoder4 (H/32): hidden_state[3]\n",
    "        hs_skip_res_H32 = outputs.hidden_states[3] \n",
    "        b, n, c = hs_skip_res_H32.shape\n",
    "        s = int(np.sqrt(n))\n",
    "        encoder_features.append(hs_skip_res_H32.permute(0, 2, 1).reshape(b, c, s, s)) # index 3\n",
    "\n",
    "        # Bottleneck feature: t hidden_state[3] (768 knh, 8x8)\n",
    "        # LU : Nu hidden_state[4] l u ra sau cng ca Swin block, n c th l mt la chn tt cho bottleneck\n",
    "        # Tuy nhin, nu bn mun s dng hidden_state[3] lm c bottleneck v skip cho decoder4, iu  cng OK.\n",
    "        #  y, ti s s dng hidden_state[3] cho bottleneck  ng b vi skip connection cho decoder4.\n",
    "        x_bottleneck = outputs.hidden_states[3] \n",
    "        b, n, c = x_bottleneck.shape\n",
    "        s = int(np.sqrt(n))\n",
    "        x_bottleneck = x_bottleneck.permute(0, 2, 1).reshape(b, c, s, s)\n",
    "        \n",
    "        x = self.bottleneck(x_bottleneck)\n",
    "\n",
    "        # Gii m:\n",
    "        # decoder4: x_in t bottleneck (768), upsample to 384, skip t encoder_features[3] (768 knh)\n",
    "        x = self.decoder4(x, encoder_features[3]) \n",
    "        # x_out ca decoder4 l 384 knh, 16x16\n",
    "\n",
    "        # decoder3: x_in t decoder4 (384), upsample to 192, skip t encoder_features[2] (384 knh)\n",
    "        x = self.decoder3(x, encoder_features[2]) \n",
    "        # x_out ca decoder3 l 192 knh, 32x32\n",
    "\n",
    "        # decoder2: x_in t decoder3 (192), upsample to 96, skip t encoder_features[1] (192 knh)\n",
    "        x = self.decoder2(x, encoder_features[1]) \n",
    "        # x_out ca decoder2 l 96 knh, 64x64\n",
    "        \n",
    "        # decoder1: x_in t decoder2 (96), upsample to 48, skip t encoder_features[0] (96 knh)\n",
    "        x = self.decoder1(x, encoder_features[0]) \n",
    "        # x_out ca decoder1 l 48 knh, 128x128\n",
    "        \n",
    "        x = self.final_upsample(x)\n",
    "        outputs = self.final_conv(x)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# --- nh ngha IoU Loss ---\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(IoULoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs: prediction (sau sigmoid)\n",
    "        # targets: ground truth\n",
    "        \n",
    "        # Flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = (inputs * targets).sum()\n",
    "        union = (inputs + targets).sum() - intersection # Union = inputs.sum() + targets.sum() - intersection\n",
    "        \n",
    "        iou = (intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        return 1 - iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5e840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.iou = IoULoss(smooth=smooth) # Thay th DiceLoss bng IoULoss\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "\n",
    "        sigmoid_inputs = torch.sigmoid(inputs)\n",
    "        iou_loss = self.iou(sigmoid_inputs, targets)\n",
    "        \n",
    "        return iou_loss \n",
    "\n",
    "def calculate_metrics(predicted_masks, true_masks, smooth=1e-6):\n",
    "\n",
    "    intersection = (predicted_masks * true_masks).sum()\n",
    "    union = (predicted_masks + true_masks).sum() - intersection\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    dice = (2. * intersection + smooth) / ((predicted_masks.sum() + true_masks.sum()) + smooth)\n",
    "    f1_score = dice \n",
    "\n",
    "    return iou.item(), f1_score.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ed0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, callbacks_config, start_epoch=0, best_val_loss_so_far=float('inf')):\n",
    "    best_val_loss = best_val_loss_so_far \n",
    "    patience_counter = 0\n",
    "    model_checkpoint_path = callbacks_config.get('checkpoint_path', 'swin_unet_best_pytorch.pth')\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_iou = 0.0\n",
    "        running_f1 = 0.0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Bt u...\")\n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, masks) # criterion s x l sigmoid bn trong cho Dice loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            #  tnh metrics, cn p dng sigmoid cho outputs v sau  ngng\n",
    "            predicted_masks = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            batch_iou, batch_f1 = calculate_metrics(predicted_masks, masks)\n",
    "            running_iou += batch_iou * images.size(0)\n",
    "            running_f1 += batch_f1 * images.size(0)\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_iou = running_iou / len(train_loader.dataset)\n",
    "        epoch_f1 = running_f1 / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1} Kt thc - Mt mt Hun luyn: {epoch_loss:.4f}, IoU Hun luyn: {epoch_iou:.4f}, F1-Score Hun luyn: {epoch_f1:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_iou = 0.0\n",
    "        val_f1 = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(DEVICE)\n",
    "                masks = masks.to(DEVICE)\n",
    "\n",
    "                outputs = model(images) # outputs l logits\n",
    "                loss = criterion(outputs, masks) # criterion s x l sigmoid bn trong\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                #  tnh metrics, cn p dng sigmoid cho outputs v sau  ngng\n",
    "                predicted_masks = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                \n",
    "                batch_iou, batch_f1 = calculate_metrics(predicted_masks, masks)\n",
    "                val_iou += batch_iou * images.size(0)\n",
    "                val_f1 += batch_f1 * images.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_iou /= len(val_loader.dataset)\n",
    "        val_f1 /= len(val_loader.dataset)\n",
    "        print(f\"Mt mt Xc thc: {val_loss:.4f}, IoU Xc thc: {val_iou:.4f}, F1-Score Xc thc: {val_f1:.4f}\")\n",
    "        \n",
    "        # THAY I 2: Gi scheduler.step()\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Learning Rate hin ti: {current_lr:.8f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            print(f\"Mt mt xc thc tt nht c cp nht: {best_val_loss:.4f}. Lu m hnh v trng thi...\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                # THM scheduler_state_dict VO CHECKPOINT NU BN MUN TIP TC SCHEDULE SAU KHI LOAD\n",
    "                'scheduler_state_dict': scheduler.state_dict()\n",
    "            }, model_checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Mt mt xc thc khng ci thin. S kin nhn: {patience_counter}/{callbacks_config['patience']}\")\n",
    "            if patience_counter >= callbacks_config['patience']:\n",
    "                print(\"Dng sm!\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d4b7906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinUNet(\n",
      "  (swin): SwinModel(\n",
      "    (embeddings): SwinEmbeddings(\n",
      "      (patch_embeddings): SwinPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      )\n",
      "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): SwinEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (key): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (key): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.00909090880304575)\n",
      "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): SwinPatchMerging(\n",
      "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (key): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.0181818176060915)\n",
      "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (key): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.027272727340459824)\n",
      "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): SwinPatchMerging(\n",
      "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.036363635212183)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.045454543083906174)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.054545458406209946)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.06363636255264282)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.0727272778749466)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.08181818574666977)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): SwinPatchMerging(\n",
      "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.09090909361839294)\n",
      "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.10000000149011612)\n",
      "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
      "  )\n",
      "  (bottleneck): ConvBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder4): DecoderBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(1152, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder3): DecoderBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder2): DecoderBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(288, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder1): DecoderBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(144, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_upsample): DecoderBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (1): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_conv): Conv2d(24, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Khng tm thy checkpoint. Bt u hun luyn t u (Epoch 0).\n",
      "\n",
      "Bt u hun luyn m hnh Swin-Unet...\n",
      "Epoch 1/10000 Bt u...\n",
      "Epoch 1/10000, Batch 10/188, Loss: 0.9684\n",
      "Epoch 1/10000, Batch 20/188, Loss: 0.9420\n",
      "Epoch 1/10000, Batch 30/188, Loss: 0.9322\n",
      "Epoch 1/10000, Batch 40/188, Loss: 0.9484\n",
      "Epoch 1/10000, Batch 50/188, Loss: 0.9528\n",
      "Epoch 1/10000, Batch 60/188, Loss: 0.9771\n",
      "Epoch 1/10000, Batch 70/188, Loss: 0.9234\n",
      "Epoch 1/10000, Batch 80/188, Loss: 0.9110\n",
      "Epoch 1/10000, Batch 90/188, Loss: 0.8988\n",
      "Epoch 1/10000, Batch 100/188, Loss: 0.9435\n",
      "Epoch 1/10000, Batch 110/188, Loss: 0.9211\n",
      "Epoch 1/10000, Batch 120/188, Loss: 0.9145\n",
      "Epoch 1/10000, Batch 130/188, Loss: 0.9348\n",
      "Epoch 1/10000, Batch 140/188, Loss: 0.9454\n",
      "Epoch 1/10000, Batch 150/188, Loss: 0.9702\n",
      "Epoch 1/10000, Batch 160/188, Loss: 0.9614\n",
      "Epoch 1/10000, Batch 170/188, Loss: 0.9340\n",
      "Epoch 1/10000, Batch 180/188, Loss: 0.8518\n",
      "Epoch 1 Kt thc - Mt mt Hun luyn: 0.9329, IoU Hun luyn: 0.1561, F1-Score Hun luyn: 0.2611\n",
      "Mt mt Xc thc: 0.9527, IoU Xc thc: 0.1235, F1-Score Xc thc: 0.2086\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.9527. Lu m hnh v trng thi...\n",
      "Epoch 2/10000 Bt u...\n",
      "Epoch 2/10000, Batch 10/188, Loss: 0.9100\n",
      "Epoch 2/10000, Batch 20/188, Loss: 0.9277\n",
      "Epoch 2/10000, Batch 30/188, Loss: 0.9023\n",
      "Epoch 2/10000, Batch 40/188, Loss: 0.9443\n",
      "Epoch 2/10000, Batch 50/188, Loss: 0.9103\n",
      "Epoch 2/10000, Batch 60/188, Loss: 0.9193\n",
      "Epoch 2/10000, Batch 70/188, Loss: 0.9548\n",
      "Epoch 2/10000, Batch 80/188, Loss: 0.9426\n",
      "Epoch 2/10000, Batch 90/188, Loss: 0.9319\n",
      "Epoch 2/10000, Batch 100/188, Loss: 0.9747\n",
      "Epoch 2/10000, Batch 110/188, Loss: 0.9420\n",
      "Epoch 2/10000, Batch 120/188, Loss: 0.9363\n",
      "Epoch 2/10000, Batch 130/188, Loss: 0.9358\n",
      "Epoch 2/10000, Batch 140/188, Loss: 0.9527\n",
      "Epoch 2/10000, Batch 150/188, Loss: 0.8187\n",
      "Epoch 2/10000, Batch 160/188, Loss: 0.9615\n",
      "Epoch 2/10000, Batch 170/188, Loss: 0.9147\n",
      "Epoch 2/10000, Batch 180/188, Loss: 0.9499\n",
      "Epoch 2 Kt thc - Mt mt Hun luyn: 0.9132, IoU Hun luyn: 0.2635, F1-Score Hun luyn: 0.4053\n",
      "Mt mt Xc thc: 0.9394, IoU Xc thc: 0.1901, F1-Score Xc thc: 0.3032\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.9394. Lu m hnh v trng thi...\n",
      "Epoch 3/10000 Bt u...\n",
      "Epoch 3/10000, Batch 10/188, Loss: 0.9454\n",
      "Epoch 3/10000, Batch 20/188, Loss: 0.8714\n",
      "Epoch 3/10000, Batch 30/188, Loss: 0.9417\n",
      "Epoch 3/10000, Batch 40/188, Loss: 0.9377\n",
      "Epoch 3/10000, Batch 50/188, Loss: 0.9233\n",
      "Epoch 3/10000, Batch 60/188, Loss: 0.9131\n",
      "Epoch 3/10000, Batch 70/188, Loss: 0.8310\n",
      "Epoch 3/10000, Batch 80/188, Loss: 0.9236\n",
      "Epoch 3/10000, Batch 90/188, Loss: 0.9119\n",
      "Epoch 3/10000, Batch 100/188, Loss: 0.9407\n",
      "Epoch 3/10000, Batch 110/188, Loss: 0.9082\n",
      "Epoch 3/10000, Batch 120/188, Loss: 0.9027\n",
      "Epoch 3/10000, Batch 130/188, Loss: 0.9303\n",
      "Epoch 3/10000, Batch 140/188, Loss: 0.8954\n",
      "Epoch 3/10000, Batch 150/188, Loss: 0.8809\n",
      "Epoch 3/10000, Batch 160/188, Loss: 0.8930\n",
      "Epoch 3/10000, Batch 170/188, Loss: 0.8937\n",
      "Epoch 3/10000, Batch 180/188, Loss: 0.8386\n",
      "Epoch 3 Kt thc - Mt mt Hun luyn: 0.9026, IoU Hun luyn: 0.3044, F1-Score Hun luyn: 0.4545\n",
      "Mt mt Xc thc: 0.9236, IoU Xc thc: 0.2397, F1-Score Xc thc: 0.3590\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.9236. Lu m hnh v trng thi...\n",
      "Epoch 4/10000 Bt u...\n",
      "Epoch 4/10000, Batch 10/188, Loss: 0.9235\n",
      "Epoch 4/10000, Batch 20/188, Loss: 0.8111\n",
      "Epoch 4/10000, Batch 30/188, Loss: 0.8605\n",
      "Epoch 4/10000, Batch 40/188, Loss: 0.9747\n",
      "Epoch 4/10000, Batch 50/188, Loss: 0.9071\n",
      "Epoch 4/10000, Batch 60/188, Loss: 0.9305\n",
      "Epoch 4/10000, Batch 70/188, Loss: 0.9528\n",
      "Epoch 4/10000, Batch 80/188, Loss: 0.8793\n",
      "Epoch 4/10000, Batch 90/188, Loss: 0.8851\n",
      "Epoch 4/10000, Batch 100/188, Loss: 0.9341\n",
      "Epoch 4/10000, Batch 110/188, Loss: 0.8830\n",
      "Epoch 4/10000, Batch 120/188, Loss: 0.9422\n",
      "Epoch 4/10000, Batch 130/188, Loss: 0.8908\n",
      "Epoch 4/10000, Batch 140/188, Loss: 0.8794\n",
      "Epoch 4/10000, Batch 150/188, Loss: 0.8548\n",
      "Epoch 4/10000, Batch 160/188, Loss: 0.8970\n",
      "Epoch 4/10000, Batch 170/188, Loss: 0.8929\n",
      "Epoch 4/10000, Batch 180/188, Loss: 0.9303\n",
      "Epoch 4 Kt thc - Mt mt Hun luyn: 0.8887, IoU Hun luyn: 0.3369, F1-Score Hun luyn: 0.4907\n",
      "Mt mt Xc thc: 0.9468, IoU Xc thc: 0.0844, F1-Score Xc thc: 0.1502\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 1/30\n",
      "Epoch 5/10000 Bt u...\n",
      "Epoch 5/10000, Batch 10/188, Loss: 0.9137\n",
      "Epoch 5/10000, Batch 20/188, Loss: 0.8420\n",
      "Epoch 5/10000, Batch 30/188, Loss: 0.7997\n",
      "Epoch 5/10000, Batch 40/188, Loss: 0.8280\n",
      "Epoch 5/10000, Batch 50/188, Loss: 0.9178\n",
      "Epoch 5/10000, Batch 60/188, Loss: 0.9028\n",
      "Epoch 5/10000, Batch 70/188, Loss: 0.9075\n",
      "Epoch 5/10000, Batch 80/188, Loss: 0.8381\n",
      "Epoch 5/10000, Batch 90/188, Loss: 0.8456\n",
      "Epoch 5/10000, Batch 100/188, Loss: 0.8379\n",
      "Epoch 5/10000, Batch 110/188, Loss: 0.8701\n",
      "Epoch 5/10000, Batch 120/188, Loss: 0.9356\n",
      "Epoch 5/10000, Batch 130/188, Loss: 0.9415\n",
      "Epoch 5/10000, Batch 140/188, Loss: 0.8475\n",
      "Epoch 5/10000, Batch 150/188, Loss: 0.7348\n",
      "Epoch 5/10000, Batch 160/188, Loss: 0.8316\n",
      "Epoch 5/10000, Batch 170/188, Loss: 0.9301\n",
      "Epoch 5/10000, Batch 180/188, Loss: 0.9340\n",
      "Epoch 5 Kt thc - Mt mt Hun luyn: 0.8725, IoU Hun luyn: 0.3646, F1-Score Hun luyn: 0.5220\n",
      "Mt mt Xc thc: 0.9100, IoU Xc thc: 0.2089, F1-Score Xc thc: 0.3257\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.9100. Lu m hnh v trng thi...\n",
      "Epoch 6/10000 Bt u...\n",
      "Epoch 6/10000, Batch 10/188, Loss: 0.8569\n",
      "Epoch 6/10000, Batch 20/188, Loss: 0.8737\n",
      "Epoch 6/10000, Batch 30/188, Loss: 0.8866\n",
      "Epoch 6/10000, Batch 40/188, Loss: 0.9265\n",
      "Epoch 6/10000, Batch 50/188, Loss: 0.6915\n",
      "Epoch 6/10000, Batch 60/188, Loss: 0.9222\n",
      "Epoch 6/10000, Batch 70/188, Loss: 0.8181\n",
      "Epoch 6/10000, Batch 80/188, Loss: 0.8032\n",
      "Epoch 6/10000, Batch 90/188, Loss: 0.7941\n",
      "Epoch 6/10000, Batch 100/188, Loss: 0.8510\n",
      "Epoch 6/10000, Batch 110/188, Loss: 0.7863\n",
      "Epoch 6/10000, Batch 120/188, Loss: 0.8987\n",
      "Epoch 6/10000, Batch 130/188, Loss: 0.8449\n",
      "Epoch 6/10000, Batch 140/188, Loss: 0.8938\n",
      "Epoch 6/10000, Batch 150/188, Loss: 0.8002\n",
      "Epoch 6/10000, Batch 160/188, Loss: 0.8282\n",
      "Epoch 6/10000, Batch 170/188, Loss: 0.9184\n",
      "Epoch 6/10000, Batch 180/188, Loss: 0.8789\n",
      "Epoch 6 Kt thc - Mt mt Hun luyn: 0.8556, IoU Hun luyn: 0.3815, F1-Score Hun luyn: 0.5400\n",
      "Mt mt Xc thc: 0.8935, IoU Xc thc: 0.2985, F1-Score Xc thc: 0.4244\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.8935. Lu m hnh v trng thi...\n",
      "Epoch 7/10000 Bt u...\n",
      "Epoch 7/10000, Batch 10/188, Loss: 0.7866\n",
      "Epoch 7/10000, Batch 20/188, Loss: 0.8810\n",
      "Epoch 7/10000, Batch 30/188, Loss: 0.8132\n",
      "Epoch 7/10000, Batch 40/188, Loss: 0.7754\n",
      "Epoch 7/10000, Batch 50/188, Loss: 0.8826\n",
      "Epoch 7/10000, Batch 60/188, Loss: 0.7713\n",
      "Epoch 7/10000, Batch 70/188, Loss: 0.7407\n",
      "Epoch 7/10000, Batch 80/188, Loss: 0.9011\n",
      "Epoch 7/10000, Batch 90/188, Loss: 0.9300\n",
      "Epoch 7/10000, Batch 100/188, Loss: 0.7416\n",
      "Epoch 7/10000, Batch 110/188, Loss: 0.8869\n",
      "Epoch 7/10000, Batch 120/188, Loss: 0.7706\n",
      "Epoch 7/10000, Batch 130/188, Loss: 0.8939\n",
      "Epoch 7/10000, Batch 140/188, Loss: 0.9188\n",
      "Epoch 7/10000, Batch 150/188, Loss: 0.7604\n",
      "Epoch 7/10000, Batch 160/188, Loss: 0.8165\n",
      "Epoch 7/10000, Batch 170/188, Loss: 0.8187\n",
      "Epoch 7/10000, Batch 180/188, Loss: 0.7580\n",
      "Epoch 7 Kt thc - Mt mt Hun luyn: 0.8286, IoU Hun luyn: 0.4142, F1-Score Hun luyn: 0.5735\n",
      "Mt mt Xc thc: 0.8851, IoU Xc thc: 0.2461, F1-Score Xc thc: 0.3694\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.8851. Lu m hnh v trng thi...\n",
      "Epoch 8/10000 Bt u...\n",
      "Epoch 8/10000, Batch 10/188, Loss: 0.7033\n",
      "Epoch 8/10000, Batch 20/188, Loss: 0.8574\n",
      "Epoch 8/10000, Batch 30/188, Loss: 0.8431\n",
      "Epoch 8/10000, Batch 40/188, Loss: 0.8681\n",
      "Epoch 8/10000, Batch 50/188, Loss: 0.7901\n",
      "Epoch 8/10000, Batch 60/188, Loss: 0.8592\n",
      "Epoch 8/10000, Batch 70/188, Loss: 0.7529\n",
      "Epoch 8/10000, Batch 80/188, Loss: 0.7585\n",
      "Epoch 8/10000, Batch 90/188, Loss: 0.7661\n",
      "Epoch 8/10000, Batch 100/188, Loss: 0.8606\n",
      "Epoch 8/10000, Batch 110/188, Loss: 0.8273\n",
      "Epoch 8/10000, Batch 120/188, Loss: 0.8542\n",
      "Epoch 8/10000, Batch 130/188, Loss: 0.8009\n",
      "Epoch 8/10000, Batch 140/188, Loss: 0.7051\n",
      "Epoch 8/10000, Batch 150/188, Loss: 0.7835\n",
      "Epoch 8/10000, Batch 160/188, Loss: 0.6942\n",
      "Epoch 8/10000, Batch 170/188, Loss: 0.8597\n",
      "Epoch 8/10000, Batch 180/188, Loss: 0.7607\n",
      "Epoch 8 Kt thc - Mt mt Hun luyn: 0.8066, IoU Hun luyn: 0.4305, F1-Score Hun luyn: 0.5916\n",
      "Mt mt Xc thc: 0.8639, IoU Xc thc: 0.2845, F1-Score Xc thc: 0.4063\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.8639. Lu m hnh v trng thi...\n",
      "Epoch 9/10000 Bt u...\n",
      "Epoch 9/10000, Batch 10/188, Loss: 0.7608\n",
      "Epoch 9/10000, Batch 20/188, Loss: 0.7214\n",
      "Epoch 9/10000, Batch 30/188, Loss: 0.7360\n",
      "Epoch 9/10000, Batch 40/188, Loss: 0.8025\n",
      "Epoch 9/10000, Batch 50/188, Loss: 0.7613\n",
      "Epoch 9/10000, Batch 60/188, Loss: 0.8057\n",
      "Epoch 9/10000, Batch 70/188, Loss: 0.8148\n",
      "Epoch 9/10000, Batch 80/188, Loss: 0.7892\n",
      "Epoch 9/10000, Batch 90/188, Loss: 0.7791\n",
      "Epoch 9/10000, Batch 100/188, Loss: 0.7213\n",
      "Epoch 9/10000, Batch 110/188, Loss: 0.8392\n",
      "Epoch 9/10000, Batch 120/188, Loss: 0.7931\n",
      "Epoch 9/10000, Batch 130/188, Loss: 0.8454\n",
      "Epoch 9/10000, Batch 140/188, Loss: 0.7794\n",
      "Epoch 9/10000, Batch 150/188, Loss: 0.7330\n",
      "Epoch 9/10000, Batch 160/188, Loss: 0.8506\n",
      "Epoch 9/10000, Batch 170/188, Loss: 0.7771\n",
      "Epoch 9/10000, Batch 180/188, Loss: 0.7940\n",
      "Epoch 9 Kt thc - Mt mt Hun luyn: 0.7732, IoU Hun luyn: 0.4567, F1-Score Hun luyn: 0.6170\n",
      "Mt mt Xc thc: 0.8317, IoU Xc thc: 0.3168, F1-Score Xc thc: 0.4462\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.8317. Lu m hnh v trng thi...\n",
      "Epoch 10/10000 Bt u...\n",
      "Epoch 10/10000, Batch 10/188, Loss: 0.6683\n",
      "Epoch 10/10000, Batch 20/188, Loss: 0.7633\n",
      "Epoch 10/10000, Batch 30/188, Loss: 0.5767\n",
      "Epoch 10/10000, Batch 40/188, Loss: 0.6744\n",
      "Epoch 10/10000, Batch 50/188, Loss: 0.6719\n",
      "Epoch 10/10000, Batch 60/188, Loss: 0.7164\n",
      "Epoch 10/10000, Batch 70/188, Loss: 0.8104\n",
      "Epoch 10/10000, Batch 80/188, Loss: 0.6538\n",
      "Epoch 10/10000, Batch 90/188, Loss: 0.7374\n",
      "Epoch 10/10000, Batch 100/188, Loss: 0.6331\n",
      "Epoch 10/10000, Batch 110/188, Loss: 0.6299\n",
      "Epoch 10/10000, Batch 120/188, Loss: 0.9547\n",
      "Epoch 10/10000, Batch 130/188, Loss: 0.7881\n",
      "Epoch 10/10000, Batch 140/188, Loss: 0.8487\n",
      "Epoch 10/10000, Batch 150/188, Loss: 0.8082\n",
      "Epoch 10/10000, Batch 160/188, Loss: 0.6916\n",
      "Epoch 10/10000, Batch 170/188, Loss: 0.7516\n",
      "Epoch 10/10000, Batch 180/188, Loss: 0.6093\n",
      "Epoch 10 Kt thc - Mt mt Hun luyn: 0.7419, IoU Hun luyn: 0.4654, F1-Score Hun luyn: 0.6248\n",
      "Mt mt Xc thc: 0.8383, IoU Xc thc: 0.2444, F1-Score Xc thc: 0.3684\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 1/30\n",
      "Epoch 11/10000 Bt u...\n",
      "Epoch 11/10000, Batch 10/188, Loss: 0.8214\n",
      "Epoch 11/10000, Batch 20/188, Loss: 0.7497\n",
      "Epoch 11/10000, Batch 30/188, Loss: 0.6754\n",
      "Epoch 11/10000, Batch 40/188, Loss: 0.6595\n",
      "Epoch 11/10000, Batch 50/188, Loss: 0.5759\n",
      "Epoch 11/10000, Batch 60/188, Loss: 0.5835\n",
      "Epoch 11/10000, Batch 70/188, Loss: 0.8039\n",
      "Epoch 11/10000, Batch 80/188, Loss: 0.8010\n",
      "Epoch 11/10000, Batch 90/188, Loss: 0.8067\n",
      "Epoch 11/10000, Batch 100/188, Loss: 0.5378\n",
      "Epoch 11/10000, Batch 110/188, Loss: 0.7909\n",
      "Epoch 11/10000, Batch 120/188, Loss: 0.7167\n",
      "Epoch 11/10000, Batch 130/188, Loss: 0.8672\n",
      "Epoch 11/10000, Batch 140/188, Loss: 0.6190\n",
      "Epoch 11/10000, Batch 150/188, Loss: 0.8044\n",
      "Epoch 11/10000, Batch 160/188, Loss: 0.7622\n",
      "Epoch 11/10000, Batch 170/188, Loss: 0.8325\n",
      "Epoch 11/10000, Batch 180/188, Loss: 0.6067\n",
      "Epoch 11 Kt thc - Mt mt Hun luyn: 0.7106, IoU Hun luyn: 0.4735, F1-Score Hun luyn: 0.6328\n",
      "Mt mt Xc thc: 0.8023, IoU Xc thc: 0.3171, F1-Score Xc thc: 0.4363\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.8023. Lu m hnh v trng thi...\n",
      "Epoch 12/10000 Bt u...\n",
      "Epoch 12/10000, Batch 10/188, Loss: 0.6793\n",
      "Epoch 12/10000, Batch 20/188, Loss: 0.4770\n",
      "Epoch 12/10000, Batch 30/188, Loss: 0.7301\n",
      "Epoch 12/10000, Batch 40/188, Loss: 0.7120\n",
      "Epoch 12/10000, Batch 50/188, Loss: 0.5581\n",
      "Epoch 12/10000, Batch 60/188, Loss: 0.7871\n",
      "Epoch 12/10000, Batch 70/188, Loss: 0.7388\n",
      "Epoch 12/10000, Batch 80/188, Loss: 0.7243\n",
      "Epoch 12/10000, Batch 90/188, Loss: 0.6673\n",
      "Epoch 12/10000, Batch 100/188, Loss: 0.8302\n",
      "Epoch 12/10000, Batch 110/188, Loss: 0.6531\n",
      "Epoch 12/10000, Batch 120/188, Loss: 0.8489\n",
      "Epoch 12/10000, Batch 130/188, Loss: 0.7092\n",
      "Epoch 12/10000, Batch 140/188, Loss: 0.7047\n",
      "Epoch 12/10000, Batch 150/188, Loss: 0.4925\n",
      "Epoch 12/10000, Batch 160/188, Loss: 0.6024\n",
      "Epoch 12/10000, Batch 170/188, Loss: 0.5827\n",
      "Epoch 12/10000, Batch 180/188, Loss: 0.5466\n",
      "Epoch 12 Kt thc - Mt mt Hun luyn: 0.6710, IoU Hun luyn: 0.4944, F1-Score Hun luyn: 0.6537\n",
      "Mt mt Xc thc: 0.8026, IoU Xc thc: 0.2957, F1-Score Xc thc: 0.4047\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 1/30\n",
      "Epoch 13/10000 Bt u...\n",
      "Epoch 13/10000, Batch 10/188, Loss: 0.5996\n",
      "Epoch 13/10000, Batch 20/188, Loss: 0.5355\n",
      "Epoch 13/10000, Batch 30/188, Loss: 0.6004\n",
      "Epoch 13/10000, Batch 40/188, Loss: 0.5953\n",
      "Epoch 13/10000, Batch 50/188, Loss: 0.6371\n",
      "Epoch 13/10000, Batch 60/188, Loss: 0.7168\n",
      "Epoch 13/10000, Batch 70/188, Loss: 0.6012\n",
      "Epoch 13/10000, Batch 80/188, Loss: 0.7108\n",
      "Epoch 13/10000, Batch 90/188, Loss: 0.7234\n",
      "Epoch 13/10000, Batch 100/188, Loss: 0.5789\n",
      "Epoch 13/10000, Batch 110/188, Loss: 0.6873\n",
      "Epoch 13/10000, Batch 120/188, Loss: 0.5903\n",
      "Epoch 13/10000, Batch 130/188, Loss: 0.5635\n",
      "Epoch 13/10000, Batch 140/188, Loss: 0.6509\n",
      "Epoch 13/10000, Batch 150/188, Loss: 0.6467\n",
      "Epoch 13/10000, Batch 160/188, Loss: 0.6473\n",
      "Epoch 13/10000, Batch 170/188, Loss: 0.6804\n",
      "Epoch 13/10000, Batch 180/188, Loss: 0.7282\n",
      "Epoch 13 Kt thc - Mt mt Hun luyn: 0.6455, IoU Hun luyn: 0.5001, F1-Score Hun luyn: 0.6577\n",
      "Mt mt Xc thc: 0.7522, IoU Xc thc: 0.3345, F1-Score Xc thc: 0.4603\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.7522. Lu m hnh v trng thi...\n",
      "Epoch 14/10000 Bt u...\n",
      "Epoch 14/10000, Batch 10/188, Loss: 0.8632\n",
      "Epoch 14/10000, Batch 20/188, Loss: 0.4694\n",
      "Epoch 14/10000, Batch 30/188, Loss: 0.6805\n",
      "Epoch 14/10000, Batch 40/188, Loss: 0.6779\n",
      "Epoch 14/10000, Batch 50/188, Loss: 0.6486\n",
      "Epoch 14/10000, Batch 60/188, Loss: 0.4191\n",
      "Epoch 14/10000, Batch 70/188, Loss: 0.7254\n",
      "Epoch 14/10000, Batch 80/188, Loss: 0.5534\n",
      "Epoch 14/10000, Batch 90/188, Loss: 0.6614\n",
      "Epoch 14/10000, Batch 100/188, Loss: 0.4817\n",
      "Epoch 14/10000, Batch 110/188, Loss: 0.4278\n",
      "Epoch 14/10000, Batch 120/188, Loss: 0.5627\n",
      "Epoch 14/10000, Batch 130/188, Loss: 0.4203\n",
      "Epoch 14/10000, Batch 140/188, Loss: 0.4662\n",
      "Epoch 14/10000, Batch 150/188, Loss: 0.5615\n",
      "Epoch 14/10000, Batch 160/188, Loss: 0.7162\n",
      "Epoch 14/10000, Batch 170/188, Loss: 0.8034\n",
      "Epoch 14/10000, Batch 180/188, Loss: 0.5038\n",
      "Epoch 14 Kt thc - Mt mt Hun luyn: 0.6252, IoU Hun luyn: 0.4929, F1-Score Hun luyn: 0.6492\n",
      "Mt mt Xc thc: 0.7411, IoU Xc thc: 0.3278, F1-Score Xc thc: 0.4573\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.7411. Lu m hnh v trng thi...\n",
      "Epoch 15/10000 Bt u...\n",
      "Epoch 15/10000, Batch 10/188, Loss: 0.4905\n",
      "Epoch 15/10000, Batch 20/188, Loss: 0.4182\n",
      "Epoch 15/10000, Batch 30/188, Loss: 0.5977\n",
      "Epoch 15/10000, Batch 40/188, Loss: 0.6068\n",
      "Epoch 15/10000, Batch 50/188, Loss: 0.4566\n",
      "Epoch 15/10000, Batch 60/188, Loss: 0.5754\n",
      "Epoch 15/10000, Batch 70/188, Loss: 0.6627\n",
      "Epoch 15/10000, Batch 80/188, Loss: 0.7702\n",
      "Epoch 15/10000, Batch 90/188, Loss: 0.5465\n",
      "Epoch 15/10000, Batch 100/188, Loss: 0.5214\n",
      "Epoch 15/10000, Batch 110/188, Loss: 0.7157\n",
      "Epoch 15/10000, Batch 120/188, Loss: 0.5829\n",
      "Epoch 15/10000, Batch 130/188, Loss: 0.5568\n",
      "Epoch 15/10000, Batch 140/188, Loss: 0.6459\n",
      "Epoch 15/10000, Batch 150/188, Loss: 0.7544\n",
      "Epoch 15/10000, Batch 160/188, Loss: 0.5882\n",
      "Epoch 15/10000, Batch 170/188, Loss: 0.6943\n",
      "Epoch 15/10000, Batch 180/188, Loss: 0.5605\n",
      "Epoch 15 Kt thc - Mt mt Hun luyn: 0.5950, IoU Hun luyn: 0.5053, F1-Score Hun luyn: 0.6622\n",
      "Mt mt Xc thc: 0.7319, IoU Xc thc: 0.3297, F1-Score Xc thc: 0.4528\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.7319. Lu m hnh v trng thi...\n",
      "Epoch 16/10000 Bt u...\n",
      "Epoch 16/10000, Batch 10/188, Loss: 0.6220\n",
      "Epoch 16/10000, Batch 20/188, Loss: 0.5552\n",
      "Epoch 16/10000, Batch 30/188, Loss: 0.5373\n",
      "Epoch 16/10000, Batch 40/188, Loss: 0.4167\n",
      "Epoch 16/10000, Batch 50/188, Loss: 0.5635\n",
      "Epoch 16/10000, Batch 60/188, Loss: 0.4090\n",
      "Epoch 16/10000, Batch 70/188, Loss: 0.4690\n",
      "Epoch 16/10000, Batch 80/188, Loss: 0.5440\n",
      "Epoch 16/10000, Batch 90/188, Loss: 0.6176\n",
      "Epoch 16/10000, Batch 100/188, Loss: 0.5107\n",
      "Epoch 16/10000, Batch 110/188, Loss: 0.6204\n",
      "Epoch 16/10000, Batch 120/188, Loss: 0.5588\n",
      "Epoch 16/10000, Batch 130/188, Loss: 0.5087\n",
      "Epoch 16/10000, Batch 140/188, Loss: 0.6548\n",
      "Epoch 16/10000, Batch 150/188, Loss: 0.3572\n",
      "Epoch 16/10000, Batch 160/188, Loss: 0.5788\n",
      "Epoch 16/10000, Batch 170/188, Loss: 0.4526\n",
      "Epoch 16/10000, Batch 180/188, Loss: 0.6280\n",
      "Epoch 16 Kt thc - Mt mt Hun luyn: 0.5714, IoU Hun luyn: 0.5142, F1-Score Hun luyn: 0.6706\n",
      "Mt mt Xc thc: 0.7077, IoU Xc thc: 0.3476, F1-Score Xc thc: 0.4732\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.7077. Lu m hnh v trng thi...\n",
      "Epoch 17/10000 Bt u...\n",
      "Epoch 17/10000, Batch 10/188, Loss: 0.6579\n",
      "Epoch 17/10000, Batch 20/188, Loss: 0.5719\n",
      "Epoch 17/10000, Batch 30/188, Loss: 0.4334\n",
      "Epoch 17/10000, Batch 40/188, Loss: 0.4295\n",
      "Epoch 17/10000, Batch 50/188, Loss: 0.7381\n",
      "Epoch 17/10000, Batch 60/188, Loss: 0.4410\n",
      "Epoch 17/10000, Batch 70/188, Loss: 0.6258\n",
      "Epoch 17/10000, Batch 80/188, Loss: 0.5063\n",
      "Epoch 17/10000, Batch 90/188, Loss: 0.3479\n",
      "Epoch 17/10000, Batch 100/188, Loss: 0.7560\n",
      "Epoch 17/10000, Batch 110/188, Loss: 0.5685\n",
      "Epoch 17/10000, Batch 120/188, Loss: 0.5214\n",
      "Epoch 17/10000, Batch 130/188, Loss: 0.4985\n",
      "Epoch 17/10000, Batch 140/188, Loss: 0.7304\n",
      "Epoch 17/10000, Batch 150/188, Loss: 0.5975\n",
      "Epoch 17/10000, Batch 160/188, Loss: 0.6575\n",
      "Epoch 17/10000, Batch 170/188, Loss: 0.4733\n",
      "Epoch 17/10000, Batch 180/188, Loss: 0.5701\n",
      "Epoch 17 Kt thc - Mt mt Hun luyn: 0.5543, IoU Hun luyn: 0.5187, F1-Score Hun luyn: 0.6735\n",
      "Mt mt Xc thc: 0.7014, IoU Xc thc: 0.3417, F1-Score Xc thc: 0.4676\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.7014. Lu m hnh v trng thi...\n",
      "Epoch 18/10000 Bt u...\n",
      "Epoch 18/10000, Batch 10/188, Loss: 0.5191\n",
      "Epoch 18/10000, Batch 20/188, Loss: 0.6022\n",
      "Epoch 18/10000, Batch 30/188, Loss: 0.3791\n",
      "Epoch 18/10000, Batch 40/188, Loss: 0.6363\n",
      "Epoch 18/10000, Batch 50/188, Loss: 0.5513\n",
      "Epoch 18/10000, Batch 60/188, Loss: 0.6790\n",
      "Epoch 18/10000, Batch 70/188, Loss: 0.4780\n",
      "Epoch 18/10000, Batch 80/188, Loss: 0.5037\n",
      "Epoch 18/10000, Batch 90/188, Loss: 0.6406\n",
      "Epoch 18/10000, Batch 100/188, Loss: 0.4481\n",
      "Epoch 18/10000, Batch 110/188, Loss: 0.4716\n",
      "Epoch 18/10000, Batch 120/188, Loss: 0.5269\n",
      "Epoch 18/10000, Batch 130/188, Loss: 0.5296\n",
      "Epoch 18/10000, Batch 140/188, Loss: 0.5914\n",
      "Epoch 18/10000, Batch 150/188, Loss: 0.4750\n",
      "Epoch 18/10000, Batch 160/188, Loss: 0.5790\n",
      "Epoch 18/10000, Batch 170/188, Loss: 0.5461\n",
      "Epoch 18/10000, Batch 180/188, Loss: 0.6123\n",
      "Epoch 18 Kt thc - Mt mt Hun luyn: 0.5400, IoU Hun luyn: 0.5213, F1-Score Hun luyn: 0.6787\n",
      "Mt mt Xc thc: 0.6981, IoU Xc thc: 0.3362, F1-Score Xc thc: 0.4633\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.6981. Lu m hnh v trng thi...\n",
      "Epoch 19/10000 Bt u...\n",
      "Epoch 19/10000, Batch 10/188, Loss: 0.4067\n",
      "Epoch 19/10000, Batch 20/188, Loss: 0.4986\n",
      "Epoch 19/10000, Batch 30/188, Loss: 0.4642\n",
      "Epoch 19/10000, Batch 40/188, Loss: 0.7227\n",
      "Epoch 19/10000, Batch 50/188, Loss: 0.5606\n",
      "Epoch 19/10000, Batch 60/188, Loss: 0.6302\n",
      "Epoch 19/10000, Batch 70/188, Loss: 0.4669\n",
      "Epoch 19/10000, Batch 80/188, Loss: 0.3166\n",
      "Epoch 19/10000, Batch 90/188, Loss: 0.6762\n",
      "Epoch 19/10000, Batch 100/188, Loss: 0.7832\n",
      "Epoch 19/10000, Batch 110/188, Loss: 0.5145\n",
      "Epoch 19/10000, Batch 120/188, Loss: 0.4952\n",
      "Epoch 19/10000, Batch 130/188, Loss: 0.4849\n",
      "Epoch 19/10000, Batch 140/188, Loss: 0.4808\n",
      "Epoch 19/10000, Batch 150/188, Loss: 0.5814\n",
      "Epoch 19/10000, Batch 160/188, Loss: 0.3174\n",
      "Epoch 19/10000, Batch 170/188, Loss: 0.4776\n",
      "Epoch 19/10000, Batch 180/188, Loss: 0.4908\n",
      "Epoch 19 Kt thc - Mt mt Hun luyn: 0.5302, IoU Hun luyn: 0.5213, F1-Score Hun luyn: 0.6759\n",
      "Mt mt Xc thc: 0.6840, IoU Xc thc: 0.3525, F1-Score Xc thc: 0.4806\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.6840. Lu m hnh v trng thi...\n",
      "Epoch 20/10000 Bt u...\n",
      "Epoch 20/10000, Batch 10/188, Loss: 0.4990\n",
      "Epoch 20/10000, Batch 20/188, Loss: 0.7285\n",
      "Epoch 20/10000, Batch 30/188, Loss: 0.4955\n",
      "Epoch 20/10000, Batch 40/188, Loss: 0.6995\n",
      "Epoch 20/10000, Batch 50/188, Loss: 0.5364\n",
      "Epoch 20/10000, Batch 60/188, Loss: 0.5093\n",
      "Epoch 20/10000, Batch 70/188, Loss: 0.4360\n",
      "Epoch 20/10000, Batch 80/188, Loss: 0.6721\n",
      "Epoch 20/10000, Batch 90/188, Loss: 0.6061\n",
      "Epoch 20/10000, Batch 100/188, Loss: 0.5186\n",
      "Epoch 20/10000, Batch 110/188, Loss: 0.6496\n",
      "Epoch 20/10000, Batch 120/188, Loss: 0.5889\n",
      "Epoch 20/10000, Batch 130/188, Loss: 0.4864\n",
      "Epoch 20/10000, Batch 140/188, Loss: 0.5484\n",
      "Epoch 20/10000, Batch 150/188, Loss: 0.5887\n",
      "Epoch 20/10000, Batch 160/188, Loss: 0.3028\n",
      "Epoch 20/10000, Batch 170/188, Loss: 0.5450\n",
      "Epoch 20/10000, Batch 180/188, Loss: 0.5574\n",
      "Epoch 20 Kt thc - Mt mt Hun luyn: 0.5255, IoU Hun luyn: 0.5189, F1-Score Hun luyn: 0.6750\n",
      "Mt mt Xc thc: 0.6915, IoU Xc thc: 0.3341, F1-Score Xc thc: 0.4574\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 1/30\n",
      "Epoch 21/10000 Bt u...\n",
      "Epoch 21/10000, Batch 10/188, Loss: 0.5419\n",
      "Epoch 21/10000, Batch 20/188, Loss: 0.4289\n",
      "Epoch 21/10000, Batch 30/188, Loss: 0.4423\n",
      "Epoch 21/10000, Batch 40/188, Loss: 0.2475\n",
      "Epoch 21/10000, Batch 50/188, Loss: 0.4856\n",
      "Epoch 21/10000, Batch 60/188, Loss: 0.4020\n",
      "Epoch 21/10000, Batch 70/188, Loss: 0.7513\n",
      "Epoch 21/10000, Batch 80/188, Loss: 0.4764\n",
      "Epoch 21/10000, Batch 90/188, Loss: 0.8507\n",
      "Epoch 21/10000, Batch 100/188, Loss: 0.5816\n",
      "Epoch 21/10000, Batch 110/188, Loss: 0.6728\n",
      "Epoch 21/10000, Batch 120/188, Loss: 0.6409\n",
      "Epoch 21/10000, Batch 130/188, Loss: 0.3582\n",
      "Epoch 21/10000, Batch 140/188, Loss: 0.7081\n",
      "Epoch 21/10000, Batch 150/188, Loss: 0.6383\n",
      "Epoch 21/10000, Batch 160/188, Loss: 0.5558\n",
      "Epoch 21/10000, Batch 170/188, Loss: 0.6436\n",
      "Epoch 21/10000, Batch 180/188, Loss: 0.4589\n",
      "Epoch 21 Kt thc - Mt mt Hun luyn: 0.5081, IoU Hun luyn: 0.5301, F1-Score Hun luyn: 0.6842\n",
      "Mt mt Xc thc: 0.6678, IoU Xc thc: 0.3583, F1-Score Xc thc: 0.4843\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.6678. Lu m hnh v trng thi...\n",
      "Epoch 22/10000 Bt u...\n",
      "Epoch 22/10000, Batch 10/188, Loss: 0.3671\n",
      "Epoch 22/10000, Batch 20/188, Loss: 0.6971\n",
      "Epoch 22/10000, Batch 30/188, Loss: 0.4680\n",
      "Epoch 22/10000, Batch 40/188, Loss: 0.6317\n",
      "Epoch 22/10000, Batch 50/188, Loss: 0.6275\n",
      "Epoch 22/10000, Batch 60/188, Loss: 0.8626\n",
      "Epoch 22/10000, Batch 70/188, Loss: 0.4232\n",
      "Epoch 22/10000, Batch 80/188, Loss: 0.5484\n",
      "Epoch 22/10000, Batch 90/188, Loss: 0.3531\n",
      "Epoch 22/10000, Batch 100/188, Loss: 0.3550\n",
      "Epoch 22/10000, Batch 110/188, Loss: 0.4426\n",
      "Epoch 22/10000, Batch 120/188, Loss: 0.5955\n",
      "Epoch 22/10000, Batch 130/188, Loss: 0.6195\n",
      "Epoch 22/10000, Batch 140/188, Loss: 0.5286\n",
      "Epoch 22/10000, Batch 150/188, Loss: 0.4412\n",
      "Epoch 22/10000, Batch 160/188, Loss: 0.5248\n",
      "Epoch 22/10000, Batch 170/188, Loss: 0.5153\n",
      "Epoch 22/10000, Batch 180/188, Loss: 0.5735\n",
      "Epoch 22 Kt thc - Mt mt Hun luyn: 0.4985, IoU Hun luyn: 0.5341, F1-Score Hun luyn: 0.6871\n",
      "Mt mt Xc thc: 0.6782, IoU Xc thc: 0.3426, F1-Score Xc thc: 0.4591\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 1/30\n",
      "Epoch 23/10000 Bt u...\n",
      "Epoch 23/10000, Batch 10/188, Loss: 0.5759\n",
      "Epoch 23/10000, Batch 20/188, Loss: 0.4090\n",
      "Epoch 23/10000, Batch 30/188, Loss: 0.4303\n",
      "Epoch 23/10000, Batch 40/188, Loss: 0.4502\n",
      "Epoch 23/10000, Batch 50/188, Loss: 0.3029\n",
      "Epoch 23/10000, Batch 60/188, Loss: 0.4860\n",
      "Epoch 23/10000, Batch 70/188, Loss: 0.3124\n",
      "Epoch 23/10000, Batch 80/188, Loss: 0.5463\n",
      "Epoch 23/10000, Batch 90/188, Loss: 0.5191\n",
      "Epoch 23/10000, Batch 100/188, Loss: 0.3447\n",
      "Epoch 23/10000, Batch 110/188, Loss: 0.4295\n",
      "Epoch 23/10000, Batch 120/188, Loss: 0.5587\n",
      "Epoch 23/10000, Batch 130/188, Loss: 0.3906\n",
      "Epoch 23/10000, Batch 140/188, Loss: 0.4864\n",
      "Epoch 23/10000, Batch 150/188, Loss: 0.4020\n",
      "Epoch 23/10000, Batch 160/188, Loss: 0.4958\n",
      "Epoch 23/10000, Batch 170/188, Loss: 0.5624\n",
      "Epoch 23/10000, Batch 180/188, Loss: 0.5027\n",
      "Epoch 23 Kt thc - Mt mt Hun luyn: 0.4911, IoU Hun luyn: 0.5377, F1-Score Hun luyn: 0.6922\n",
      "Mt mt Xc thc: 0.6622, IoU Xc thc: 0.3566, F1-Score Xc thc: 0.4820\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.6622. Lu m hnh v trng thi...\n",
      "Epoch 24/10000 Bt u...\n",
      "Epoch 24/10000, Batch 10/188, Loss: 0.5505\n",
      "Epoch 24/10000, Batch 20/188, Loss: 0.4331\n",
      "Epoch 24/10000, Batch 30/188, Loss: 0.4930\n",
      "Epoch 24/10000, Batch 40/188, Loss: 0.5845\n",
      "Epoch 24/10000, Batch 50/188, Loss: 0.4706\n",
      "Epoch 24/10000, Batch 60/188, Loss: 0.3563\n",
      "Epoch 24/10000, Batch 70/188, Loss: 0.4871\n",
      "Epoch 24/10000, Batch 80/188, Loss: 0.7385\n",
      "Epoch 24/10000, Batch 90/188, Loss: 0.4401\n",
      "Epoch 24/10000, Batch 100/188, Loss: 0.5614\n",
      "Epoch 24/10000, Batch 110/188, Loss: 0.4947\n",
      "Epoch 24/10000, Batch 120/188, Loss: 0.4759\n",
      "Epoch 24/10000, Batch 130/188, Loss: 0.4841\n",
      "Epoch 24/10000, Batch 140/188, Loss: 0.4450\n",
      "Epoch 24/10000, Batch 150/188, Loss: 0.6094\n",
      "Epoch 24/10000, Batch 160/188, Loss: 0.4607\n",
      "Epoch 24/10000, Batch 170/188, Loss: 0.4297\n",
      "Epoch 24/10000, Batch 180/188, Loss: 0.6475\n",
      "Epoch 24 Kt thc - Mt mt Hun luyn: 0.4833, IoU Hun luyn: 0.5424, F1-Score Hun luyn: 0.6968\n",
      "Mt mt Xc thc: 0.6711, IoU Xc thc: 0.3441, F1-Score Xc thc: 0.4749\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 1/30\n",
      "Epoch 25/10000 Bt u...\n",
      "Epoch 25/10000, Batch 10/188, Loss: 0.3087\n",
      "Epoch 25/10000, Batch 20/188, Loss: 0.5289\n",
      "Epoch 25/10000, Batch 30/188, Loss: 0.3091\n",
      "Epoch 25/10000, Batch 40/188, Loss: 0.6214\n",
      "Epoch 25/10000, Batch 50/188, Loss: 0.4307\n",
      "Epoch 25/10000, Batch 60/188, Loss: 0.6560\n",
      "Epoch 25/10000, Batch 70/188, Loss: 0.5206\n",
      "Epoch 25/10000, Batch 80/188, Loss: 0.6986\n",
      "Epoch 25/10000, Batch 90/188, Loss: 0.3334\n",
      "Epoch 25/10000, Batch 100/188, Loss: 0.4812\n",
      "Epoch 25/10000, Batch 110/188, Loss: 0.6759\n",
      "Epoch 25/10000, Batch 120/188, Loss: 0.7252\n",
      "Epoch 25/10000, Batch 130/188, Loss: 0.5383\n",
      "Epoch 25/10000, Batch 140/188, Loss: 0.5682\n",
      "Epoch 25/10000, Batch 150/188, Loss: 0.4847\n",
      "Epoch 25/10000, Batch 160/188, Loss: 0.3624\n",
      "Epoch 25/10000, Batch 170/188, Loss: 0.5336\n",
      "Epoch 25/10000, Batch 180/188, Loss: 0.5801\n",
      "Epoch 25 Kt thc - Mt mt Hun luyn: 0.4838, IoU Hun luyn: 0.5379, F1-Score Hun luyn: 0.6926\n",
      "Mt mt Xc thc: 0.6689, IoU Xc thc: 0.3449, F1-Score Xc thc: 0.4797\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 2/30\n",
      "Epoch 26/10000 Bt u...\n",
      "Epoch 26/10000, Batch 10/188, Loss: 0.3392\n",
      "Epoch 26/10000, Batch 20/188, Loss: 0.5090\n",
      "Epoch 26/10000, Batch 30/188, Loss: 0.4634\n",
      "Epoch 26/10000, Batch 40/188, Loss: 0.4043\n",
      "Epoch 26/10000, Batch 50/188, Loss: 0.5007\n",
      "Epoch 26/10000, Batch 60/188, Loss: 0.5555\n",
      "Epoch 26/10000, Batch 70/188, Loss: 0.7237\n",
      "Epoch 26/10000, Batch 80/188, Loss: 0.6974\n",
      "Epoch 26/10000, Batch 90/188, Loss: 0.5420\n",
      "Epoch 26/10000, Batch 100/188, Loss: 0.3415\n",
      "Epoch 26/10000, Batch 110/188, Loss: 0.5150\n",
      "Epoch 26/10000, Batch 120/188, Loss: 0.3638\n",
      "Epoch 26/10000, Batch 130/188, Loss: 0.5487\n",
      "Epoch 26/10000, Batch 140/188, Loss: 0.5254\n",
      "Epoch 26/10000, Batch 150/188, Loss: 0.6438\n",
      "Epoch 26/10000, Batch 160/188, Loss: 0.3895\n",
      "Epoch 26/10000, Batch 170/188, Loss: 0.4378\n",
      "Epoch 26/10000, Batch 180/188, Loss: 0.4590\n",
      "Epoch 26 Kt thc - Mt mt Hun luyn: 0.4834, IoU Hun luyn: 0.5358, F1-Score Hun luyn: 0.6894\n",
      "Mt mt Xc thc: 0.6380, IoU Xc thc: 0.3760, F1-Score Xc thc: 0.5065\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.6380. Lu m hnh v trng thi...\n",
      "Epoch 27/10000 Bt u...\n",
      "Epoch 27/10000, Batch 10/188, Loss: 0.3711\n",
      "Epoch 27/10000, Batch 20/188, Loss: 0.3717\n",
      "Epoch 27/10000, Batch 30/188, Loss: 0.4064\n",
      "Epoch 27/10000, Batch 40/188, Loss: 0.6479\n",
      "Epoch 27/10000, Batch 50/188, Loss: 0.4595\n",
      "Epoch 27/10000, Batch 60/188, Loss: 0.3294\n",
      "Epoch 27/10000, Batch 70/188, Loss: 0.4627\n",
      "Epoch 27/10000, Batch 80/188, Loss: 0.2623\n",
      "Epoch 27/10000, Batch 90/188, Loss: 0.5701\n",
      "Epoch 27/10000, Batch 100/188, Loss: 0.4986\n",
      "Epoch 27/10000, Batch 110/188, Loss: 0.6137\n",
      "Epoch 27/10000, Batch 120/188, Loss: 0.4875\n",
      "Epoch 27/10000, Batch 130/188, Loss: 0.5354\n",
      "Epoch 27/10000, Batch 140/188, Loss: 0.4177\n",
      "Epoch 27/10000, Batch 150/188, Loss: 0.4820\n",
      "Epoch 27/10000, Batch 160/188, Loss: 0.6636\n",
      "Epoch 27/10000, Batch 170/188, Loss: 0.5515\n",
      "Epoch 27/10000, Batch 180/188, Loss: 0.6381\n",
      "Epoch 27 Kt thc - Mt mt Hun luyn: 0.4797, IoU Hun luyn: 0.5372, F1-Score Hun luyn: 0.6913\n",
      "Mt mt Xc thc: 0.6346, IoU Xc thc: 0.3779, F1-Score Xc thc: 0.5079\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc tt nht c cp nht: 0.6346. Lu m hnh v trng thi...\n",
      "Epoch 28/10000 Bt u...\n",
      "Epoch 28/10000, Batch 10/188, Loss: 0.4188\n",
      "Epoch 28/10000, Batch 20/188, Loss: 0.4967\n",
      "Epoch 28/10000, Batch 30/188, Loss: 0.5188\n",
      "Epoch 28/10000, Batch 40/188, Loss: 0.3340\n",
      "Epoch 28/10000, Batch 50/188, Loss: 0.4306\n",
      "Epoch 28/10000, Batch 60/188, Loss: 0.2976\n",
      "Epoch 28/10000, Batch 70/188, Loss: 0.4761\n",
      "Epoch 28/10000, Batch 80/188, Loss: 0.3946\n",
      "Epoch 28/10000, Batch 90/188, Loss: 0.3327\n",
      "Epoch 28/10000, Batch 100/188, Loss: 0.3971\n",
      "Epoch 28/10000, Batch 110/188, Loss: 0.4115\n",
      "Epoch 28/10000, Batch 120/188, Loss: 0.3548\n",
      "Epoch 28/10000, Batch 130/188, Loss: 0.2511\n",
      "Epoch 28/10000, Batch 140/188, Loss: 0.4690\n",
      "Epoch 28/10000, Batch 150/188, Loss: 0.4786\n",
      "Epoch 28/10000, Batch 160/188, Loss: 0.4889\n",
      "Epoch 28/10000, Batch 170/188, Loss: 0.2559\n",
      "Epoch 28/10000, Batch 180/188, Loss: 0.5864\n",
      "Epoch 28 Kt thc - Mt mt Hun luyn: 0.4749, IoU Hun luyn: 0.5398, F1-Score Hun luyn: 0.6935\n",
      "Mt mt Xc thc: 0.6483, IoU Xc thc: 0.3621, F1-Score Xc thc: 0.4958\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 1/30\n",
      "Epoch 29/10000 Bt u...\n",
      "Epoch 29/10000, Batch 10/188, Loss: 0.5186\n",
      "Epoch 29/10000, Batch 20/188, Loss: 0.4925\n",
      "Epoch 29/10000, Batch 30/188, Loss: 0.5505\n",
      "Epoch 29/10000, Batch 40/188, Loss: 0.3473\n",
      "Epoch 29/10000, Batch 50/188, Loss: 0.5943\n",
      "Epoch 29/10000, Batch 60/188, Loss: 0.5256\n",
      "Epoch 29/10000, Batch 70/188, Loss: 0.5874\n",
      "Epoch 29/10000, Batch 80/188, Loss: 0.4780\n",
      "Epoch 29/10000, Batch 90/188, Loss: 0.5275\n",
      "Epoch 29/10000, Batch 100/188, Loss: 0.5743\n",
      "Epoch 29/10000, Batch 110/188, Loss: 0.8553\n",
      "Epoch 29/10000, Batch 120/188, Loss: 0.4391\n",
      "Epoch 29/10000, Batch 130/188, Loss: 0.6311\n",
      "Epoch 29/10000, Batch 140/188, Loss: 0.4012\n",
      "Epoch 29/10000, Batch 150/188, Loss: 0.4468\n",
      "Epoch 29/10000, Batch 160/188, Loss: 0.5246\n",
      "Epoch 29/10000, Batch 170/188, Loss: 0.3431\n",
      "Epoch 29/10000, Batch 180/188, Loss: 0.5283\n",
      "Epoch 29 Kt thc - Mt mt Hun luyn: 0.4744, IoU Hun luyn: 0.5386, F1-Score Hun luyn: 0.6924\n",
      "Mt mt Xc thc: 0.6473, IoU Xc thc: 0.3614, F1-Score Xc thc: 0.4851\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 2/30\n",
      "Epoch 30/10000 Bt u...\n",
      "Epoch 30/10000, Batch 10/188, Loss: 0.6136\n",
      "Epoch 30/10000, Batch 20/188, Loss: 0.3733\n",
      "Epoch 30/10000, Batch 30/188, Loss: 0.3282\n",
      "Epoch 30/10000, Batch 40/188, Loss: 0.2667\n",
      "Epoch 30/10000, Batch 50/188, Loss: 0.4382\n",
      "Epoch 30/10000, Batch 60/188, Loss: 0.5144\n",
      "Epoch 30/10000, Batch 70/188, Loss: 0.4964\n",
      "Epoch 30/10000, Batch 80/188, Loss: 0.6168\n",
      "Epoch 30/10000, Batch 90/188, Loss: 0.5159\n",
      "Epoch 30/10000, Batch 100/188, Loss: 0.5438\n",
      "Epoch 30/10000, Batch 110/188, Loss: 0.5550\n",
      "Epoch 30/10000, Batch 120/188, Loss: 0.6456\n",
      "Epoch 30/10000, Batch 130/188, Loss: 0.4091\n",
      "Epoch 30/10000, Batch 140/188, Loss: 0.4719\n",
      "Epoch 30/10000, Batch 150/188, Loss: 0.7707\n",
      "Epoch 30/10000, Batch 160/188, Loss: 0.5613\n",
      "Epoch 30/10000, Batch 170/188, Loss: 0.4550\n",
      "Epoch 30/10000, Batch 180/188, Loss: 0.3782\n",
      "Epoch 30 Kt thc - Mt mt Hun luyn: 0.4761, IoU Hun luyn: 0.5357, F1-Score Hun luyn: 0.6898\n",
      "Mt mt Xc thc: 0.6350, IoU Xc thc: 0.3737, F1-Score Xc thc: 0.5036\n",
      "Learning Rate hin ti: 0.00010000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 3/30\n",
      "Epoch 31/10000 Bt u...\n",
      "Epoch 31/10000, Batch 10/188, Loss: 0.5230\n",
      "Epoch 31/10000, Batch 20/188, Loss: 0.3923\n",
      "Epoch 31/10000, Batch 30/188, Loss: 0.2431\n",
      "Epoch 31/10000, Batch 40/188, Loss: 0.4211\n",
      "Epoch 31/10000, Batch 50/188, Loss: 0.5222\n",
      "Epoch 31/10000, Batch 60/188, Loss: 0.6574\n",
      "Epoch 31/10000, Batch 70/188, Loss: 0.5833\n",
      "Epoch 31/10000, Batch 80/188, Loss: 0.6124\n",
      "Epoch 31/10000, Batch 90/188, Loss: 0.4397\n",
      "Epoch 31/10000, Batch 100/188, Loss: 0.4938\n",
      "Epoch 31/10000, Batch 110/188, Loss: 0.2116\n",
      "Epoch 31/10000, Batch 120/188, Loss: 0.4279\n",
      "Epoch 31/10000, Batch 130/188, Loss: 0.4226\n",
      "Epoch 31/10000, Batch 140/188, Loss: 0.3767\n",
      "Epoch 31/10000, Batch 150/188, Loss: 0.4363\n",
      "Epoch 31/10000, Batch 160/188, Loss: 0.3923\n",
      "Epoch 31/10000, Batch 170/188, Loss: 0.6186\n",
      "Epoch 31/10000, Batch 180/188, Loss: 0.3670\n",
      "Epoch 31 Kt thc - Mt mt Hun luyn: 0.4663, IoU Hun luyn: 0.5441, F1-Score Hun luyn: 0.6967\n",
      "Mt mt Xc thc: 0.6425, IoU Xc thc: 0.3634, F1-Score Xc thc: 0.4903\n",
      "Learning Rate hin ti: 0.00005000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 4/30\n",
      "Epoch 32/10000 Bt u...\n",
      "Epoch 32/10000, Batch 10/188, Loss: 0.6444\n",
      "Epoch 32/10000, Batch 20/188, Loss: 0.2435\n",
      "Epoch 32/10000, Batch 30/188, Loss: 0.3199\n",
      "Epoch 32/10000, Batch 40/188, Loss: 0.4699\n",
      "Epoch 32/10000, Batch 50/188, Loss: 0.3292\n",
      "Epoch 32/10000, Batch 60/188, Loss: 0.6316\n",
      "Epoch 32/10000, Batch 70/188, Loss: 0.3701\n",
      "Epoch 32/10000, Batch 80/188, Loss: 0.4315\n",
      "Epoch 32/10000, Batch 90/188, Loss: 0.4552\n",
      "Epoch 32/10000, Batch 100/188, Loss: 0.3569\n",
      "Epoch 32/10000, Batch 110/188, Loss: 0.4269\n",
      "Epoch 32/10000, Batch 120/188, Loss: 0.5453\n",
      "Epoch 32/10000, Batch 130/188, Loss: 0.5425\n",
      "Epoch 32/10000, Batch 140/188, Loss: 0.6855\n",
      "Epoch 32/10000, Batch 150/188, Loss: 0.4114\n",
      "Epoch 32/10000, Batch 160/188, Loss: 0.4192\n",
      "Epoch 32/10000, Batch 170/188, Loss: 0.4181\n",
      "Epoch 32/10000, Batch 180/188, Loss: 0.4971\n",
      "Epoch 32 Kt thc - Mt mt Hun luyn: 0.4568, IoU Hun luyn: 0.5529, F1-Score Hun luyn: 0.7057\n",
      "Mt mt Xc thc: 0.6404, IoU Xc thc: 0.3672, F1-Score Xc thc: 0.4927\n",
      "Learning Rate hin ti: 0.00005000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 5/30\n",
      "Epoch 33/10000 Bt u...\n",
      "Epoch 33/10000, Batch 10/188, Loss: 0.4423\n",
      "Epoch 33/10000, Batch 20/188, Loss: 0.4583\n",
      "Epoch 33/10000, Batch 30/188, Loss: 0.6335\n",
      "Epoch 33/10000, Batch 40/188, Loss: 0.5736\n",
      "Epoch 33/10000, Batch 50/188, Loss: 0.4372\n",
      "Epoch 33/10000, Batch 60/188, Loss: 0.4678\n",
      "Epoch 33/10000, Batch 70/188, Loss: 0.5922\n",
      "Epoch 33/10000, Batch 80/188, Loss: 0.3780\n",
      "Epoch 33/10000, Batch 90/188, Loss: 0.3174\n",
      "Epoch 33/10000, Batch 100/188, Loss: 0.3282\n",
      "Epoch 33/10000, Batch 110/188, Loss: 0.5958\n",
      "Epoch 33/10000, Batch 120/188, Loss: 0.2895\n",
      "Epoch 33/10000, Batch 130/188, Loss: 0.4428\n",
      "Epoch 33/10000, Batch 140/188, Loss: 0.4075\n",
      "Epoch 33/10000, Batch 150/188, Loss: 0.3130\n",
      "Epoch 33/10000, Batch 160/188, Loss: 0.3545\n",
      "Epoch 33/10000, Batch 170/188, Loss: 0.4923\n",
      "Epoch 33/10000, Batch 180/188, Loss: 0.3289\n",
      "Epoch 33 Kt thc - Mt mt Hun luyn: 0.4528, IoU Hun luyn: 0.5563, F1-Score Hun luyn: 0.7083\n",
      "Mt mt Xc thc: 0.6297, IoU Xc thc: 0.3775, F1-Score Xc thc: 0.5041\n",
      "Learning Rate hin ti: 0.00005000\n",
      "Mt mt xc thc tt nht c cp nht: 0.6297. Lu m hnh v trng thi...\n",
      "Epoch 34/10000 Bt u...\n",
      "Epoch 34/10000, Batch 10/188, Loss: 0.5909\n",
      "Epoch 34/10000, Batch 20/188, Loss: 0.4433\n",
      "Epoch 34/10000, Batch 30/188, Loss: 0.5783\n",
      "Epoch 34/10000, Batch 40/188, Loss: 0.4527\n",
      "Epoch 34/10000, Batch 50/188, Loss: 0.5501\n",
      "Epoch 34/10000, Batch 60/188, Loss: 0.2974\n",
      "Epoch 34/10000, Batch 70/188, Loss: 0.3807\n",
      "Epoch 34/10000, Batch 80/188, Loss: 0.4684\n",
      "Epoch 34/10000, Batch 90/188, Loss: 0.4837\n",
      "Epoch 34/10000, Batch 100/188, Loss: 0.3373\n",
      "Epoch 34/10000, Batch 110/188, Loss: 0.4658\n",
      "Epoch 34/10000, Batch 120/188, Loss: 0.5844\n",
      "Epoch 34/10000, Batch 130/188, Loss: 0.3714\n",
      "Epoch 34/10000, Batch 140/188, Loss: 0.5924\n",
      "Epoch 34/10000, Batch 150/188, Loss: 0.4131\n",
      "Epoch 34/10000, Batch 160/188, Loss: 0.7470\n",
      "Epoch 34/10000, Batch 170/188, Loss: 0.4648\n",
      "Epoch 34/10000, Batch 180/188, Loss: 0.3795\n",
      "Epoch 34 Kt thc - Mt mt Hun luyn: 0.4590, IoU Hun luyn: 0.5494, F1-Score Hun luyn: 0.7003\n",
      "Mt mt Xc thc: 0.6313, IoU Xc thc: 0.3749, F1-Score Xc thc: 0.5030\n",
      "Learning Rate hin ti: 0.00005000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 1/30\n",
      "Epoch 35/10000 Bt u...\n",
      "Epoch 35/10000, Batch 10/188, Loss: 0.4404\n",
      "Epoch 35/10000, Batch 20/188, Loss: 0.2204\n",
      "Epoch 35/10000, Batch 30/188, Loss: 0.3556\n",
      "Epoch 35/10000, Batch 40/188, Loss: 0.3472\n",
      "Epoch 35/10000, Batch 50/188, Loss: 0.2886\n",
      "Epoch 35/10000, Batch 60/188, Loss: 0.2886\n",
      "Epoch 35/10000, Batch 70/188, Loss: 0.2308\n",
      "Epoch 35/10000, Batch 80/188, Loss: 0.4281\n",
      "Epoch 35/10000, Batch 90/188, Loss: 0.3541\n",
      "Epoch 35/10000, Batch 100/188, Loss: 0.4186\n",
      "Epoch 35/10000, Batch 110/188, Loss: 0.3798\n",
      "Epoch 35/10000, Batch 120/188, Loss: 0.4860\n",
      "Epoch 35/10000, Batch 130/188, Loss: 0.3768\n",
      "Epoch 35/10000, Batch 140/188, Loss: 0.4372\n",
      "Epoch 35/10000, Batch 150/188, Loss: 0.5017\n",
      "Epoch 35/10000, Batch 160/188, Loss: 0.4603\n",
      "Epoch 35/10000, Batch 170/188, Loss: 0.4895\n",
      "Epoch 35/10000, Batch 180/188, Loss: 0.4078\n",
      "Epoch 35 Kt thc - Mt mt Hun luyn: 0.4513, IoU Hun luyn: 0.5569, F1-Score Hun luyn: 0.7083\n",
      "Mt mt Xc thc: 0.6239, IoU Xc thc: 0.3822, F1-Score Xc thc: 0.5137\n",
      "Learning Rate hin ti: 0.00005000\n",
      "Mt mt xc thc tt nht c cp nht: 0.6239. Lu m hnh v trng thi...\n",
      "Epoch 36/10000 Bt u...\n",
      "Epoch 36/10000, Batch 10/188, Loss: 0.3051\n",
      "Epoch 36/10000, Batch 20/188, Loss: 0.4830\n",
      "Epoch 36/10000, Batch 30/188, Loss: 0.6857\n",
      "Epoch 36/10000, Batch 40/188, Loss: 0.4806\n",
      "Epoch 36/10000, Batch 50/188, Loss: 0.3411\n",
      "Epoch 36/10000, Batch 60/188, Loss: 0.7417\n",
      "Epoch 36/10000, Batch 70/188, Loss: 0.6833\n",
      "Epoch 36/10000, Batch 80/188, Loss: 0.3779\n",
      "Epoch 36/10000, Batch 90/188, Loss: 0.6017\n",
      "Epoch 36/10000, Batch 100/188, Loss: 0.4449\n",
      "Epoch 36/10000, Batch 110/188, Loss: 0.7011\n",
      "Epoch 36/10000, Batch 120/188, Loss: 0.3630\n",
      "Epoch 36/10000, Batch 130/188, Loss: 0.2887\n",
      "Epoch 36/10000, Batch 140/188, Loss: 0.5666\n",
      "Epoch 36/10000, Batch 150/188, Loss: 0.5740\n",
      "Epoch 36/10000, Batch 160/188, Loss: 0.5157\n",
      "Epoch 36/10000, Batch 170/188, Loss: 0.4389\n",
      "Epoch 36/10000, Batch 180/188, Loss: 0.5319\n",
      "Epoch 36 Kt thc - Mt mt Hun luyn: 0.4459, IoU Hun luyn: 0.5620, F1-Score Hun luyn: 0.7126\n",
      "Mt mt Xc thc: 0.6430, IoU Xc thc: 0.3626, F1-Score Xc thc: 0.4870\n",
      "Learning Rate hin ti: 0.00005000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 1/30\n",
      "Epoch 37/10000 Bt u...\n",
      "Epoch 37/10000, Batch 10/188, Loss: 0.2610\n",
      "Epoch 37/10000, Batch 20/188, Loss: 0.4453\n",
      "Epoch 37/10000, Batch 30/188, Loss: 0.5327\n",
      "Epoch 37/10000, Batch 40/188, Loss: 0.4585\n",
      "Epoch 37/10000, Batch 50/188, Loss: 0.4409\n",
      "Epoch 37/10000, Batch 60/188, Loss: 0.4403\n",
      "Epoch 37/10000, Batch 70/188, Loss: 0.4338\n",
      "Epoch 37/10000, Batch 80/188, Loss: 0.3903\n",
      "Epoch 37/10000, Batch 90/188, Loss: 0.4220\n",
      "Epoch 37/10000, Batch 100/188, Loss: 0.4308\n",
      "Epoch 37/10000, Batch 110/188, Loss: 0.3002\n",
      "Epoch 37/10000, Batch 120/188, Loss: 0.4710\n",
      "Epoch 37/10000, Batch 130/188, Loss: 0.4589\n",
      "Epoch 37/10000, Batch 140/188, Loss: 0.5746\n",
      "Epoch 37/10000, Batch 150/188, Loss: 0.4886\n",
      "Epoch 37/10000, Batch 160/188, Loss: 0.4863\n",
      "Epoch 37/10000, Batch 170/188, Loss: 0.6800\n",
      "Epoch 37/10000, Batch 180/188, Loss: 0.7497\n",
      "Epoch 37 Kt thc - Mt mt Hun luyn: 0.4488, IoU Hun luyn: 0.5584, F1-Score Hun luyn: 0.7093\n",
      "Mt mt Xc thc: 0.6380, IoU Xc thc: 0.3668, F1-Score Xc thc: 0.4967\n",
      "Learning Rate hin ti: 0.00005000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 2/30\n",
      "Epoch 38/10000 Bt u...\n",
      "Epoch 38/10000, Batch 10/188, Loss: 0.4099\n",
      "Epoch 38/10000, Batch 20/188, Loss: 0.5501\n",
      "Epoch 38/10000, Batch 30/188, Loss: 0.3208\n",
      "Epoch 38/10000, Batch 40/188, Loss: 0.5713\n",
      "Epoch 38/10000, Batch 50/188, Loss: 0.4110\n",
      "Epoch 38/10000, Batch 60/188, Loss: 0.2193\n",
      "Epoch 38/10000, Batch 70/188, Loss: 0.3663\n",
      "Epoch 38/10000, Batch 80/188, Loss: 0.4018\n",
      "Epoch 38/10000, Batch 90/188, Loss: 0.3400\n",
      "Epoch 38/10000, Batch 100/188, Loss: 0.5402\n",
      "Epoch 38/10000, Batch 110/188, Loss: 0.3032\n",
      "Epoch 38/10000, Batch 120/188, Loss: 0.2351\n",
      "Epoch 38/10000, Batch 130/188, Loss: 0.5995\n",
      "Epoch 38/10000, Batch 140/188, Loss: 0.4372\n",
      "Epoch 38/10000, Batch 150/188, Loss: 0.6549\n",
      "Epoch 38/10000, Batch 160/188, Loss: 0.2979\n",
      "Epoch 38/10000, Batch 170/188, Loss: 0.3707\n",
      "Epoch 38/10000, Batch 180/188, Loss: 0.5955\n",
      "Epoch 38 Kt thc - Mt mt Hun luyn: 0.4495, IoU Hun luyn: 0.5573, F1-Score Hun luyn: 0.7085\n",
      "Mt mt Xc thc: 0.6321, IoU Xc thc: 0.3729, F1-Score Xc thc: 0.5012\n",
      "Learning Rate hin ti: 0.00005000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 3/30\n",
      "Epoch 39/10000 Bt u...\n",
      "Epoch 39/10000, Batch 10/188, Loss: 0.6195\n",
      "Epoch 39/10000, Batch 20/188, Loss: 0.6538\n",
      "Epoch 39/10000, Batch 30/188, Loss: 0.2711\n",
      "Epoch 39/10000, Batch 40/188, Loss: 0.6030\n",
      "Epoch 39/10000, Batch 50/188, Loss: 0.5122\n",
      "Epoch 39/10000, Batch 60/188, Loss: 0.6487\n",
      "Epoch 39/10000, Batch 70/188, Loss: 0.6870\n",
      "Epoch 39/10000, Batch 80/188, Loss: 0.4086\n",
      "Epoch 39/10000, Batch 90/188, Loss: 0.5460\n",
      "Epoch 39/10000, Batch 100/188, Loss: 0.7411\n",
      "Epoch 39/10000, Batch 110/188, Loss: 0.2108\n",
      "Epoch 39/10000, Batch 120/188, Loss: 0.2716\n",
      "Epoch 39/10000, Batch 130/188, Loss: 0.5442\n",
      "Epoch 39/10000, Batch 140/188, Loss: 0.3984\n",
      "Epoch 39/10000, Batch 150/188, Loss: 0.4654\n",
      "Epoch 39/10000, Batch 160/188, Loss: 0.4091\n",
      "Epoch 39/10000, Batch 170/188, Loss: 0.3336\n",
      "Epoch 39/10000, Batch 180/188, Loss: 0.3198\n",
      "Epoch 39 Kt thc - Mt mt Hun luyn: 0.4540, IoU Hun luyn: 0.5521, F1-Score Hun luyn: 0.7036\n",
      "Mt mt Xc thc: 0.6221, IoU Xc thc: 0.3826, F1-Score Xc thc: 0.5142\n",
      "Learning Rate hin ti: 0.00005000\n",
      "Mt mt xc thc tt nht c cp nht: 0.6221. Lu m hnh v trng thi...\n",
      "Epoch 40/10000 Bt u...\n",
      "Epoch 40/10000, Batch 10/188, Loss: 0.3158\n",
      "Epoch 40/10000, Batch 20/188, Loss: 0.2546\n",
      "Epoch 40/10000, Batch 30/188, Loss: 0.4557\n",
      "Epoch 40/10000, Batch 40/188, Loss: 0.5452\n",
      "Epoch 40/10000, Batch 50/188, Loss: 0.3634\n",
      "Epoch 40/10000, Batch 60/188, Loss: 0.5283\n",
      "Epoch 40/10000, Batch 70/188, Loss: 0.3950\n",
      "Epoch 40/10000, Batch 80/188, Loss: 0.5804\n",
      "Epoch 40/10000, Batch 90/188, Loss: 0.4771\n",
      "Epoch 40/10000, Batch 100/188, Loss: 0.6889\n",
      "Epoch 40/10000, Batch 110/188, Loss: 0.3807\n",
      "Epoch 40/10000, Batch 120/188, Loss: 0.6080\n",
      "Epoch 40/10000, Batch 130/188, Loss: 0.5342\n",
      "Epoch 40/10000, Batch 140/188, Loss: 0.7411\n",
      "Epoch 40/10000, Batch 150/188, Loss: 0.4378\n",
      "Epoch 40/10000, Batch 160/188, Loss: 0.3545\n",
      "Epoch 40/10000, Batch 170/188, Loss: 0.4926\n",
      "Epoch 40/10000, Batch 180/188, Loss: 0.4708\n",
      "Epoch 40 Kt thc - Mt mt Hun luyn: 0.4494, IoU Hun luyn: 0.5565, F1-Score Hun luyn: 0.7081\n",
      "Mt mt Xc thc: 0.6104, IoU Xc thc: 0.3942, F1-Score Xc thc: 0.5260\n",
      "Learning Rate hin ti: 0.00005000\n",
      "Mt mt xc thc tt nht c cp nht: 0.6104. Lu m hnh v trng thi...\n",
      "Epoch 41/10000 Bt u...\n",
      "Epoch 41/10000, Batch 10/188, Loss: 0.4844\n",
      "Epoch 41/10000, Batch 20/188, Loss: 0.5890\n",
      "Epoch 41/10000, Batch 30/188, Loss: 0.3980\n",
      "Epoch 41/10000, Batch 40/188, Loss: 0.4493\n",
      "Epoch 41/10000, Batch 50/188, Loss: 0.5231\n",
      "Epoch 41/10000, Batch 60/188, Loss: 0.3481\n",
      "Epoch 41/10000, Batch 70/188, Loss: 0.4297\n",
      "Epoch 41/10000, Batch 80/188, Loss: 0.3820\n",
      "Epoch 41/10000, Batch 90/188, Loss: 0.2752\n",
      "Epoch 41/10000, Batch 100/188, Loss: 0.5062\n",
      "Epoch 41/10000, Batch 110/188, Loss: 0.3662\n",
      "Epoch 41/10000, Batch 120/188, Loss: 0.4125\n",
      "Epoch 41/10000, Batch 130/188, Loss: 0.4531\n",
      "Epoch 41/10000, Batch 140/188, Loss: 0.2713\n",
      "Epoch 41/10000, Batch 150/188, Loss: 0.4690\n",
      "Epoch 41/10000, Batch 160/188, Loss: 0.4981\n",
      "Epoch 41/10000, Batch 170/188, Loss: 0.5397\n",
      "Epoch 41/10000, Batch 180/188, Loss: 0.5888\n",
      "Epoch 41 Kt thc - Mt mt Hun luyn: 0.4509, IoU Hun luyn: 0.5548, F1-Score Hun luyn: 0.7055\n",
      "Mt mt Xc thc: 0.6216, IoU Xc thc: 0.3830, F1-Score Xc thc: 0.5163\n",
      "Learning Rate hin ti: 0.00005000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 1/30\n",
      "Epoch 42/10000 Bt u...\n",
      "Epoch 42/10000, Batch 10/188, Loss: 0.3587\n",
      "Epoch 42/10000, Batch 20/188, Loss: 0.3870\n",
      "Epoch 42/10000, Batch 30/188, Loss: 0.4708\n",
      "Epoch 42/10000, Batch 40/188, Loss: 0.3898\n",
      "Epoch 42/10000, Batch 50/188, Loss: 0.3949\n",
      "Epoch 42/10000, Batch 60/188, Loss: 0.6389\n",
      "Epoch 42/10000, Batch 70/188, Loss: 0.3706\n",
      "Epoch 42/10000, Batch 80/188, Loss: 0.4609\n",
      "Epoch 42/10000, Batch 90/188, Loss: 0.6618\n",
      "Epoch 42/10000, Batch 100/188, Loss: 0.3486\n",
      "Epoch 42/10000, Batch 110/188, Loss: 0.2996\n",
      "Epoch 42/10000, Batch 120/188, Loss: 0.5159\n",
      "Epoch 42/10000, Batch 130/188, Loss: 0.5266\n",
      "Epoch 42/10000, Batch 140/188, Loss: 0.3168\n",
      "Epoch 42/10000, Batch 150/188, Loss: 0.5023\n",
      "Epoch 42/10000, Batch 160/188, Loss: 0.3341\n",
      "Epoch 42/10000, Batch 170/188, Loss: 0.3983\n",
      "Epoch 42/10000, Batch 180/188, Loss: 0.4360\n",
      "Epoch 42 Kt thc - Mt mt Hun luyn: 0.4373, IoU Hun luyn: 0.5678, F1-Score Hun luyn: 0.7174\n",
      "Mt mt Xc thc: 0.6342, IoU Xc thc: 0.3694, F1-Score Xc thc: 0.4993\n",
      "Learning Rate hin ti: 0.00005000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 2/30\n",
      "Epoch 43/10000 Bt u...\n",
      "Epoch 43/10000, Batch 10/188, Loss: 0.3825\n",
      "Epoch 43/10000, Batch 20/188, Loss: 0.4523\n",
      "Epoch 43/10000, Batch 30/188, Loss: 0.4769\n",
      "Epoch 43/10000, Batch 40/188, Loss: 0.4321\n",
      "Epoch 43/10000, Batch 50/188, Loss: 0.6373\n",
      "Epoch 43/10000, Batch 60/188, Loss: 0.3535\n",
      "Epoch 43/10000, Batch 70/188, Loss: 0.4637\n",
      "Epoch 43/10000, Batch 80/188, Loss: 0.2449\n",
      "Epoch 43/10000, Batch 90/188, Loss: 0.3909\n",
      "Epoch 43/10000, Batch 100/188, Loss: 0.3050\n",
      "Epoch 43/10000, Batch 110/188, Loss: 0.5758\n",
      "Epoch 43/10000, Batch 120/188, Loss: 0.6478\n",
      "Epoch 43/10000, Batch 130/188, Loss: 0.4449\n",
      "Epoch 43/10000, Batch 140/188, Loss: 0.4761\n",
      "Epoch 43/10000, Batch 150/188, Loss: 0.4381\n",
      "Epoch 43/10000, Batch 160/188, Loss: 0.4870\n",
      "Epoch 43/10000, Batch 170/188, Loss: 0.4840\n",
      "Epoch 43/10000, Batch 180/188, Loss: 0.3978\n",
      "Epoch 43 Kt thc - Mt mt Hun luyn: 0.4481, IoU Hun luyn: 0.5569, F1-Score Hun luyn: 0.7073\n",
      "Mt mt Xc thc: 0.6208, IoU Xc thc: 0.3828, F1-Score Xc thc: 0.5171\n",
      "Learning Rate hin ti: 0.00005000\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 3/30\n",
      "Epoch 44/10000 Bt u...\n",
      "Epoch 44/10000, Batch 10/188, Loss: 0.4125\n",
      "Epoch 44/10000, Batch 20/188, Loss: 0.4942\n",
      "Epoch 44/10000, Batch 30/188, Loss: 0.3713\n",
      "Epoch 44/10000, Batch 40/188, Loss: 0.4549\n",
      "Epoch 44/10000, Batch 50/188, Loss: 0.5110\n",
      "Epoch 44/10000, Batch 60/188, Loss: 0.2494\n",
      "Epoch 44/10000, Batch 70/188, Loss: 0.3236\n",
      "Epoch 44/10000, Batch 80/188, Loss: 0.4558\n",
      "Epoch 44/10000, Batch 90/188, Loss: 0.2338\n",
      "Epoch 44/10000, Batch 100/188, Loss: 0.3268\n",
      "Epoch 44/10000, Batch 110/188, Loss: 0.3784\n",
      "Epoch 44/10000, Batch 120/188, Loss: 0.3716\n",
      "Epoch 44/10000, Batch 130/188, Loss: 0.7214\n",
      "Epoch 44/10000, Batch 140/188, Loss: 0.4891\n",
      "Epoch 44/10000, Batch 150/188, Loss: 0.3217\n",
      "Epoch 44/10000, Batch 160/188, Loss: 0.3251\n",
      "Epoch 44/10000, Batch 170/188, Loss: 0.4701\n",
      "Epoch 44/10000, Batch 180/188, Loss: 0.5121\n",
      "Epoch 44 Kt thc - Mt mt Hun luyn: 0.4433, IoU Hun luyn: 0.5613, F1-Score Hun luyn: 0.7125\n",
      "Mt mt Xc thc: 0.6413, IoU Xc thc: 0.3619, F1-Score Xc thc: 0.4883\n",
      "Learning Rate hin ti: 0.00002500\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 4/30\n",
      "Epoch 45/10000 Bt u...\n",
      "Epoch 45/10000, Batch 10/188, Loss: 0.3253\n",
      "Epoch 45/10000, Batch 20/188, Loss: 0.3690\n",
      "Epoch 45/10000, Batch 30/188, Loss: 0.2838\n",
      "Epoch 45/10000, Batch 40/188, Loss: 0.4640\n",
      "Epoch 45/10000, Batch 50/188, Loss: 0.3991\n",
      "Epoch 45/10000, Batch 60/188, Loss: 0.4885\n",
      "Epoch 45/10000, Batch 70/188, Loss: 0.5809\n",
      "Epoch 45/10000, Batch 80/188, Loss: 0.5992\n",
      "Epoch 45/10000, Batch 90/188, Loss: 0.6247\n",
      "Epoch 45/10000, Batch 100/188, Loss: 0.5926\n",
      "Epoch 45/10000, Batch 110/188, Loss: 0.5136\n",
      "Epoch 45/10000, Batch 120/188, Loss: 0.5814\n",
      "Epoch 45/10000, Batch 130/188, Loss: 0.4320\n",
      "Epoch 45/10000, Batch 140/188, Loss: 0.2614\n",
      "Epoch 45/10000, Batch 150/188, Loss: 0.4789\n",
      "Epoch 45/10000, Batch 160/188, Loss: 0.3365\n",
      "Epoch 45/10000, Batch 170/188, Loss: 0.3994\n",
      "Epoch 45/10000, Batch 180/188, Loss: 0.6832\n",
      "Epoch 45 Kt thc - Mt mt Hun luyn: 0.4342, IoU Hun luyn: 0.5702, F1-Score Hun luyn: 0.7191\n",
      "Mt mt Xc thc: 0.6162, IoU Xc thc: 0.3872, F1-Score Xc thc: 0.5194\n",
      "Learning Rate hin ti: 0.00002500\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 5/30\n",
      "Epoch 46/10000 Bt u...\n",
      "Epoch 46/10000, Batch 10/188, Loss: 0.3277\n",
      "Epoch 46/10000, Batch 20/188, Loss: 0.4088\n",
      "Epoch 46/10000, Batch 30/188, Loss: 0.5024\n",
      "Epoch 46/10000, Batch 40/188, Loss: 0.4790\n",
      "Epoch 46/10000, Batch 50/188, Loss: 0.4136\n",
      "Epoch 46/10000, Batch 60/188, Loss: 0.3590\n",
      "Epoch 46/10000, Batch 70/188, Loss: 0.3997\n",
      "Epoch 46/10000, Batch 80/188, Loss: 0.5116\n",
      "Epoch 46/10000, Batch 90/188, Loss: 0.4618\n",
      "Epoch 46/10000, Batch 100/188, Loss: 0.2746\n",
      "Epoch 46/10000, Batch 110/188, Loss: 0.4768\n",
      "Epoch 46/10000, Batch 120/188, Loss: 0.4448\n",
      "Epoch 46/10000, Batch 130/188, Loss: 0.8001\n",
      "Epoch 46/10000, Batch 140/188, Loss: 0.5009\n",
      "Epoch 46/10000, Batch 150/188, Loss: 0.3246\n",
      "Epoch 46/10000, Batch 160/188, Loss: 0.5981\n",
      "Epoch 46/10000, Batch 170/188, Loss: 0.4404\n",
      "Epoch 46/10000, Batch 180/188, Loss: 0.5087\n",
      "Epoch 46 Kt thc - Mt mt Hun luyn: 0.4413, IoU Hun luyn: 0.5631, F1-Score Hun luyn: 0.7134\n",
      "Mt mt Xc thc: 0.6280, IoU Xc thc: 0.3752, F1-Score Xc thc: 0.5019\n",
      "Learning Rate hin ti: 0.00002500\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 6/30\n",
      "Epoch 47/10000 Bt u...\n",
      "Epoch 47/10000, Batch 10/188, Loss: 0.5504\n",
      "Epoch 47/10000, Batch 20/188, Loss: 0.5277\n",
      "Epoch 47/10000, Batch 30/188, Loss: 0.4077\n",
      "Epoch 47/10000, Batch 40/188, Loss: 0.4481\n",
      "Epoch 47/10000, Batch 50/188, Loss: 0.5183\n",
      "Epoch 47/10000, Batch 60/188, Loss: 0.5031\n",
      "Epoch 47/10000, Batch 70/188, Loss: 0.5729\n",
      "Epoch 47/10000, Batch 80/188, Loss: 0.4370\n",
      "Epoch 47/10000, Batch 90/188, Loss: 0.5646\n",
      "Epoch 47/10000, Batch 100/188, Loss: 0.4080\n",
      "Epoch 47/10000, Batch 110/188, Loss: 0.5488\n",
      "Epoch 47/10000, Batch 120/188, Loss: 0.5286\n",
      "Epoch 47/10000, Batch 130/188, Loss: 0.4533\n",
      "Epoch 47/10000, Batch 140/188, Loss: 0.4305\n",
      "Epoch 47/10000, Batch 150/188, Loss: 0.2515\n",
      "Epoch 47/10000, Batch 160/188, Loss: 0.4660\n",
      "Epoch 47/10000, Batch 170/188, Loss: 0.4293\n",
      "Epoch 47/10000, Batch 180/188, Loss: 0.1976\n",
      "Epoch 47 Kt thc - Mt mt Hun luyn: 0.4362, IoU Hun luyn: 0.5678, F1-Score Hun luyn: 0.7172\n",
      "Mt mt Xc thc: 0.6184, IoU Xc thc: 0.3848, F1-Score Xc thc: 0.5175\n",
      "Learning Rate hin ti: 0.00002500\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 7/30\n",
      "Epoch 48/10000 Bt u...\n",
      "Epoch 48/10000, Batch 10/188, Loss: 0.3867\n",
      "Epoch 48/10000, Batch 20/188, Loss: 0.4793\n",
      "Epoch 48/10000, Batch 30/188, Loss: 0.6984\n",
      "Epoch 48/10000, Batch 40/188, Loss: 0.4342\n",
      "Epoch 48/10000, Batch 50/188, Loss: 0.3079\n",
      "Epoch 48/10000, Batch 60/188, Loss: 0.3689\n",
      "Epoch 48/10000, Batch 70/188, Loss: 0.4399\n",
      "Epoch 48/10000, Batch 80/188, Loss: 0.5122\n",
      "Epoch 48/10000, Batch 90/188, Loss: 0.2382\n",
      "Epoch 48/10000, Batch 100/188, Loss: 0.3985\n",
      "Epoch 48/10000, Batch 110/188, Loss: 0.3495\n",
      "Epoch 48/10000, Batch 120/188, Loss: 0.4504\n",
      "Epoch 48/10000, Batch 130/188, Loss: 0.5379\n",
      "Epoch 48/10000, Batch 140/188, Loss: 0.3652\n",
      "Epoch 48/10000, Batch 150/188, Loss: 0.4831\n",
      "Epoch 48/10000, Batch 160/188, Loss: 0.4304\n",
      "Epoch 48/10000, Batch 170/188, Loss: 0.4096\n",
      "Epoch 48/10000, Batch 180/188, Loss: 0.4527\n",
      "Epoch 48 Kt thc - Mt mt Hun luyn: 0.4363, IoU Hun luyn: 0.5677, F1-Score Hun luyn: 0.7167\n",
      "Mt mt Xc thc: 0.6208, IoU Xc thc: 0.3826, F1-Score Xc thc: 0.5132\n",
      "Learning Rate hin ti: 0.00001250\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 8/30\n",
      "Epoch 49/10000 Bt u...\n",
      "Epoch 49/10000, Batch 10/188, Loss: 0.2266\n",
      "Epoch 49/10000, Batch 20/188, Loss: 0.5195\n",
      "Epoch 49/10000, Batch 30/188, Loss: 0.3199\n",
      "Epoch 49/10000, Batch 40/188, Loss: 0.5940\n",
      "Epoch 49/10000, Batch 50/188, Loss: 0.5468\n",
      "Epoch 49/10000, Batch 60/188, Loss: 0.3906\n",
      "Epoch 49/10000, Batch 70/188, Loss: 0.4577\n",
      "Epoch 49/10000, Batch 80/188, Loss: 0.5683\n",
      "Epoch 49/10000, Batch 90/188, Loss: 0.5990\n",
      "Epoch 49/10000, Batch 100/188, Loss: 0.5470\n",
      "Epoch 49/10000, Batch 110/188, Loss: 0.5614\n",
      "Epoch 49/10000, Batch 120/188, Loss: 0.4679\n",
      "Epoch 49/10000, Batch 130/188, Loss: 0.3127\n",
      "Epoch 49/10000, Batch 140/188, Loss: 0.4751\n",
      "Epoch 49/10000, Batch 150/188, Loss: 0.7639\n",
      "Epoch 49/10000, Batch 160/188, Loss: 0.4782\n",
      "Epoch 49/10000, Batch 170/188, Loss: 0.5268\n",
      "Epoch 49/10000, Batch 180/188, Loss: 0.3726\n",
      "Epoch 49 Kt thc - Mt mt Hun luyn: 0.4388, IoU Hun luyn: 0.5651, F1-Score Hun luyn: 0.7142\n",
      "Mt mt Xc thc: 0.6144, IoU Xc thc: 0.3887, F1-Score Xc thc: 0.5183\n",
      "Learning Rate hin ti: 0.00001250\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 9/30\n",
      "Epoch 50/10000 Bt u...\n",
      "Epoch 50/10000, Batch 10/188, Loss: 0.6471\n",
      "Epoch 50/10000, Batch 20/188, Loss: 0.4110\n",
      "Epoch 50/10000, Batch 30/188, Loss: 0.3061\n",
      "Epoch 50/10000, Batch 40/188, Loss: 0.3842\n",
      "Epoch 50/10000, Batch 50/188, Loss: 0.4323\n",
      "Epoch 50/10000, Batch 60/188, Loss: 0.3772\n",
      "Epoch 50/10000, Batch 70/188, Loss: 0.4078\n",
      "Epoch 50/10000, Batch 80/188, Loss: 0.5197\n",
      "Epoch 50/10000, Batch 90/188, Loss: 0.4942\n",
      "Epoch 50/10000, Batch 100/188, Loss: 0.3136\n",
      "Epoch 50/10000, Batch 110/188, Loss: 0.6644\n",
      "Epoch 50/10000, Batch 120/188, Loss: 0.3526\n",
      "Epoch 50/10000, Batch 130/188, Loss: 0.4622\n",
      "Epoch 50/10000, Batch 140/188, Loss: 0.3957\n",
      "Epoch 50/10000, Batch 150/188, Loss: 0.2640\n",
      "Epoch 50/10000, Batch 160/188, Loss: 0.4973\n",
      "Epoch 50/10000, Batch 170/188, Loss: 0.3698\n",
      "Epoch 50/10000, Batch 180/188, Loss: 0.3775\n",
      "Epoch 50 Kt thc - Mt mt Hun luyn: 0.4320, IoU Hun luyn: 0.5718, F1-Score Hun luyn: 0.7206\n",
      "Mt mt Xc thc: 0.6154, IoU Xc thc: 0.3875, F1-Score Xc thc: 0.5189\n",
      "Learning Rate hin ti: 0.00001250\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 10/30\n",
      "Epoch 51/10000 Bt u...\n",
      "Epoch 51/10000, Batch 10/188, Loss: 0.7019\n",
      "Epoch 51/10000, Batch 20/188, Loss: 0.6366\n",
      "Epoch 51/10000, Batch 30/188, Loss: 0.5345\n",
      "Epoch 51/10000, Batch 40/188, Loss: 0.6483\n",
      "Epoch 51/10000, Batch 50/188, Loss: 0.3424\n",
      "Epoch 51/10000, Batch 60/188, Loss: 0.4606\n",
      "Epoch 51/10000, Batch 70/188, Loss: 0.4187\n",
      "Epoch 51/10000, Batch 80/188, Loss: 0.3852\n",
      "Epoch 51/10000, Batch 90/188, Loss: 0.3404\n",
      "Epoch 51/10000, Batch 100/188, Loss: 0.2709\n",
      "Epoch 51/10000, Batch 110/188, Loss: 0.5301\n",
      "Epoch 51/10000, Batch 120/188, Loss: 0.3932\n",
      "Epoch 51/10000, Batch 130/188, Loss: 0.4718\n",
      "Epoch 51/10000, Batch 140/188, Loss: 0.7861\n",
      "Epoch 51/10000, Batch 150/188, Loss: 0.4450\n",
      "Epoch 51/10000, Batch 160/188, Loss: 0.4539\n",
      "Epoch 51/10000, Batch 170/188, Loss: 0.2912\n",
      "Epoch 51/10000, Batch 180/188, Loss: 0.6138\n",
      "Epoch 51 Kt thc - Mt mt Hun luyn: 0.4313, IoU Hun luyn: 0.5723, F1-Score Hun luyn: 0.7205\n",
      "Mt mt Xc thc: 0.6142, IoU Xc thc: 0.3887, F1-Score Xc thc: 0.5203\n",
      "Learning Rate hin ti: 0.00001250\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 11/30\n",
      "Epoch 52/10000 Bt u...\n",
      "Epoch 52/10000, Batch 10/188, Loss: 0.2149\n",
      "Epoch 52/10000, Batch 20/188, Loss: 0.4956\n",
      "Epoch 52/10000, Batch 30/188, Loss: 0.5607\n",
      "Epoch 52/10000, Batch 40/188, Loss: 0.4998\n",
      "Epoch 52/10000, Batch 50/188, Loss: 0.2752\n",
      "Epoch 52/10000, Batch 60/188, Loss: 0.4032\n",
      "Epoch 52/10000, Batch 70/188, Loss: 0.4034\n",
      "Epoch 52/10000, Batch 80/188, Loss: 0.4089\n",
      "Epoch 52/10000, Batch 90/188, Loss: 0.5644\n",
      "Epoch 52/10000, Batch 100/188, Loss: 0.5561\n",
      "Epoch 52/10000, Batch 110/188, Loss: 0.3386\n",
      "Epoch 52/10000, Batch 120/188, Loss: 0.3545\n",
      "Epoch 52/10000, Batch 130/188, Loss: 0.2919\n",
      "Epoch 52/10000, Batch 140/188, Loss: 0.4037\n",
      "Epoch 52/10000, Batch 150/188, Loss: 0.4207\n",
      "Epoch 52/10000, Batch 160/188, Loss: 0.2808\n",
      "Epoch 52/10000, Batch 170/188, Loss: 0.6695\n",
      "Epoch 52/10000, Batch 180/188, Loss: 0.4730\n",
      "Epoch 52 Kt thc - Mt mt Hun luyn: 0.4360, IoU Hun luyn: 0.5678, F1-Score Hun luyn: 0.7171\n",
      "Mt mt Xc thc: 0.6150, IoU Xc thc: 0.3877, F1-Score Xc thc: 0.5176\n",
      "Learning Rate hin ti: 0.00000625\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 12/30\n",
      "Epoch 53/10000 Bt u...\n",
      "Epoch 53/10000, Batch 10/188, Loss: 0.3309\n",
      "Epoch 53/10000, Batch 20/188, Loss: 0.3579\n",
      "Epoch 53/10000, Batch 30/188, Loss: 0.4361\n",
      "Epoch 53/10000, Batch 40/188, Loss: 0.3834\n",
      "Epoch 53/10000, Batch 50/188, Loss: 0.4972\n",
      "Epoch 53/10000, Batch 60/188, Loss: 0.2437\n",
      "Epoch 53/10000, Batch 70/188, Loss: 0.4129\n",
      "Epoch 53/10000, Batch 80/188, Loss: 0.3667\n",
      "Epoch 53/10000, Batch 90/188, Loss: 0.5416\n",
      "Epoch 53/10000, Batch 100/188, Loss: 0.3814\n",
      "Epoch 53/10000, Batch 110/188, Loss: 0.3890\n",
      "Epoch 53/10000, Batch 120/188, Loss: 0.4526\n",
      "Epoch 53/10000, Batch 130/188, Loss: 0.7265\n",
      "Epoch 53/10000, Batch 140/188, Loss: 0.4463\n",
      "Epoch 53/10000, Batch 150/188, Loss: 0.3513\n",
      "Epoch 53/10000, Batch 160/188, Loss: 0.5415\n",
      "Epoch 53/10000, Batch 170/188, Loss: 0.2960\n",
      "Epoch 53/10000, Batch 180/188, Loss: 0.3198\n",
      "Epoch 53 Kt thc - Mt mt Hun luyn: 0.4355, IoU Hun luyn: 0.5681, F1-Score Hun luyn: 0.7171\n",
      "Mt mt Xc thc: 0.6120, IoU Xc thc: 0.3908, F1-Score Xc thc: 0.5220\n",
      "Learning Rate hin ti: 0.00000625\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 13/30\n",
      "Epoch 54/10000 Bt u...\n",
      "Epoch 54/10000, Batch 10/188, Loss: 0.4656\n",
      "Epoch 54/10000, Batch 20/188, Loss: 0.3702\n",
      "Epoch 54/10000, Batch 30/188, Loss: 0.4076\n",
      "Epoch 54/10000, Batch 40/188, Loss: 0.4732\n",
      "Epoch 54/10000, Batch 50/188, Loss: 0.5834\n",
      "Epoch 54/10000, Batch 60/188, Loss: 0.3846\n",
      "Epoch 54/10000, Batch 70/188, Loss: 0.4237\n",
      "Epoch 54/10000, Batch 80/188, Loss: 0.4811\n",
      "Epoch 54/10000, Batch 90/188, Loss: 0.4524\n",
      "Epoch 54/10000, Batch 100/188, Loss: 0.4799\n",
      "Epoch 54/10000, Batch 110/188, Loss: 0.4758\n",
      "Epoch 54/10000, Batch 120/188, Loss: 0.4285\n",
      "Epoch 54/10000, Batch 130/188, Loss: 0.3292\n",
      "Epoch 54/10000, Batch 140/188, Loss: 0.4959\n",
      "Epoch 54/10000, Batch 150/188, Loss: 0.3087\n",
      "Epoch 54/10000, Batch 160/188, Loss: 0.5114\n",
      "Epoch 54/10000, Batch 170/188, Loss: 0.4217\n",
      "Epoch 54/10000, Batch 180/188, Loss: 0.3418\n",
      "Epoch 54 Kt thc - Mt mt Hun luyn: 0.4314, IoU Hun luyn: 0.5722, F1-Score Hun luyn: 0.7207\n",
      "Mt mt Xc thc: 0.6129, IoU Xc thc: 0.3899, F1-Score Xc thc: 0.5208\n",
      "Learning Rate hin ti: 0.00000625\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 14/30\n",
      "Epoch 55/10000 Bt u...\n",
      "Epoch 55/10000, Batch 10/188, Loss: 0.7378\n",
      "Epoch 55/10000, Batch 20/188, Loss: 0.3348\n",
      "Epoch 55/10000, Batch 30/188, Loss: 0.2610\n",
      "Epoch 55/10000, Batch 40/188, Loss: 0.3027\n",
      "Epoch 55/10000, Batch 50/188, Loss: 0.4873\n",
      "Epoch 55/10000, Batch 60/188, Loss: 0.4525\n",
      "Epoch 55/10000, Batch 70/188, Loss: 0.7118\n",
      "Epoch 55/10000, Batch 80/188, Loss: 0.3421\n",
      "Epoch 55/10000, Batch 90/188, Loss: 0.3048\n",
      "Epoch 55/10000, Batch 100/188, Loss: 0.3782\n",
      "Epoch 55/10000, Batch 110/188, Loss: 0.2537\n",
      "Epoch 55/10000, Batch 120/188, Loss: 0.3078\n",
      "Epoch 55/10000, Batch 130/188, Loss: 0.2341\n",
      "Epoch 55/10000, Batch 140/188, Loss: 0.2581\n",
      "Epoch 55/10000, Batch 150/188, Loss: 0.5120\n",
      "Epoch 55/10000, Batch 160/188, Loss: 0.4259\n",
      "Epoch 55/10000, Batch 170/188, Loss: 0.5814\n",
      "Epoch 55/10000, Batch 180/188, Loss: 0.4207\n",
      "Epoch 55 Kt thc - Mt mt Hun luyn: 0.4346, IoU Hun luyn: 0.5688, F1-Score Hun luyn: 0.7180\n",
      "Mt mt Xc thc: 0.6117, IoU Xc thc: 0.3911, F1-Score Xc thc: 0.5234\n",
      "Learning Rate hin ti: 0.00000625\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 15/30\n",
      "Epoch 56/10000 Bt u...\n",
      "Epoch 56/10000, Batch 10/188, Loss: 0.5296\n",
      "Epoch 56/10000, Batch 20/188, Loss: 0.4321\n",
      "Epoch 56/10000, Batch 30/188, Loss: 0.5402\n",
      "Epoch 56/10000, Batch 40/188, Loss: 0.6863\n",
      "Epoch 56/10000, Batch 50/188, Loss: 0.5131\n",
      "Epoch 56/10000, Batch 60/188, Loss: 0.4445\n",
      "Epoch 56/10000, Batch 70/188, Loss: 0.3754\n",
      "Epoch 56/10000, Batch 80/188, Loss: 0.3671\n",
      "Epoch 56/10000, Batch 90/188, Loss: 0.4351\n",
      "Epoch 56/10000, Batch 100/188, Loss: 0.6074\n",
      "Epoch 56/10000, Batch 110/188, Loss: 0.4298\n",
      "Epoch 56/10000, Batch 120/188, Loss: 0.5661\n",
      "Epoch 56/10000, Batch 130/188, Loss: 0.5551\n",
      "Epoch 56/10000, Batch 140/188, Loss: 0.4383\n",
      "Epoch 56/10000, Batch 150/188, Loss: 0.2829\n",
      "Epoch 56/10000, Batch 160/188, Loss: 0.2967\n",
      "Epoch 56/10000, Batch 170/188, Loss: 0.3148\n",
      "Epoch 56/10000, Batch 180/188, Loss: 0.3926\n",
      "Epoch 56 Kt thc - Mt mt Hun luyn: 0.4285, IoU Hun luyn: 0.5750, F1-Score Hun luyn: 0.7234\n",
      "Mt mt Xc thc: 0.6148, IoU Xc thc: 0.3879, F1-Score Xc thc: 0.5193\n",
      "Learning Rate hin ti: 0.00000313\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 16/30\n",
      "Epoch 57/10000 Bt u...\n",
      "Epoch 57/10000, Batch 10/188, Loss: 0.6729\n",
      "Epoch 57/10000, Batch 20/188, Loss: 0.1899\n",
      "Epoch 57/10000, Batch 30/188, Loss: 0.3431\n",
      "Epoch 57/10000, Batch 40/188, Loss: 0.5998\n",
      "Epoch 57/10000, Batch 50/188, Loss: 0.4300\n",
      "Epoch 57/10000, Batch 60/188, Loss: 0.2821\n",
      "Epoch 57/10000, Batch 70/188, Loss: 0.4645\n",
      "Epoch 57/10000, Batch 80/188, Loss: 0.3487\n",
      "Epoch 57/10000, Batch 90/188, Loss: 0.5525\n",
      "Epoch 57/10000, Batch 100/188, Loss: 0.5565\n",
      "Epoch 57/10000, Batch 110/188, Loss: 0.5728\n",
      "Epoch 57/10000, Batch 120/188, Loss: 0.5412\n",
      "Epoch 57/10000, Batch 130/188, Loss: 0.6420\n",
      "Epoch 57/10000, Batch 140/188, Loss: 0.2706\n",
      "Epoch 57/10000, Batch 150/188, Loss: 0.5435\n",
      "Epoch 57/10000, Batch 160/188, Loss: 0.3650\n",
      "Epoch 57/10000, Batch 170/188, Loss: 0.3648\n",
      "Epoch 57/10000, Batch 180/188, Loss: 0.4623\n",
      "Epoch 57 Kt thc - Mt mt Hun luyn: 0.4337, IoU Hun luyn: 0.5697, F1-Score Hun luyn: 0.7181\n",
      "Mt mt Xc thc: 0.6103, IoU Xc thc: 0.3925, F1-Score Xc thc: 0.5259\n",
      "Learning Rate hin ti: 0.00000313\n",
      "Mt mt xc thc tt nht c cp nht: 0.6103. Lu m hnh v trng thi...\n",
      "Epoch 58/10000 Bt u...\n",
      "Epoch 58/10000, Batch 10/188, Loss: 0.4228\n",
      "Epoch 58/10000, Batch 20/188, Loss: 0.4375\n",
      "Epoch 58/10000, Batch 30/188, Loss: 0.4143\n",
      "Epoch 58/10000, Batch 40/188, Loss: 0.5940\n",
      "Epoch 58/10000, Batch 50/188, Loss: 0.2898\n",
      "Epoch 58/10000, Batch 60/188, Loss: 0.4095\n",
      "Epoch 58/10000, Batch 70/188, Loss: 0.5074\n",
      "Epoch 58/10000, Batch 80/188, Loss: 0.6608\n",
      "Epoch 58/10000, Batch 90/188, Loss: 0.4918\n",
      "Epoch 58/10000, Batch 100/188, Loss: 0.5791\n",
      "Epoch 58/10000, Batch 110/188, Loss: 0.3970\n",
      "Epoch 58/10000, Batch 120/188, Loss: 0.4166\n",
      "Epoch 58/10000, Batch 130/188, Loss: 0.2456\n",
      "Epoch 58/10000, Batch 140/188, Loss: 0.3232\n",
      "Epoch 58/10000, Batch 150/188, Loss: 0.3214\n",
      "Epoch 58/10000, Batch 160/188, Loss: 0.4554\n",
      "Epoch 58/10000, Batch 170/188, Loss: 0.8645\n",
      "Epoch 58/10000, Batch 180/188, Loss: 0.4447\n",
      "Epoch 58 Kt thc - Mt mt Hun luyn: 0.4280, IoU Hun luyn: 0.5755, F1-Score Hun luyn: 0.7241\n",
      "Mt mt Xc thc: 0.6127, IoU Xc thc: 0.3901, F1-Score Xc thc: 0.5222\n",
      "Learning Rate hin ti: 0.00000313\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 1/30\n",
      "Epoch 59/10000 Bt u...\n",
      "Epoch 59/10000, Batch 10/188, Loss: 0.7541\n",
      "Epoch 59/10000, Batch 20/188, Loss: 0.3674\n",
      "Epoch 59/10000, Batch 30/188, Loss: 0.5453\n",
      "Epoch 59/10000, Batch 40/188, Loss: 0.4436\n",
      "Epoch 59/10000, Batch 50/188, Loss: 0.3835\n",
      "Epoch 59/10000, Batch 60/188, Loss: 0.4161\n",
      "Epoch 59/10000, Batch 70/188, Loss: 0.3362\n",
      "Epoch 59/10000, Batch 80/188, Loss: 0.5315\n",
      "Epoch 59/10000, Batch 90/188, Loss: 0.5821\n",
      "Epoch 59/10000, Batch 100/188, Loss: 0.4651\n",
      "Epoch 59/10000, Batch 110/188, Loss: 0.3893\n",
      "Epoch 59/10000, Batch 120/188, Loss: 0.3227\n",
      "Epoch 59/10000, Batch 130/188, Loss: 0.4823\n",
      "Epoch 59/10000, Batch 140/188, Loss: 0.5135\n",
      "Epoch 59/10000, Batch 150/188, Loss: 0.6577\n",
      "Epoch 59/10000, Batch 160/188, Loss: 0.5874\n",
      "Epoch 59/10000, Batch 170/188, Loss: 0.5749\n",
      "Epoch 59/10000, Batch 180/188, Loss: 0.5036\n",
      "Epoch 59 Kt thc - Mt mt Hun luyn: 0.4297, IoU Hun luyn: 0.5736, F1-Score Hun luyn: 0.7225\n",
      "Mt mt Xc thc: 0.6084, IoU Xc thc: 0.3943, F1-Score Xc thc: 0.5275\n",
      "Learning Rate hin ti: 0.00000313\n",
      "Mt mt xc thc tt nht c cp nht: 0.6084. Lu m hnh v trng thi...\n",
      "Epoch 60/10000 Bt u...\n",
      "Epoch 60/10000, Batch 10/188, Loss: 0.3333\n",
      "Epoch 60/10000, Batch 20/188, Loss: 0.4250\n",
      "Epoch 60/10000, Batch 30/188, Loss: 0.7053\n",
      "Epoch 60/10000, Batch 40/188, Loss: 0.2486\n",
      "Epoch 60/10000, Batch 50/188, Loss: 0.5856\n",
      "Epoch 60/10000, Batch 60/188, Loss: 0.3432\n",
      "Epoch 60/10000, Batch 70/188, Loss: 0.2788\n",
      "Epoch 60/10000, Batch 80/188, Loss: 0.5025\n",
      "Epoch 60/10000, Batch 90/188, Loss: 0.4268\n",
      "Epoch 60/10000, Batch 100/188, Loss: 0.5508\n",
      "Epoch 60/10000, Batch 110/188, Loss: 0.3570\n",
      "Epoch 60/10000, Batch 120/188, Loss: 0.6472\n",
      "Epoch 60/10000, Batch 130/188, Loss: 0.4070\n",
      "Epoch 60/10000, Batch 140/188, Loss: 0.3494\n",
      "Epoch 60/10000, Batch 150/188, Loss: 0.3556\n",
      "Epoch 60/10000, Batch 160/188, Loss: 0.4413\n",
      "Epoch 60/10000, Batch 170/188, Loss: 0.5300\n",
      "Epoch 60/10000, Batch 180/188, Loss: 0.4984\n",
      "Epoch 60 Kt thc - Mt mt Hun luyn: 0.4274, IoU Hun luyn: 0.5760, F1-Score Hun luyn: 0.7251\n",
      "Mt mt Xc thc: 0.6091, IoU Xc thc: 0.3938, F1-Score Xc thc: 0.5272\n",
      "Learning Rate hin ti: 0.00000313\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 1/30\n",
      "Epoch 61/10000 Bt u...\n",
      "Epoch 61/10000, Batch 10/188, Loss: 0.5730\n",
      "Epoch 61/10000, Batch 20/188, Loss: 0.4046\n",
      "Epoch 61/10000, Batch 30/188, Loss: 0.4086\n",
      "Epoch 61/10000, Batch 40/188, Loss: 0.6117\n",
      "Epoch 61/10000, Batch 50/188, Loss: 0.3972\n",
      "Epoch 61/10000, Batch 60/188, Loss: 0.1922\n",
      "Epoch 61/10000, Batch 70/188, Loss: 0.5007\n",
      "Epoch 61/10000, Batch 80/188, Loss: 0.3389\n",
      "Epoch 61/10000, Batch 90/188, Loss: 0.5169\n",
      "Epoch 61/10000, Batch 100/188, Loss: 0.4731\n",
      "Epoch 61/10000, Batch 110/188, Loss: 0.2935\n",
      "Epoch 61/10000, Batch 120/188, Loss: 0.3390\n",
      "Epoch 61/10000, Batch 130/188, Loss: 0.3489\n",
      "Epoch 61/10000, Batch 140/188, Loss: 0.4379\n",
      "Epoch 61/10000, Batch 150/188, Loss: 0.3700\n",
      "Epoch 61/10000, Batch 160/188, Loss: 0.5736\n",
      "Epoch 61/10000, Batch 170/188, Loss: 0.3501\n",
      "Epoch 61/10000, Batch 180/188, Loss: 0.3896\n",
      "Epoch 61 Kt thc - Mt mt Hun luyn: 0.4305, IoU Hun luyn: 0.5729, F1-Score Hun luyn: 0.7217\n",
      "Mt mt Xc thc: 0.6072, IoU Xc thc: 0.3955, F1-Score Xc thc: 0.5285\n",
      "Learning Rate hin ti: 0.00000313\n",
      "Mt mt xc thc tt nht c cp nht: 0.6072. Lu m hnh v trng thi...\n",
      "Epoch 62/10000 Bt u...\n",
      "Epoch 62/10000, Batch 10/188, Loss: 0.5468\n",
      "Epoch 62/10000, Batch 20/188, Loss: 0.6813\n",
      "Epoch 62/10000, Batch 30/188, Loss: 0.5275\n",
      "Epoch 62/10000, Batch 40/188, Loss: 0.3629\n",
      "Epoch 62/10000, Batch 50/188, Loss: 0.2471\n",
      "Epoch 62/10000, Batch 60/188, Loss: 0.3550\n",
      "Epoch 62/10000, Batch 70/188, Loss: 0.2778\n",
      "Epoch 62/10000, Batch 80/188, Loss: 0.2926\n",
      "Epoch 62/10000, Batch 90/188, Loss: 0.3332\n",
      "Epoch 62/10000, Batch 100/188, Loss: 0.4666\n",
      "Epoch 62/10000, Batch 110/188, Loss: 0.3190\n",
      "Epoch 62/10000, Batch 120/188, Loss: 0.5569\n",
      "Epoch 62/10000, Batch 130/188, Loss: 0.2412\n",
      "Epoch 62/10000, Batch 140/188, Loss: 0.6186\n",
      "Epoch 62/10000, Batch 150/188, Loss: 0.4421\n",
      "Epoch 62/10000, Batch 160/188, Loss: 0.1883\n",
      "Epoch 62/10000, Batch 170/188, Loss: 0.4727\n",
      "Epoch 62/10000, Batch 180/188, Loss: 0.5080\n",
      "Epoch 62 Kt thc - Mt mt Hun luyn: 0.4313, IoU Hun luyn: 0.5720, F1-Score Hun luyn: 0.7218\n",
      "Mt mt Xc thc: 0.6095, IoU Xc thc: 0.3932, F1-Score Xc thc: 0.5263\n",
      "Learning Rate hin ti: 0.00000313\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 1/30\n",
      "Epoch 63/10000 Bt u...\n",
      "Epoch 63/10000, Batch 10/188, Loss: 0.3559\n",
      "Epoch 63/10000, Batch 20/188, Loss: 0.4306\n",
      "Epoch 63/10000, Batch 30/188, Loss: 0.6202\n",
      "Epoch 63/10000, Batch 40/188, Loss: 0.3237\n",
      "Epoch 63/10000, Batch 50/188, Loss: 0.6236\n",
      "Epoch 63/10000, Batch 60/188, Loss: 0.4836\n",
      "Epoch 63/10000, Batch 70/188, Loss: 0.5707\n",
      "Epoch 63/10000, Batch 80/188, Loss: 0.5703\n",
      "Epoch 63/10000, Batch 90/188, Loss: 0.5363\n",
      "Epoch 63/10000, Batch 100/188, Loss: 0.4134\n",
      "Epoch 63/10000, Batch 110/188, Loss: 0.3399\n",
      "Epoch 63/10000, Batch 120/188, Loss: 0.3006\n",
      "Epoch 63/10000, Batch 130/188, Loss: 0.4873\n",
      "Epoch 63/10000, Batch 140/188, Loss: 0.5427\n",
      "Epoch 63/10000, Batch 150/188, Loss: 0.2521\n",
      "Epoch 63/10000, Batch 160/188, Loss: 0.5197\n",
      "Epoch 63/10000, Batch 170/188, Loss: 0.3179\n",
      "Epoch 63/10000, Batch 180/188, Loss: 0.2225\n",
      "Epoch 63 Kt thc - Mt mt Hun luyn: 0.4290, IoU Hun luyn: 0.5743, F1-Score Hun luyn: 0.7231\n",
      "Mt mt Xc thc: 0.6125, IoU Xc thc: 0.3900, F1-Score Xc thc: 0.5218\n",
      "Learning Rate hin ti: 0.00000313\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 2/30\n",
      "Epoch 64/10000 Bt u...\n",
      "Epoch 64/10000, Batch 10/188, Loss: 0.6436\n",
      "Epoch 64/10000, Batch 20/188, Loss: 0.6283\n",
      "Epoch 64/10000, Batch 30/188, Loss: 0.3731\n",
      "Epoch 64/10000, Batch 40/188, Loss: 0.5373\n",
      "Epoch 64/10000, Batch 50/188, Loss: 0.2880\n",
      "Epoch 64/10000, Batch 60/188, Loss: 0.3731\n",
      "Epoch 64/10000, Batch 70/188, Loss: 0.4311\n",
      "Epoch 64/10000, Batch 80/188, Loss: 0.3712\n",
      "Epoch 64/10000, Batch 90/188, Loss: 0.4576\n",
      "Epoch 64/10000, Batch 100/188, Loss: 0.4847\n",
      "Epoch 64/10000, Batch 110/188, Loss: 0.2416\n",
      "Epoch 64/10000, Batch 120/188, Loss: 0.3639\n",
      "Epoch 64/10000, Batch 130/188, Loss: 0.5810\n",
      "Epoch 64/10000, Batch 140/188, Loss: 0.3438\n",
      "Epoch 64/10000, Batch 150/188, Loss: 0.4666\n",
      "Epoch 64/10000, Batch 160/188, Loss: 0.3592\n",
      "Epoch 64/10000, Batch 170/188, Loss: 0.3631\n",
      "Epoch 64/10000, Batch 180/188, Loss: 0.4238\n",
      "Epoch 64 Kt thc - Mt mt Hun luyn: 0.4341, IoU Hun luyn: 0.5691, F1-Score Hun luyn: 0.7180\n",
      "Mt mt Xc thc: 0.6150, IoU Xc thc: 0.3876, F1-Score Xc thc: 0.5188\n",
      "Learning Rate hin ti: 0.00000313\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 3/30\n",
      "Epoch 65/10000 Bt u...\n",
      "Epoch 65/10000, Batch 10/188, Loss: 0.3840\n",
      "Epoch 65/10000, Batch 20/188, Loss: 0.3974\n",
      "Epoch 65/10000, Batch 30/188, Loss: 0.2923\n",
      "Epoch 65/10000, Batch 40/188, Loss: 0.2516\n",
      "Epoch 65/10000, Batch 50/188, Loss: 0.2985\n",
      "Epoch 65/10000, Batch 60/188, Loss: 0.2896\n",
      "Epoch 65/10000, Batch 70/188, Loss: 0.2920\n",
      "Epoch 65/10000, Batch 80/188, Loss: 0.6265\n",
      "Epoch 65/10000, Batch 90/188, Loss: 0.4900\n",
      "Epoch 65/10000, Batch 100/188, Loss: 0.6300\n",
      "Epoch 65/10000, Batch 110/188, Loss: 0.3321\n",
      "Epoch 65/10000, Batch 120/188, Loss: 0.5845\n",
      "Epoch 65/10000, Batch 130/188, Loss: 0.5349\n",
      "Epoch 65/10000, Batch 140/188, Loss: 0.5010\n",
      "Epoch 65/10000, Batch 150/188, Loss: 0.6367\n",
      "Epoch 65/10000, Batch 160/188, Loss: 0.3907\n",
      "Epoch 65/10000, Batch 170/188, Loss: 0.3229\n",
      "Epoch 65/10000, Batch 180/188, Loss: 0.5427\n",
      "Epoch 65 Kt thc - Mt mt Hun luyn: 0.4289, IoU Hun luyn: 0.5744, F1-Score Hun luyn: 0.7225\n",
      "Mt mt Xc thc: 0.6101, IoU Xc thc: 0.3925, F1-Score Xc thc: 0.5247\n",
      "Learning Rate hin ti: 0.00000156\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 4/30\n",
      "Epoch 66/10000 Bt u...\n",
      "Epoch 66/10000, Batch 10/188, Loss: 0.3133\n",
      "Epoch 66/10000, Batch 20/188, Loss: 0.2979\n",
      "Epoch 66/10000, Batch 30/188, Loss: 0.3515\n",
      "Epoch 66/10000, Batch 40/188, Loss: 0.5726\n",
      "Epoch 66/10000, Batch 50/188, Loss: 0.6019\n",
      "Epoch 66/10000, Batch 60/188, Loss: 0.4183\n",
      "Epoch 66/10000, Batch 70/188, Loss: 0.3725\n",
      "Epoch 66/10000, Batch 80/188, Loss: 0.3857\n",
      "Epoch 66/10000, Batch 90/188, Loss: 0.3794\n",
      "Epoch 66/10000, Batch 100/188, Loss: 0.2459\n",
      "Epoch 66/10000, Batch 110/188, Loss: 0.5228\n",
      "Epoch 66/10000, Batch 120/188, Loss: 0.3360\n",
      "Epoch 66/10000, Batch 130/188, Loss: 0.4347\n",
      "Epoch 66/10000, Batch 140/188, Loss: 0.4611\n",
      "Epoch 66/10000, Batch 150/188, Loss: 0.2275\n",
      "Epoch 66/10000, Batch 160/188, Loss: 0.2647\n",
      "Epoch 66/10000, Batch 170/188, Loss: 0.3774\n",
      "Epoch 66/10000, Batch 180/188, Loss: 0.3620\n",
      "Epoch 66 Kt thc - Mt mt Hun luyn: 0.4330, IoU Hun luyn: 0.5701, F1-Score Hun luyn: 0.7180\n",
      "Mt mt Xc thc: 0.6112, IoU Xc thc: 0.3914, F1-Score Xc thc: 0.5233\n",
      "Learning Rate hin ti: 0.00000156\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 5/30\n",
      "Epoch 67/10000 Bt u...\n",
      "Epoch 67/10000, Batch 10/188, Loss: 0.5946\n",
      "Epoch 67/10000, Batch 20/188, Loss: 0.4740\n",
      "Epoch 67/10000, Batch 30/188, Loss: 0.3513\n",
      "Epoch 67/10000, Batch 40/188, Loss: 0.5576\n",
      "Epoch 67/10000, Batch 50/188, Loss: 0.3181\n",
      "Epoch 67/10000, Batch 60/188, Loss: 0.3994\n",
      "Epoch 67/10000, Batch 70/188, Loss: 0.2723\n",
      "Epoch 67/10000, Batch 80/188, Loss: 0.4038\n",
      "Epoch 67/10000, Batch 90/188, Loss: 0.5849\n",
      "Epoch 67/10000, Batch 100/188, Loss: 0.3303\n",
      "Epoch 67/10000, Batch 110/188, Loss: 0.3737\n",
      "Epoch 67/10000, Batch 120/188, Loss: 0.3104\n",
      "Epoch 67/10000, Batch 130/188, Loss: 0.5342\n",
      "Epoch 67/10000, Batch 140/188, Loss: 0.5205\n",
      "Epoch 67/10000, Batch 150/188, Loss: 0.4000\n",
      "Epoch 67/10000, Batch 160/188, Loss: 0.4156\n",
      "Epoch 67/10000, Batch 170/188, Loss: 0.3531\n",
      "Epoch 67/10000, Batch 180/188, Loss: 0.5722\n",
      "Epoch 67 Kt thc - Mt mt Hun luyn: 0.4283, IoU Hun luyn: 0.5749, F1-Score Hun luyn: 0.7232\n",
      "Mt mt Xc thc: 0.6152, IoU Xc thc: 0.3874, F1-Score Xc thc: 0.5177\n",
      "Learning Rate hin ti: 0.00000156\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 6/30\n",
      "Epoch 68/10000 Bt u...\n",
      "Epoch 68/10000, Batch 10/188, Loss: 0.2962\n",
      "Epoch 68/10000, Batch 20/188, Loss: 0.3494\n",
      "Epoch 68/10000, Batch 30/188, Loss: 0.4509\n",
      "Epoch 68/10000, Batch 40/188, Loss: 0.4883\n",
      "Epoch 68/10000, Batch 50/188, Loss: 0.2989\n",
      "Epoch 68/10000, Batch 60/188, Loss: 0.6179\n",
      "Epoch 68/10000, Batch 70/188, Loss: 0.5921\n",
      "Epoch 68/10000, Batch 80/188, Loss: 0.4326\n",
      "Epoch 68/10000, Batch 90/188, Loss: 0.3559\n",
      "Epoch 68/10000, Batch 100/188, Loss: 0.4660\n",
      "Epoch 68/10000, Batch 110/188, Loss: 0.7018\n",
      "Epoch 68/10000, Batch 120/188, Loss: 0.2640\n",
      "Epoch 68/10000, Batch 130/188, Loss: 0.3998\n",
      "Epoch 68/10000, Batch 140/188, Loss: 0.5171\n",
      "Epoch 68/10000, Batch 150/188, Loss: 0.2232\n",
      "Epoch 68/10000, Batch 160/188, Loss: 0.4449\n",
      "Epoch 68/10000, Batch 170/188, Loss: 0.4984\n",
      "Epoch 68/10000, Batch 180/188, Loss: 0.4184\n",
      "Epoch 68 Kt thc - Mt mt Hun luyn: 0.4257, IoU Hun luyn: 0.5775, F1-Score Hun luyn: 0.7257\n",
      "Mt mt Xc thc: 0.6105, IoU Xc thc: 0.3920, F1-Score Xc thc: 0.5246\n",
      "Learning Rate hin ti: 0.00000156\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 7/30\n",
      "Epoch 69/10000 Bt u...\n",
      "Epoch 69/10000, Batch 10/188, Loss: 0.3154\n",
      "Epoch 69/10000, Batch 20/188, Loss: 0.3578\n",
      "Epoch 69/10000, Batch 30/188, Loss: 0.3074\n",
      "Epoch 69/10000, Batch 40/188, Loss: 0.2626\n",
      "Epoch 69/10000, Batch 50/188, Loss: 0.4382\n",
      "Epoch 69/10000, Batch 60/188, Loss: 0.5111\n",
      "Epoch 69/10000, Batch 70/188, Loss: 0.3102\n",
      "Epoch 69/10000, Batch 80/188, Loss: 0.2237\n",
      "Epoch 69/10000, Batch 90/188, Loss: 0.4007\n",
      "Epoch 69/10000, Batch 100/188, Loss: 0.2473\n",
      "Epoch 69/10000, Batch 110/188, Loss: 0.5873\n",
      "Epoch 69/10000, Batch 120/188, Loss: 0.5262\n",
      "Epoch 69/10000, Batch 130/188, Loss: 0.7406\n",
      "Epoch 69/10000, Batch 140/188, Loss: 0.5721\n",
      "Epoch 69/10000, Batch 150/188, Loss: 0.4899\n",
      "Epoch 69/10000, Batch 160/188, Loss: 0.6446\n",
      "Epoch 69/10000, Batch 170/188, Loss: 0.2349\n",
      "Epoch 69/10000, Batch 180/188, Loss: 0.4474\n",
      "Epoch 69 Kt thc - Mt mt Hun luyn: 0.4266, IoU Hun luyn: 0.5767, F1-Score Hun luyn: 0.7247\n",
      "Mt mt Xc thc: 0.6094, IoU Xc thc: 0.3931, F1-Score Xc thc: 0.5262\n",
      "Learning Rate hin ti: 0.00000078\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 8/30\n",
      "Epoch 70/10000 Bt u...\n",
      "Epoch 70/10000, Batch 10/188, Loss: 0.6009\n",
      "Epoch 70/10000, Batch 20/188, Loss: 0.3040\n",
      "Epoch 70/10000, Batch 30/188, Loss: 0.3881\n",
      "Epoch 70/10000, Batch 40/188, Loss: 0.5747\n",
      "Epoch 70/10000, Batch 50/188, Loss: 0.4037\n",
      "Epoch 70/10000, Batch 60/188, Loss: 0.5886\n",
      "Epoch 70/10000, Batch 70/188, Loss: 0.3062\n",
      "Epoch 70/10000, Batch 80/188, Loss: 0.5887\n",
      "Epoch 70/10000, Batch 90/188, Loss: 0.2580\n",
      "Epoch 70/10000, Batch 100/188, Loss: 0.4284\n",
      "Epoch 70/10000, Batch 110/188, Loss: 0.4732\n",
      "Epoch 70/10000, Batch 120/188, Loss: 0.4409\n",
      "Epoch 70/10000, Batch 130/188, Loss: 0.2421\n",
      "Epoch 70/10000, Batch 140/188, Loss: 0.4451\n",
      "Epoch 70/10000, Batch 150/188, Loss: 0.4839\n",
      "Epoch 70/10000, Batch 160/188, Loss: 0.2278\n",
      "Epoch 70/10000, Batch 170/188, Loss: 0.2232\n",
      "Epoch 70/10000, Batch 180/188, Loss: 0.5939\n",
      "Epoch 70 Kt thc - Mt mt Hun luyn: 0.4295, IoU Hun luyn: 0.5736, F1-Score Hun luyn: 0.7212\n",
      "Mt mt Xc thc: 0.6094, IoU Xc thc: 0.3932, F1-Score Xc thc: 0.5268\n",
      "Learning Rate hin ti: 0.00000078\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 9/30\n",
      "Epoch 71/10000 Bt u...\n",
      "Epoch 71/10000, Batch 10/188, Loss: 0.5039\n",
      "Epoch 71/10000, Batch 20/188, Loss: 0.3942\n",
      "Epoch 71/10000, Batch 30/188, Loss: 0.4985\n",
      "Epoch 71/10000, Batch 40/188, Loss: 0.3600\n",
      "Epoch 71/10000, Batch 50/188, Loss: 0.4175\n",
      "Epoch 71/10000, Batch 60/188, Loss: 0.3601\n",
      "Epoch 71/10000, Batch 70/188, Loss: 0.3721\n",
      "Epoch 71/10000, Batch 80/188, Loss: 0.5909\n",
      "Epoch 71/10000, Batch 90/188, Loss: 0.4675\n",
      "Epoch 71/10000, Batch 100/188, Loss: 0.3759\n",
      "Epoch 71/10000, Batch 110/188, Loss: 0.4560\n",
      "Epoch 71/10000, Batch 120/188, Loss: 0.3893\n",
      "Epoch 71/10000, Batch 130/188, Loss: 0.5332\n",
      "Epoch 71/10000, Batch 140/188, Loss: 0.4791\n",
      "Epoch 71/10000, Batch 150/188, Loss: 0.3410\n",
      "Epoch 71/10000, Batch 160/188, Loss: 0.4555\n",
      "Epoch 71/10000, Batch 170/188, Loss: 0.4465\n",
      "Epoch 71/10000, Batch 180/188, Loss: 0.3793\n",
      "Epoch 71 Kt thc - Mt mt Hun luyn: 0.4291, IoU Hun luyn: 0.5742, F1-Score Hun luyn: 0.7218\n",
      "Mt mt Xc thc: 0.6087, IoU Xc thc: 0.3939, F1-Score Xc thc: 0.5272\n",
      "Learning Rate hin ti: 0.00000078\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 10/30\n",
      "Epoch 72/10000 Bt u...\n",
      "Epoch 72/10000, Batch 10/188, Loss: 0.5089\n",
      "Epoch 72/10000, Batch 20/188, Loss: 0.3912\n",
      "Epoch 72/10000, Batch 30/188, Loss: 0.4085\n",
      "Epoch 72/10000, Batch 40/188, Loss: 0.4487\n",
      "Epoch 72/10000, Batch 50/188, Loss: 0.6012\n",
      "Epoch 72/10000, Batch 60/188, Loss: 0.5426\n",
      "Epoch 72/10000, Batch 70/188, Loss: 0.4077\n",
      "Epoch 72/10000, Batch 80/188, Loss: 0.6071\n",
      "Epoch 72/10000, Batch 90/188, Loss: 0.3422\n",
      "Epoch 72/10000, Batch 100/188, Loss: 0.4014\n",
      "Epoch 72/10000, Batch 110/188, Loss: 0.3516\n",
      "Epoch 72/10000, Batch 120/188, Loss: 0.5155\n",
      "Epoch 72/10000, Batch 130/188, Loss: 0.3807\n",
      "Epoch 72/10000, Batch 140/188, Loss: 0.2116\n",
      "Epoch 72/10000, Batch 150/188, Loss: 0.4321\n",
      "Epoch 72/10000, Batch 160/188, Loss: 0.3465\n",
      "Epoch 72/10000, Batch 170/188, Loss: 0.3146\n",
      "Epoch 72/10000, Batch 180/188, Loss: 0.4363\n",
      "Epoch 72 Kt thc - Mt mt Hun luyn: 0.4281, IoU Hun luyn: 0.5751, F1-Score Hun luyn: 0.7245\n",
      "Mt mt Xc thc: 0.6098, IoU Xc thc: 0.3929, F1-Score Xc thc: 0.5256\n",
      "Learning Rate hin ti: 0.00000078\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 11/30\n",
      "Epoch 73/10000 Bt u...\n",
      "Epoch 73/10000, Batch 10/188, Loss: 0.5323\n",
      "Epoch 73/10000, Batch 20/188, Loss: 0.2634\n",
      "Epoch 73/10000, Batch 30/188, Loss: 0.4087\n",
      "Epoch 73/10000, Batch 40/188, Loss: 0.3514\n",
      "Epoch 73/10000, Batch 50/188, Loss: 0.5847\n",
      "Epoch 73/10000, Batch 60/188, Loss: 0.2002\n",
      "Epoch 73/10000, Batch 70/188, Loss: 0.6677\n",
      "Epoch 73/10000, Batch 80/188, Loss: 0.5445\n",
      "Epoch 73/10000, Batch 90/188, Loss: 0.4152\n",
      "Epoch 73/10000, Batch 100/188, Loss: 0.4087\n",
      "Epoch 73/10000, Batch 110/188, Loss: 0.2652\n",
      "Epoch 73/10000, Batch 120/188, Loss: 0.3842\n",
      "Epoch 73/10000, Batch 130/188, Loss: 0.3170\n",
      "Epoch 73/10000, Batch 140/188, Loss: 0.3656\n",
      "Epoch 73/10000, Batch 150/188, Loss: 0.3538\n",
      "Epoch 73/10000, Batch 160/188, Loss: 0.4501\n",
      "Epoch 73/10000, Batch 170/188, Loss: 0.4015\n",
      "Epoch 73/10000, Batch 180/188, Loss: 0.4970\n",
      "Epoch 73 Kt thc - Mt mt Hun luyn: 0.4312, IoU Hun luyn: 0.5720, F1-Score Hun luyn: 0.7210\n",
      "Mt mt Xc thc: 0.6111, IoU Xc thc: 0.3912, F1-Score Xc thc: 0.5235\n",
      "Learning Rate hin ti: 0.00000039\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 12/30\n",
      "Epoch 74/10000 Bt u...\n",
      "Epoch 74/10000, Batch 10/188, Loss: 0.3695\n",
      "Epoch 74/10000, Batch 20/188, Loss: 0.4768\n",
      "Epoch 74/10000, Batch 30/188, Loss: 0.3887\n",
      "Epoch 74/10000, Batch 40/188, Loss: 0.4064\n",
      "Epoch 74/10000, Batch 50/188, Loss: 0.2729\n",
      "Epoch 74/10000, Batch 60/188, Loss: 0.4131\n",
      "Epoch 74/10000, Batch 70/188, Loss: 0.4444\n",
      "Epoch 74/10000, Batch 80/188, Loss: 0.3793\n",
      "Epoch 74/10000, Batch 90/188, Loss: 0.3097\n",
      "Epoch 74/10000, Batch 100/188, Loss: 0.5083\n",
      "Epoch 74/10000, Batch 110/188, Loss: 0.2571\n",
      "Epoch 74/10000, Batch 120/188, Loss: 0.4249\n",
      "Epoch 74/10000, Batch 130/188, Loss: 0.4447\n",
      "Epoch 74/10000, Batch 140/188, Loss: 0.4515\n",
      "Epoch 74/10000, Batch 150/188, Loss: 0.6763\n",
      "Epoch 74/10000, Batch 160/188, Loss: 0.4959\n",
      "Epoch 74/10000, Batch 170/188, Loss: 0.5258\n",
      "Epoch 74/10000, Batch 180/188, Loss: 0.3173\n",
      "Epoch 74 Kt thc - Mt mt Hun luyn: 0.4277, IoU Hun luyn: 0.5755, F1-Score Hun luyn: 0.7242\n",
      "Mt mt Xc thc: 0.6103, IoU Xc thc: 0.3923, F1-Score Xc thc: 0.5249\n",
      "Learning Rate hin ti: 0.00000039\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 13/30\n",
      "Epoch 75/10000 Bt u...\n",
      "Epoch 75/10000, Batch 10/188, Loss: 0.5331\n",
      "Epoch 75/10000, Batch 20/188, Loss: 0.4099\n",
      "Epoch 75/10000, Batch 30/188, Loss: 0.4337\n",
      "Epoch 75/10000, Batch 40/188, Loss: 0.2991\n",
      "Epoch 75/10000, Batch 50/188, Loss: 0.4582\n",
      "Epoch 75/10000, Batch 60/188, Loss: 0.4418\n",
      "Epoch 75/10000, Batch 70/188, Loss: 0.4518\n",
      "Epoch 75/10000, Batch 80/188, Loss: 0.3491\n",
      "Epoch 75/10000, Batch 90/188, Loss: 0.6029\n",
      "Epoch 75/10000, Batch 100/188, Loss: 0.4573\n",
      "Epoch 75/10000, Batch 110/188, Loss: 0.6145\n",
      "Epoch 75/10000, Batch 120/188, Loss: 0.5055\n",
      "Epoch 75/10000, Batch 130/188, Loss: 0.2953\n",
      "Epoch 75/10000, Batch 140/188, Loss: 0.3891\n",
      "Epoch 75/10000, Batch 150/188, Loss: 0.4708\n",
      "Epoch 75/10000, Batch 160/188, Loss: 0.4422\n",
      "Epoch 75/10000, Batch 170/188, Loss: 0.2672\n",
      "Epoch 75/10000, Batch 180/188, Loss: 0.4288\n",
      "Epoch 75 Kt thc - Mt mt Hun luyn: 0.4259, IoU Hun luyn: 0.5772, F1-Score Hun luyn: 0.7248\n",
      "Mt mt Xc thc: 0.6102, IoU Xc thc: 0.3923, F1-Score Xc thc: 0.5252\n",
      "Learning Rate hin ti: 0.00000039\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 14/30\n",
      "Epoch 76/10000 Bt u...\n",
      "Epoch 76/10000, Batch 10/188, Loss: 0.3091\n",
      "Epoch 76/10000, Batch 20/188, Loss: 0.3334\n",
      "Epoch 76/10000, Batch 30/188, Loss: 0.6656\n",
      "Epoch 76/10000, Batch 40/188, Loss: 0.4897\n",
      "Epoch 76/10000, Batch 50/188, Loss: 0.4434\n",
      "Epoch 76/10000, Batch 60/188, Loss: 0.1624\n",
      "Epoch 76/10000, Batch 70/188, Loss: 0.2699\n",
      "Epoch 76/10000, Batch 80/188, Loss: 0.3529\n",
      "Epoch 76/10000, Batch 90/188, Loss: 0.4793\n",
      "Epoch 76/10000, Batch 100/188, Loss: 0.3325\n",
      "Epoch 76/10000, Batch 110/188, Loss: 0.3874\n",
      "Epoch 76/10000, Batch 120/188, Loss: 0.4444\n",
      "Epoch 76/10000, Batch 130/188, Loss: 0.3029\n",
      "Epoch 76/10000, Batch 140/188, Loss: 0.3325\n",
      "Epoch 76/10000, Batch 150/188, Loss: 0.5032\n",
      "Epoch 76/10000, Batch 160/188, Loss: 0.4743\n",
      "Epoch 76/10000, Batch 170/188, Loss: 0.3775\n",
      "Epoch 76/10000, Batch 180/188, Loss: 0.3909\n",
      "Epoch 76 Kt thc - Mt mt Hun luyn: 0.4268, IoU Hun luyn: 0.5764, F1-Score Hun luyn: 0.7252\n",
      "Mt mt Xc thc: 0.6097, IoU Xc thc: 0.3929, F1-Score Xc thc: 0.5262\n",
      "Learning Rate hin ti: 0.00000039\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 15/30\n",
      "Epoch 77/10000 Bt u...\n",
      "Epoch 77/10000, Batch 10/188, Loss: 0.2698\n",
      "Epoch 77/10000, Batch 20/188, Loss: 0.3321\n",
      "Epoch 77/10000, Batch 30/188, Loss: 0.4667\n",
      "Epoch 77/10000, Batch 40/188, Loss: 0.4461\n",
      "Epoch 77/10000, Batch 50/188, Loss: 0.4002\n",
      "Epoch 77/10000, Batch 60/188, Loss: 0.4765\n",
      "Epoch 77/10000, Batch 70/188, Loss: 0.3786\n",
      "Epoch 77/10000, Batch 80/188, Loss: 0.3813\n",
      "Epoch 77/10000, Batch 90/188, Loss: 0.6806\n",
      "Epoch 77/10000, Batch 100/188, Loss: 0.5713\n",
      "Epoch 77/10000, Batch 110/188, Loss: 0.2822\n",
      "Epoch 77/10000, Batch 120/188, Loss: 0.5573\n",
      "Epoch 77/10000, Batch 130/188, Loss: 0.5253\n",
      "Epoch 77/10000, Batch 140/188, Loss: 0.5828\n",
      "Epoch 77/10000, Batch 150/188, Loss: 0.2125\n",
      "Epoch 77/10000, Batch 160/188, Loss: 0.1918\n",
      "Epoch 77/10000, Batch 170/188, Loss: 0.6413\n",
      "Epoch 77/10000, Batch 180/188, Loss: 0.4973\n",
      "Epoch 77 Kt thc - Mt mt Hun luyn: 0.4281, IoU Hun luyn: 0.5752, F1-Score Hun luyn: 0.7241\n",
      "Mt mt Xc thc: 0.6121, IoU Xc thc: 0.3903, F1-Score Xc thc: 0.5223\n",
      "Learning Rate hin ti: 0.00000020\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 16/30\n",
      "Epoch 78/10000 Bt u...\n",
      "Epoch 78/10000, Batch 10/188, Loss: 0.4743\n",
      "Epoch 78/10000, Batch 20/188, Loss: 0.4695\n",
      "Epoch 78/10000, Batch 30/188, Loss: 0.3292\n",
      "Epoch 78/10000, Batch 40/188, Loss: 0.4239\n",
      "Epoch 78/10000, Batch 50/188, Loss: 0.5197\n",
      "Epoch 78/10000, Batch 60/188, Loss: 0.4898\n",
      "Epoch 78/10000, Batch 70/188, Loss: 0.2824\n",
      "Epoch 78/10000, Batch 80/188, Loss: 0.4250\n",
      "Epoch 78/10000, Batch 90/188, Loss: 0.5208\n",
      "Epoch 78/10000, Batch 100/188, Loss: 0.5101\n",
      "Epoch 78/10000, Batch 110/188, Loss: 0.4102\n",
      "Epoch 78/10000, Batch 120/188, Loss: 0.4930\n",
      "Epoch 78/10000, Batch 130/188, Loss: 0.3579\n",
      "Epoch 78/10000, Batch 140/188, Loss: 0.2572\n",
      "Epoch 78/10000, Batch 150/188, Loss: 0.3296\n",
      "Epoch 78/10000, Batch 160/188, Loss: 0.3233\n",
      "Epoch 78/10000, Batch 170/188, Loss: 0.4192\n",
      "Epoch 78/10000, Batch 180/188, Loss: 0.3985\n",
      "Epoch 78 Kt thc - Mt mt Hun luyn: 0.4256, IoU Hun luyn: 0.5776, F1-Score Hun luyn: 0.7262\n",
      "Mt mt Xc thc: 0.6108, IoU Xc thc: 0.3916, F1-Score Xc thc: 0.5247\n",
      "Learning Rate hin ti: 0.00000020\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 17/30\n",
      "Epoch 79/10000 Bt u...\n",
      "Epoch 79/10000, Batch 10/188, Loss: 0.3059\n",
      "Epoch 79/10000, Batch 20/188, Loss: 0.5686\n",
      "Epoch 79/10000, Batch 30/188, Loss: 0.3322\n",
      "Epoch 79/10000, Batch 40/188, Loss: 0.3031\n",
      "Epoch 79/10000, Batch 50/188, Loss: 0.4679\n",
      "Epoch 79/10000, Batch 60/188, Loss: 0.7863\n",
      "Epoch 79/10000, Batch 70/188, Loss: 0.5783\n",
      "Epoch 79/10000, Batch 80/188, Loss: 0.4282\n",
      "Epoch 79/10000, Batch 90/188, Loss: 0.7084\n",
      "Epoch 79/10000, Batch 100/188, Loss: 0.3922\n",
      "Epoch 79/10000, Batch 110/188, Loss: 0.5528\n",
      "Epoch 79/10000, Batch 120/188, Loss: 0.4815\n",
      "Epoch 79/10000, Batch 130/188, Loss: 0.4435\n",
      "Epoch 79/10000, Batch 140/188, Loss: 0.4323\n",
      "Epoch 79/10000, Batch 150/188, Loss: 0.3545\n",
      "Epoch 79/10000, Batch 160/188, Loss: 0.3431\n",
      "Epoch 79/10000, Batch 170/188, Loss: 0.4796\n",
      "Epoch 79/10000, Batch 180/188, Loss: 0.6883\n",
      "Epoch 79 Kt thc - Mt mt Hun luyn: 0.4251, IoU Hun luyn: 0.5781, F1-Score Hun luyn: 0.7261\n",
      "Mt mt Xc thc: 0.6112, IoU Xc thc: 0.3914, F1-Score Xc thc: 0.5237\n",
      "Learning Rate hin ti: 0.00000020\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 18/30\n",
      "Epoch 80/10000 Bt u...\n",
      "Epoch 80/10000, Batch 10/188, Loss: 0.2823\n",
      "Epoch 80/10000, Batch 20/188, Loss: 0.2621\n",
      "Epoch 80/10000, Batch 30/188, Loss: 0.3109\n",
      "Epoch 80/10000, Batch 40/188, Loss: 0.5316\n",
      "Epoch 80/10000, Batch 50/188, Loss: 0.1520\n",
      "Epoch 80/10000, Batch 60/188, Loss: 0.3719\n",
      "Epoch 80/10000, Batch 70/188, Loss: 0.4253\n",
      "Epoch 80/10000, Batch 80/188, Loss: 0.4622\n",
      "Epoch 80/10000, Batch 90/188, Loss: 0.3656\n",
      "Epoch 80/10000, Batch 100/188, Loss: 0.5347\n",
      "Epoch 80/10000, Batch 110/188, Loss: 0.2683\n",
      "Epoch 80/10000, Batch 120/188, Loss: 0.5425\n",
      "Epoch 80/10000, Batch 130/188, Loss: 0.2732\n",
      "Epoch 80/10000, Batch 140/188, Loss: 0.4378\n",
      "Epoch 80/10000, Batch 150/188, Loss: 0.5573\n",
      "Epoch 80/10000, Batch 160/188, Loss: 0.4057\n",
      "Epoch 80/10000, Batch 170/188, Loss: 0.5251\n",
      "Epoch 80/10000, Batch 180/188, Loss: 0.3482\n",
      "Epoch 80 Kt thc - Mt mt Hun luyn: 0.4194, IoU Hun luyn: 0.5836, F1-Score Hun luyn: 0.7301\n",
      "Mt mt Xc thc: 0.6140, IoU Xc thc: 0.3885, F1-Score Xc thc: 0.5196\n",
      "Learning Rate hin ti: 0.00000020\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 19/30\n",
      "Epoch 81/10000 Bt u...\n",
      "Epoch 81/10000, Batch 10/188, Loss: 0.3596\n",
      "Epoch 81/10000, Batch 20/188, Loss: 0.6145\n",
      "Epoch 81/10000, Batch 30/188, Loss: 0.5119\n",
      "Epoch 81/10000, Batch 40/188, Loss: 0.4039\n",
      "Epoch 81/10000, Batch 50/188, Loss: 0.2784\n",
      "Epoch 81/10000, Batch 60/188, Loss: 0.3421\n",
      "Epoch 81/10000, Batch 70/188, Loss: 0.3350\n",
      "Epoch 81/10000, Batch 80/188, Loss: 0.3405\n",
      "Epoch 81/10000, Batch 90/188, Loss: 0.2587\n",
      "Epoch 81/10000, Batch 100/188, Loss: 0.5082\n",
      "Epoch 81/10000, Batch 110/188, Loss: 0.5340\n",
      "Epoch 81/10000, Batch 120/188, Loss: 0.3306\n",
      "Epoch 81/10000, Batch 130/188, Loss: 0.4636\n",
      "Epoch 81/10000, Batch 140/188, Loss: 0.4405\n",
      "Epoch 81/10000, Batch 150/188, Loss: 0.3700\n",
      "Epoch 81/10000, Batch 160/188, Loss: 0.5799\n",
      "Epoch 81/10000, Batch 170/188, Loss: 0.4557\n",
      "Epoch 81/10000, Batch 180/188, Loss: 0.5973\n",
      "Epoch 81 Kt thc - Mt mt Hun luyn: 0.4295, IoU Hun luyn: 0.5737, F1-Score Hun luyn: 0.7229\n",
      "Mt mt Xc thc: 0.6161, IoU Xc thc: 0.3862, F1-Score Xc thc: 0.5160\n",
      "Learning Rate hin ti: 0.00000010\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 20/30\n",
      "Epoch 82/10000 Bt u...\n",
      "Epoch 82/10000, Batch 10/188, Loss: 0.3815\n",
      "Epoch 82/10000, Batch 20/188, Loss: 0.5372\n",
      "Epoch 82/10000, Batch 30/188, Loss: 0.6964\n",
      "Epoch 82/10000, Batch 40/188, Loss: 0.4018\n",
      "Epoch 82/10000, Batch 50/188, Loss: 0.6656\n",
      "Epoch 82/10000, Batch 60/188, Loss: 0.4405\n",
      "Epoch 82/10000, Batch 70/188, Loss: 0.2489\n",
      "Epoch 82/10000, Batch 80/188, Loss: 0.6135\n",
      "Epoch 82/10000, Batch 90/188, Loss: 0.4110\n",
      "Epoch 82/10000, Batch 100/188, Loss: 0.3033\n",
      "Epoch 82/10000, Batch 110/188, Loss: 0.5148\n",
      "Epoch 82/10000, Batch 120/188, Loss: 0.5231\n",
      "Epoch 82/10000, Batch 130/188, Loss: 0.7296\n",
      "Epoch 82/10000, Batch 140/188, Loss: 0.5007\n",
      "Epoch 82/10000, Batch 150/188, Loss: 0.3007\n",
      "Epoch 82/10000, Batch 160/188, Loss: 0.4640\n",
      "Epoch 82/10000, Batch 170/188, Loss: 0.5392\n",
      "Epoch 82/10000, Batch 180/188, Loss: 0.3632\n",
      "Epoch 82 Kt thc - Mt mt Hun luyn: 0.4312, IoU Hun luyn: 0.5719, F1-Score Hun luyn: 0.7192\n",
      "Mt mt Xc thc: 0.6128, IoU Xc thc: 0.3897, F1-Score Xc thc: 0.5219\n",
      "Learning Rate hin ti: 0.00000010\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 21/30\n",
      "Epoch 83/10000 Bt u...\n",
      "Epoch 83/10000, Batch 10/188, Loss: 0.3906\n",
      "Epoch 83/10000, Batch 20/188, Loss: 0.4192\n",
      "Epoch 83/10000, Batch 30/188, Loss: 0.4765\n",
      "Epoch 83/10000, Batch 40/188, Loss: 0.6018\n",
      "Epoch 83/10000, Batch 50/188, Loss: 0.5840\n",
      "Epoch 83/10000, Batch 60/188, Loss: 0.4876\n",
      "Epoch 83/10000, Batch 70/188, Loss: 0.5000\n",
      "Epoch 83/10000, Batch 80/188, Loss: 0.5297\n",
      "Epoch 83/10000, Batch 90/188, Loss: 0.5276\n",
      "Epoch 83/10000, Batch 100/188, Loss: 0.5655\n",
      "Epoch 83/10000, Batch 110/188, Loss: 0.3381\n",
      "Epoch 83/10000, Batch 120/188, Loss: 0.4484\n",
      "Epoch 83/10000, Batch 130/188, Loss: 0.4583\n",
      "Epoch 83/10000, Batch 140/188, Loss: 0.6324\n",
      "Epoch 83/10000, Batch 150/188, Loss: 0.4825\n",
      "Epoch 83/10000, Batch 160/188, Loss: 0.3656\n",
      "Epoch 83/10000, Batch 170/188, Loss: 0.1817\n",
      "Epoch 83/10000, Batch 180/188, Loss: 0.4681\n",
      "Epoch 83 Kt thc - Mt mt Hun luyn: 0.4276, IoU Hun luyn: 0.5756, F1-Score Hun luyn: 0.7242\n",
      "Mt mt Xc thc: 0.6120, IoU Xc thc: 0.3904, F1-Score Xc thc: 0.5228\n",
      "Learning Rate hin ti: 0.00000010\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 22/30\n",
      "Epoch 84/10000 Bt u...\n",
      "Epoch 84/10000, Batch 10/188, Loss: 0.3664\n",
      "Epoch 84/10000, Batch 20/188, Loss: 0.5168\n",
      "Epoch 84/10000, Batch 30/188, Loss: 0.7435\n",
      "Epoch 84/10000, Batch 40/188, Loss: 0.5983\n",
      "Epoch 84/10000, Batch 50/188, Loss: 0.1654\n",
      "Epoch 84/10000, Batch 60/188, Loss: 0.2353\n",
      "Epoch 84/10000, Batch 70/188, Loss: 0.4680\n",
      "Epoch 84/10000, Batch 80/188, Loss: 0.3529\n",
      "Epoch 84/10000, Batch 90/188, Loss: 0.3482\n",
      "Epoch 84/10000, Batch 100/188, Loss: 0.4483\n",
      "Epoch 84/10000, Batch 110/188, Loss: 0.4906\n",
      "Epoch 84/10000, Batch 120/188, Loss: 0.2685\n",
      "Epoch 84/10000, Batch 130/188, Loss: 0.4454\n",
      "Epoch 84/10000, Batch 140/188, Loss: 0.3702\n",
      "Epoch 84/10000, Batch 150/188, Loss: 0.2644\n",
      "Epoch 84/10000, Batch 160/188, Loss: 0.4685\n",
      "Epoch 84/10000, Batch 170/188, Loss: 0.7675\n",
      "Epoch 84/10000, Batch 180/188, Loss: 0.3035\n",
      "Epoch 84 Kt thc - Mt mt Hun luyn: 0.4301, IoU Hun luyn: 0.5730, F1-Score Hun luyn: 0.7214\n",
      "Mt mt Xc thc: 0.6122, IoU Xc thc: 0.3903, F1-Score Xc thc: 0.5222\n",
      "Learning Rate hin ti: 0.00000010\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 23/30\n",
      "Epoch 85/10000 Bt u...\n",
      "Epoch 85/10000, Batch 10/188, Loss: 0.2958\n",
      "Epoch 85/10000, Batch 20/188, Loss: 0.3441\n",
      "Epoch 85/10000, Batch 30/188, Loss: 0.3146\n",
      "Epoch 85/10000, Batch 40/188, Loss: 0.2165\n",
      "Epoch 85/10000, Batch 50/188, Loss: 0.3883\n",
      "Epoch 85/10000, Batch 60/188, Loss: 0.3768\n",
      "Epoch 85/10000, Batch 70/188, Loss: 0.3092\n",
      "Epoch 85/10000, Batch 80/188, Loss: 0.3467\n",
      "Epoch 85/10000, Batch 90/188, Loss: 0.4827\n",
      "Epoch 85/10000, Batch 100/188, Loss: 0.6329\n",
      "Epoch 85/10000, Batch 110/188, Loss: 0.3709\n",
      "Epoch 85/10000, Batch 120/188, Loss: 0.4939\n",
      "Epoch 85/10000, Batch 130/188, Loss: 0.7027\n",
      "Epoch 85/10000, Batch 140/188, Loss: 0.5188\n",
      "Epoch 85/10000, Batch 150/188, Loss: 0.4648\n",
      "Epoch 85/10000, Batch 160/188, Loss: 0.2334\n",
      "Epoch 85/10000, Batch 170/188, Loss: 0.6000\n",
      "Epoch 85/10000, Batch 180/188, Loss: 0.4225\n",
      "Epoch 85 Kt thc - Mt mt Hun luyn: 0.4268, IoU Hun luyn: 0.5764, F1-Score Hun luyn: 0.7247\n",
      "Mt mt Xc thc: 0.6093, IoU Xc thc: 0.3932, F1-Score Xc thc: 0.5270\n",
      "Learning Rate hin ti: 0.00000010\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 24/30\n",
      "Epoch 86/10000 Bt u...\n",
      "Epoch 86/10000, Batch 10/188, Loss: 0.4945\n",
      "Epoch 86/10000, Batch 20/188, Loss: 0.4024\n",
      "Epoch 86/10000, Batch 30/188, Loss: 0.3593\n",
      "Epoch 86/10000, Batch 40/188, Loss: 0.5909\n",
      "Epoch 86/10000, Batch 50/188, Loss: 0.3455\n",
      "Epoch 86/10000, Batch 60/188, Loss: 0.6631\n",
      "Epoch 86/10000, Batch 70/188, Loss: 0.3954\n",
      "Epoch 86/10000, Batch 80/188, Loss: 0.4976\n",
      "Epoch 86/10000, Batch 90/188, Loss: 0.3553\n",
      "Epoch 86/10000, Batch 100/188, Loss: 0.5162\n",
      "Epoch 86/10000, Batch 110/188, Loss: 0.5125\n",
      "Epoch 86/10000, Batch 120/188, Loss: 0.5220\n",
      "Epoch 86/10000, Batch 130/188, Loss: 0.3080\n",
      "Epoch 86/10000, Batch 140/188, Loss: 0.4222\n",
      "Epoch 86/10000, Batch 150/188, Loss: 0.4610\n",
      "Epoch 86/10000, Batch 160/188, Loss: 0.3188\n",
      "Epoch 86/10000, Batch 170/188, Loss: 0.3014\n",
      "Epoch 86/10000, Batch 180/188, Loss: 0.4697\n",
      "Epoch 86 Kt thc - Mt mt Hun luyn: 0.4287, IoU Hun luyn: 0.5745, F1-Score Hun luyn: 0.7229\n",
      "Mt mt Xc thc: 0.6125, IoU Xc thc: 0.3900, F1-Score Xc thc: 0.5219\n",
      "Learning Rate hin ti: 0.00000010\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 25/30\n",
      "Epoch 87/10000 Bt u...\n",
      "Epoch 87/10000, Batch 10/188, Loss: 0.6477\n",
      "Epoch 87/10000, Batch 20/188, Loss: 0.4043\n",
      "Epoch 87/10000, Batch 30/188, Loss: 0.3885\n",
      "Epoch 87/10000, Batch 40/188, Loss: 0.2304\n",
      "Epoch 87/10000, Batch 50/188, Loss: 0.3893\n",
      "Epoch 87/10000, Batch 60/188, Loss: 0.3096\n",
      "Epoch 87/10000, Batch 70/188, Loss: 0.2722\n",
      "Epoch 87/10000, Batch 80/188, Loss: 0.5329\n",
      "Epoch 87/10000, Batch 90/188, Loss: 0.3494\n",
      "Epoch 87/10000, Batch 100/188, Loss: 0.3597\n",
      "Epoch 87/10000, Batch 110/188, Loss: 0.3477\n",
      "Epoch 87/10000, Batch 120/188, Loss: 0.6072\n",
      "Epoch 87/10000, Batch 130/188, Loss: 0.3972\n",
      "Epoch 87/10000, Batch 140/188, Loss: 0.4690\n",
      "Epoch 87/10000, Batch 150/188, Loss: 0.4402\n",
      "Epoch 87/10000, Batch 160/188, Loss: 0.3217\n",
      "Epoch 87/10000, Batch 170/188, Loss: 0.3105\n",
      "Epoch 87/10000, Batch 180/188, Loss: 0.4703\n",
      "Epoch 87 Kt thc - Mt mt Hun luyn: 0.4252, IoU Hun luyn: 0.5779, F1-Score Hun luyn: 0.7255\n",
      "Mt mt Xc thc: 0.6098, IoU Xc thc: 0.3926, F1-Score Xc thc: 0.5262\n",
      "Learning Rate hin ti: 0.00000010\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 26/30\n",
      "Epoch 88/10000 Bt u...\n",
      "Epoch 88/10000, Batch 10/188, Loss: 0.3320\n",
      "Epoch 88/10000, Batch 20/188, Loss: 0.4124\n",
      "Epoch 88/10000, Batch 30/188, Loss: 0.2805\n",
      "Epoch 88/10000, Batch 40/188, Loss: 0.4345\n",
      "Epoch 88/10000, Batch 50/188, Loss: 0.2718\n",
      "Epoch 88/10000, Batch 60/188, Loss: 0.5637\n",
      "Epoch 88/10000, Batch 70/188, Loss: 0.4152\n",
      "Epoch 88/10000, Batch 80/188, Loss: 0.4306\n",
      "Epoch 88/10000, Batch 90/188, Loss: 0.4646\n",
      "Epoch 88/10000, Batch 100/188, Loss: 0.5713\n",
      "Epoch 88/10000, Batch 110/188, Loss: 0.3328\n",
      "Epoch 88/10000, Batch 120/188, Loss: 0.4179\n",
      "Epoch 88/10000, Batch 130/188, Loss: 0.3132\n",
      "Epoch 88/10000, Batch 140/188, Loss: 0.3556\n",
      "Epoch 88/10000, Batch 150/188, Loss: 0.4315\n",
      "Epoch 88/10000, Batch 160/188, Loss: 0.4690\n",
      "Epoch 88/10000, Batch 170/188, Loss: 0.5010\n",
      "Epoch 88/10000, Batch 180/188, Loss: 0.4620\n",
      "Epoch 88 Kt thc - Mt mt Hun luyn: 0.4167, IoU Hun luyn: 0.5864, F1-Score Hun luyn: 0.7333\n",
      "Mt mt Xc thc: 0.6122, IoU Xc thc: 0.3903, F1-Score Xc thc: 0.5226\n",
      "Learning Rate hin ti: 0.00000010\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 27/30\n",
      "Epoch 89/10000 Bt u...\n",
      "Epoch 89/10000, Batch 10/188, Loss: 0.2755\n",
      "Epoch 89/10000, Batch 20/188, Loss: 0.4541\n",
      "Epoch 89/10000, Batch 30/188, Loss: 0.5626\n",
      "Epoch 89/10000, Batch 40/188, Loss: 0.4006\n",
      "Epoch 89/10000, Batch 50/188, Loss: 0.5960\n",
      "Epoch 89/10000, Batch 60/188, Loss: 0.4233\n",
      "Epoch 89/10000, Batch 70/188, Loss: 0.4738\n",
      "Epoch 89/10000, Batch 80/188, Loss: 0.5649\n",
      "Epoch 89/10000, Batch 90/188, Loss: 0.3037\n",
      "Epoch 89/10000, Batch 100/188, Loss: 0.5402\n",
      "Epoch 89/10000, Batch 110/188, Loss: 0.3293\n",
      "Epoch 89/10000, Batch 120/188, Loss: 0.4360\n",
      "Epoch 89/10000, Batch 130/188, Loss: 0.5608\n",
      "Epoch 89/10000, Batch 140/188, Loss: 0.3871\n",
      "Epoch 89/10000, Batch 150/188, Loss: 0.5237\n",
      "Epoch 89/10000, Batch 160/188, Loss: 0.4427\n",
      "Epoch 89/10000, Batch 170/188, Loss: 0.6397\n",
      "Epoch 89/10000, Batch 180/188, Loss: 0.5171\n",
      "Epoch 89 Kt thc - Mt mt Hun luyn: 0.4337, IoU Hun luyn: 0.5695, F1-Score Hun luyn: 0.7184\n",
      "Mt mt Xc thc: 0.6103, IoU Xc thc: 0.3921, F1-Score Xc thc: 0.5249\n",
      "Learning Rate hin ti: 0.00000010\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 28/30\n",
      "Epoch 90/10000 Bt u...\n",
      "Epoch 90/10000, Batch 10/188, Loss: 0.4359\n",
      "Epoch 90/10000, Batch 20/188, Loss: 0.4468\n",
      "Epoch 90/10000, Batch 30/188, Loss: 0.3932\n",
      "Epoch 90/10000, Batch 40/188, Loss: 0.3105\n",
      "Epoch 90/10000, Batch 50/188, Loss: 0.6181\n",
      "Epoch 90/10000, Batch 60/188, Loss: 0.3341\n",
      "Epoch 90/10000, Batch 70/188, Loss: 0.4433\n",
      "Epoch 90/10000, Batch 80/188, Loss: 0.4811\n",
      "Epoch 90/10000, Batch 90/188, Loss: 0.3338\n",
      "Epoch 90/10000, Batch 100/188, Loss: 0.3944\n",
      "Epoch 90/10000, Batch 110/188, Loss: 0.3615\n",
      "Epoch 90/10000, Batch 120/188, Loss: 0.3414\n",
      "Epoch 90/10000, Batch 130/188, Loss: 0.4727\n",
      "Epoch 90/10000, Batch 140/188, Loss: 0.2993\n",
      "Epoch 90/10000, Batch 150/188, Loss: 0.5820\n",
      "Epoch 90/10000, Batch 160/188, Loss: 0.2728\n",
      "Epoch 90/10000, Batch 170/188, Loss: 0.3631\n",
      "Epoch 90/10000, Batch 180/188, Loss: 0.4191\n",
      "Epoch 90 Kt thc - Mt mt Hun luyn: 0.4272, IoU Hun luyn: 0.5759, F1-Score Hun luyn: 0.7225\n",
      "Mt mt Xc thc: 0.6126, IoU Xc thc: 0.3897, F1-Score Xc thc: 0.5216\n",
      "Learning Rate hin ti: 0.00000010\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 29/30\n",
      "Epoch 91/10000 Bt u...\n",
      "Epoch 91/10000, Batch 10/188, Loss: 0.3905\n",
      "Epoch 91/10000, Batch 20/188, Loss: 0.3588\n",
      "Epoch 91/10000, Batch 30/188, Loss: 0.6322\n",
      "Epoch 91/10000, Batch 40/188, Loss: 0.2459\n",
      "Epoch 91/10000, Batch 50/188, Loss: 0.4834\n",
      "Epoch 91/10000, Batch 60/188, Loss: 0.6175\n",
      "Epoch 91/10000, Batch 70/188, Loss: 0.4223\n",
      "Epoch 91/10000, Batch 80/188, Loss: 0.4572\n",
      "Epoch 91/10000, Batch 90/188, Loss: 0.4462\n",
      "Epoch 91/10000, Batch 100/188, Loss: 0.4324\n",
      "Epoch 91/10000, Batch 110/188, Loss: 0.3867\n",
      "Epoch 91/10000, Batch 120/188, Loss: 0.2773\n",
      "Epoch 91/10000, Batch 130/188, Loss: 0.4015\n",
      "Epoch 91/10000, Batch 140/188, Loss: 0.2876\n",
      "Epoch 91/10000, Batch 150/188, Loss: 0.6824\n",
      "Epoch 91/10000, Batch 160/188, Loss: 0.3155\n",
      "Epoch 91/10000, Batch 170/188, Loss: 0.2905\n",
      "Epoch 91/10000, Batch 180/188, Loss: 0.5813\n",
      "Epoch 91 Kt thc - Mt mt Hun luyn: 0.4289, IoU Hun luyn: 0.5743, F1-Score Hun luyn: 0.7225\n",
      "Mt mt Xc thc: 0.6086, IoU Xc thc: 0.3939, F1-Score Xc thc: 0.5275\n",
      "Learning Rate hin ti: 0.00000010\n",
      "Mt mt xc thc khng ci thin. S kin nhn: 30/30\n",
      "Dng sm!\n",
      "\n",
      "Qu trnh hun luyn  hon tt.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Chun b d liu ---\n",
    "train_dataset = CrackDetectionDataset(train_img_paths, train_mask_paths, augment=True)\n",
    "val_dataset = CrackDetectionDataset(val_img_paths, val_mask_paths, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- Khi to m hnh, optimizer, criterion ---\n",
    "model = SwinUNet(input_channels=3, num_classes=1).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3) # Bn  dng AdamW trc , nn gi nguyn\n",
    "\n",
    "# THAY I 3: Khi to scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-7)\n",
    "\n",
    "# Khi to criterion (ch dng DiceLoss)\n",
    "criterion = CombinedLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "callbacks_config = {\n",
    "    'patience': 30,\n",
    "    'checkpoint_path': 'swin_unet_best_pytorch_IOULoss.pth'\n",
    "}\n",
    "\n",
    "# --- Logic  tip tc hun luyn t checkpoint ---\n",
    "start_epoch = 0\n",
    "best_val_loss_so_far = float('inf')\n",
    "checkpoint_path = callbacks_config['checkpoint_path']\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Pht hin checkpoint ti {checkpoint_path}. ang ti  tip tc hun luyn...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    # THAY I 4: Ti trng thi ca scheduler nu c trong checkpoint\n",
    "    if 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        print(\" ti trng thi ca Scheduler.\")\n",
    "    \n",
    "    start_epoch = checkpoint['epoch'] + 1 \n",
    "    best_val_loss_so_far = checkpoint['best_val_loss']\n",
    "    \n",
    "    print(f\" ti checkpoint t Epoch {start_epoch-1}. Tip tc hun luyn t Epoch {start_epoch}.\")\n",
    "    print(f\"Mt mt xc thc tt nht trc : {best_val_loss_so_far:.4f}\")\n",
    "else:\n",
    "    print(\"Khng tm thy checkpoint. Bt u hun luyn t u (Epoch 0).\")\n",
    "\n",
    "print(\"\\nBt u hun luyn m hnh Swin-Unet...\")\n",
    "# THAY I 5: Truyn i tng scheduler vo hm train_model\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, scheduler,\n",
    "            num_epochs=10000, callbacks_config=callbacks_config,\n",
    "            start_epoch=start_epoch, best_val_loss_so_far=best_val_loss_so_far)\n",
    "\n",
    "print(\"\\nQu trnh hun luyn  hon tt.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
